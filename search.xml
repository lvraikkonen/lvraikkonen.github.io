<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[foreachRDD正确使用]]></title>
    <url>%2F2018%2F09%2F04%2FforeachRDD%E6%AD%A3%E7%A1%AE%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[上面在Spark-Streaming中介绍了foreach，dstream.foreachRDD是一个功能强大的原语primitive，它允许将数据发送到外部系统。输出操作实际上是允许外部系统消费转换后的数据，它们触发的实际操作是DStream转换。所以要掌握它，对它要有深入了解。下面就是一些常见的错误用法。 错误使用在Spark驱动中创建一个连接对象，在 Spark worker 中尝试调用这个连接对象将记录保存到RDD中，很容易下出下面的代码： 123456dstream.foreachRDD &#123; rdd =&gt; val connection = createNewConnection() // executed at the driver rdd.foreach &#123; record =&gt; connection.send(record) // executed at the worker &#125;&#125; 这是不正确的，因为这需要先序列化连接对象，然后将它从driver端发送到worker中。这样的连接对象在机器之间不能传送。通常会报不能序列化的错误: 正确做法是在worker中创建连接对象 1234567dstream.foreachRDD &#123; rdd =&gt; rdd.foreach &#123; record =&gt; val connection = createNewConnection() connection.send(record) connection.close() &#125;&#125; 但是这么做，很明显也有问题：为每一条记录都创建一个连接对象。下面开始改进 改进创建一个连接对象会有资源和时间的开销，为每条记录创建和销毁连接对象会有非常高的开支，第一种优化的方案是：为RDD的每个分区创建一个连接对象： 1234567dstream.foreachRDD &#123; rdd =&gt; rdd.foreachPartition &#123; partitionOfRecords =&gt; val connection = createNewConnection() partitionOfRecords.foreach(record =&gt; connection.send(record)) connection.close() &#125;&#125; 进一步改进创建连接对象对资源和时间要求很高，那么可以利用连接池来维护有限的连接对象资源。 创建静态连接对象池： 12345678910111213141516171819202122232425262728293031323334public class ConnectionPool &#123; private static ComboPooledDataSource dataSource = new ComboPooledDataSource(); static &#123; dataSource.setJdbcUrl("jdbc:mysql://localhost:3306/dbs"); dataSource.setUser("user"); dataSource.setPassword("pwd"); dataSource.setMaxPoolSize(50); dataSource.setMinPoolSize(2); dataSource.setInitialPoolSize(10); dataSource.setMaxStatements(100); &#125; public static Connection getConnection()&#123; try&#123; return dataSource.getConnection(); &#125; catch(SQLException e)&#123; e.printStackTrace(); &#125; return null; &#125; public static void returnConnection(Connection conn)&#123; if (conn != null)&#123; try &#123; conn.close(); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 这样，每使用一个连接对象将一批数据写入外部系统之后，就将该连接对象放回连接池。 12345678dstream.foreachRDD &#123; rdd =&gt; rdd.foreachPartition &#123; partitionOfRecords =&gt; // ConnectionPool is a static, lazily initialized pool of connections val connection = ConnectionPool.getConnection() partitionOfRecords.foreach(record =&gt; connection.send(record)) ConnectionPool.returnConnection(connection) // return to the pool for future reuse &#125;&#125; 再进一步改进经过上面连接池的改进，基本上性能已经难满足要求了。如果RDD数据量比较大，也可以考虑分批次写入外部存储。 对于rdd中一个分区的数据，首先从连接池中获取一个连接对象，然后准备好SQL语句，超过500条记录后，向数据库提交一次数据(如果不足500条，就将这批数据放到一个批次)，提交数据后，将连接对象放回连接池以便后面使用。 12345678910111213141516171819202122dstream.foreachRDD &#123; (rdd, time) =&gt; rdd.foreachPartition &#123; partitionRecords =&gt; val conn = ConnectionPool.getConnection conn.setAutoCommit(false) val statement = conn.prepareStatement(s"insert into wordcount(ts, word, count) values (?, ?, ?)") partitionRecords.zipWithIndex.foreach &#123; case ((word, count), index) =&gt; statement.setLong(1, time.milliseconds) statement.setString(2, word) statement.setInt(3, count) statement.addBatch() if (index != 0 &amp;&amp; index % 500 == 0) &#123; statement.executeBatch() conn.commit() &#125; &#125; statement.executeBatch() statement.close() conn.commit() conn.setAutoCommit(true) ConnectionPool.returnConnection(conn) &#125;&#125; 参考：Design Patterns for using foreachRDD]]></content>
      <categories>
        <category>Big Data</category>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Big Data</tag>
        <tag>Spark</tag>
        <tag>流处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark-Streaming入门和实践]]></title>
    <url>%2F2018%2F08%2F31%2FSpark-Streaming%E5%85%A5%E9%97%A8%E5%92%8C%E5%AE%9E%E8%B7%B5%2F</url>
    <content type="text"><![CDATA[流处理类型Spark Streaming是Spark解决方案中实时处理的组件，本质是将数据源分割为很小的批次，以类似离线批处理的方式处理这部分数据。这种方式提升了数据吞吐能力，但是也增加了数据处理的延迟，延迟通常是秒级或者分钟级。 Spark Streaming底层依赖 Spark Core的 RDD，内部的调度方式也依赖于DAG调度器。Spark Streaming的离散数据流DStream本质上是RDD在流式数据上的抽象。 编写Spark Streaming应用添加下述依赖到你的Maven项目中： 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming_2.11&lt;/artifactId&gt; &lt;version&gt;2.3.0&lt;/version&gt;&lt;/dependency&gt; 下面是WordCount程序的Streaming版本： 123456789101112131415161718import org.apache.spark._import org.apache.spark.streaming._import org.apache.spark.streaming.StreamingContext._ // not necessary since Spark 1.3// Create a local StreamingContext with two working thread and batch interval of 1 second.val conf = new SparkConf().setMaster("local[2]").setAppName("NetworkWordCount")val ssc = new StreamingContext(conf, Seconds(1))val lines = ssc.socketTextStream("localhost", 9999)val words = lines.flatMap(_.split(" "))val pairs = words.map(word =&gt; (word, 1))val wordCounts = pairs.reduceByKey(_ + _)wordCounts.print()ssc.start() // Start the computationssc.awaitTermination() // Wait for the computation to terminate 这个HelloWorld程序主要有下面几个步骤组成： 初始化StreamingContext 创建DStream接收器 (会单独起一个线程运行接收器) 定义DStream的转换操作 定义DStream的输出 启动流处理程序 接收器接收器有两类：基础的接收器(可以自己实现) 和高级数据源(例如 Kafka, Flume等) 下面模拟实现一个自定义的接收器，需要继承Receiver抽象类，然后实现 onStart()方法和onStop()方法 123456789101112131415161718192021222324252627282930313233343536373839404142class CustomReceiver(host: String, port: Int) extends Receiver[String](StorageLevel.MEMORY_AND_DISK_2) with Logging &#123; override def onStart(): Unit = &#123; // 启动进程接收数据 new Thread("Socket Receiver")&#123; override def run()&#123; receive() &#125; &#125;.start() &#125; override def onStop(): Unit = &#123; &#125; // create a customize socket connection and receive data until receiver is stopped private def receive(): Unit =&#123; var socket: Socket = null var userInput: String = null try &#123; // connect to host:port socket = new Socket(host, port) val reader = new BufferedReader(new InputStreamReader(socket.getInputStream(), "UTF-8")) userInput = reader.readLine() while(!isStopped() &amp;&amp; userInput != null)&#123; store(userInput) userInput = reader.readLine() &#125; reader.close() socket.close() restart("Trying to connect again") &#125;catch &#123; case e: java.net.ConnectException =&gt; restart("Error connecting to " + host + ":" + port, e) case t: Throwable =&gt; restart("Error receiving data", t) &#125; &#125;&#125; Spark Streaming Custom Receivers 关于例如Kafka和Flume等高级数据源，下面会有一章介绍 DStream的转换操作上面说过DStream本质上是RDD在流式数据上的抽象，所以RDD上很多的转换操作在这里都能通用，例如下面几个： map(func) flatMap(func) filter(func) repartition(numPartitions) union(otherStream) count() reduce(func) countByValue() reduceByKey(func, [numTasks]) join(otherStream, [numTasks]) cogroup(otherStream, [numTasks]) transform(func) updateStateByKey(func) 下面是流处理需要处理的窗口函数(Window Operations) Window Operation 窗口操作 在窗口操作中需要指定两个参数: window length 窗口长度 —— 窗口的时间长度（上图中为3） sliding interval 滑动间隔 —— 窗口操作的执行间隔（上图中为2） 这两个参数必须为流处理定义的批处理间隔的整数倍。 12// Reduce last 30 seconds of data, every 10 secondsval windowedWordCounts = pairs.reduceByKeyAndWindow((a:Int,b:Int) =&gt; (a + b), Seconds(30), Seconds(10)) DStreams上的输出操作foreachRDDdstream.foreachRDD是一个功能强大的原语primitive，它允许将数据发送到外部系统。下面将结果保存到数据库中 12345678dstream.foreachRDD &#123; rdd =&gt; rdd.foreachPartition &#123; partitionOfRecords =&gt; // ConnectionPool is a static, lazily initialized pool of connections val connection = ConnectionPool.getConnection() partitionOfRecords.foreach(record =&gt; connection.send(record)) ConnectionPool.returnConnection(connection) // return to the pool for future reuse &#125;&#125; Caching/Persistence 缓存/持久化类似于RDD，DStream也允许开发者在内存中持久化stream流数据 Checkpoint 设置一个Streaming程序是需要7X24运行的，所以故障恢复能力是很重要的，为此，Spark Streaming需要检查点以便从故障中恢复。 有两种类型的检查点： Metadata检查点 数据检查点，将生成RDD保存在可靠的存储中 note: 如果在应用程序中使用updateStateByKey或者reduceByKeyAndWindow (带有反转函数)，那么必须提供checkpointing目录 集成Kafka、Flume等下面是集成Kafka的方法，首先添加依赖包 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming-kafka-0-10_2.11&lt;/artifactId&gt; &lt;version&gt;2.3.0&lt;/version&gt;&lt;/dependency&gt; 这里有个坑: 版本不匹配的依赖可能会产生很多兼容性问题，由于Kafka 0.10.0之后引入了新的Kafka Consumer API，所以现在的这个版本的集成还是实验性的，以后可能还会有变化。Spark 2.3.0开始，kafka-0-8的依赖不再被支持。 连接方式的比较 Kafka consumer传统消费者（老方式）需要连接zookeeper，简称Receiver方式，是高级的消费API，自动更新偏移量，支持WAL，但是效率比较低。 新的方式（高效的方式）不需要连接Zookeeper，但是需要自己维护偏移量，简称直连方式，直接连在broker上，但是需要手动维护偏移量，以迭代器的方式边接收数据边处理，效率较高。 Kafka 0.10以后的只支持直连方式 创建DirectStream1234567891011121314151617181920212223import org.apache.kafka.clients.consumer.ConsumerRecordimport org.apache.kafka.common.serialization.StringDeserializerimport org.apache.spark.streaming.kafka010._import org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistentimport org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribeval kafkaParams = Map[String, Object]( "bootstrap.servers" -&gt; "localhost:9092,anotherhost:9092", "key.deserializer" -&gt; classOf[StringDeserializer], "value.deserializer" -&gt; classOf[StringDeserializer], "group.id" -&gt; "use_a_separate_group_id_for_each_stream", "auto.offset.reset" -&gt; "latest", "enable.auto.commit" -&gt; (false: java.lang.Boolean))val topics = Array("topicA", "topicB")val stream = KafkaUtils.createDirectStream[String, String]( streamingContext, PreferConsistent, Subscribe[String, String](topics, kafkaParams))stream.map(record =&gt; (record.key, record.value)) KafkaUtils.creatDirectStream 需要传入3个参数： StreamingContext LocationStrategy，默认采用LocationStrategies.PreferConsistent ConsumerStrategy 偏移量操作上面说过，直连的方式需要手动维护偏移量。Kafka提供了一个提交offset的API，这个API把特定的kafka topic的offset进行存储。默认情况下，新的消费者会周期性的自动提交offset，这里把enable.auto.commit设置为false。但是，使用commitAsync api，用户可以在确保计算结果被成功保存后自己来提交offset。和使用checkpoint方式相比，Kafka是一个不用关心应用代码的变化可靠存储系统。 获取偏移量 1234567stream.foreachRDD &#123; rdd =&gt; val offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges rdd.foreachPartition &#123; iter =&gt; val o: OffsetRange = offsetRanges(TaskContext.get.partitionId) println(s"$&#123;o.topic&#125; $&#123;o.partition&#125; $&#123;o.fromOffset&#125; $&#123;o.untilOffset&#125;") &#125;&#125; 可以使用commitAsync来提交偏移量 123456stream.foreachRDD &#123; rdd =&gt; val offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges // some time later, after outputs have completed stream.asInstanceOf[CanCommitOffsets].commitAsync(offsetRanges)&#125;]]></content>
      <categories>
        <category>Big Data</category>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Big Data</tag>
        <tag>Spark</tag>
        <tag>流处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ELK+Filebeat收集日志]]></title>
    <url>%2F2018%2F08%2F22%2FELK-Filebeat%E6%94%B6%E9%9B%86%E6%97%A5%E5%BF%97%2F</url>
    <content type="text"><![CDATA[ELK(Elasticsearch + Logstash + Kibana) 简单来说，可以完成对于海量日志数据的汇总、搜索、查询以及可视化，可以快速定位和分析问题。 通过 Logstash 我们可以把各种日志进行转换后存储到 elasticsearch 中 通过 Elasticsearch 可以非常灵活的存储和索引日志，并且elasticsearch 提供了丰富的 HTTP REST API 接口来对数据进行增删查等操作 通过 Kibana 我们可以对存储在 elasticsearch 中的日志以图形化的形式进行展现，并且提供非常丰富的过滤接口，让用户能够通过过滤快速定位问题 在日常日志处理中，也常用Beats工具，可以作为轻量级的数据收集Agent 与 Logstash 不同，Beats 只是 data shipper。Beats 家族共享 libbeat 这个库，每个产品分别实现对不同数据来源的收集。目前官方实现有： Filebeat —— 文件 Metricbeat —— 系统及应用指标 Packetbeat —— 网络抓包分析，如 SQL, DNS Winlogbeat —— Windows 系统日志 Auditbeat —— 审计数据 Heartbeat —— ICMP, TCP, HTTP 监控 日志收集框架 日志数据流如下: 应用程序将日志存储在服务器上，部署在每台服务器上的FileBeat负责收集日志，然后将日志发送给Kafka进行消息缓冲，同时也可以支持流失处理(流处理应用可以从消息的offset初始的地方来读取)；然后通过Logstash将日志进行处理之后，比如解析等处理，将处理后的对象传递给ElasticSearch，进行落地并进行索引处理；最后通过Kibana来提供web界面，来查看日志等。 下面以Log4net配置的应用Log为例。 样例日志数据针对两种不同类型的应用日志，分别使用两种pattern，也分别记录到Debug.lg和Error.log文件中。 日志类型为Debug,Info,Warn的日志，使用[%date] [%thread] %-5level Log4NetTest %logger %method [%message%exception]%n模式，分别记录下时间，线程，日志等级，应用名称，日志记录类属性，日志记录方法，日志信息和异常 日志类型为Error和Fatal的日志，使用[%date] [%thread] %-5level Log4NetTest %l [%message%n%exception]%n，分别是时间，线程，日志等级，应用名称，出错位置（包含具体文件，以及所在行，需要PDB文件才能到行），日志信息和异常 配置FilebeatFilebeat隶属于Beats，Filebeat占用资源极少，简单配置filebeat.yml文件就可以使用 123456789101112131415161718192021222324252627282930313233filebeat.prospectors:- type: log # Change to true to enable this prospector configuration. enabled: true # Paths that should be crawled and fetched. Glob based paths. paths: - /Users/lvshuo/bigdata/logs/*.Error.log multiline.pattern: &apos;^[0-9]&#123;4&#125;-[0-9]&#123;2&#125;-[0-9]&#123;2&#125; [0-9]&#123;2&#125;:[0-9]&#123;2&#125;:[0-9]&#123;2&#125;,[0-9]&#123;3&#125;&apos; multiline.negate: true multiline.match: after fields: log_topics: errorlog- type: log enabled: false paths: - /Users/lvshuo/bigdata/logs/*.Info.log multiline.pattern: &apos;^[0-9]&#123;4&#125;-[0-9]&#123;2&#125;-[0-9]&#123;2&#125; [0-9]&#123;2&#125;:[0-9]&#123;2&#125;:[0-9]&#123;2&#125;,[0-9]&#123;3&#125;&apos; multiline.negate: true multiline.match: after fields: log_topics: infolog#--------------- Logstash output ----------------------output.logstash: hosts: [&quot;localhost:5044&quot;] index: &apos;&#123;[fields][log_topics]&#125;&apos; Note: 在6.0版本以后，document_type类型就不被支持了。为了分类处理INFO和ERROR日志，并写入不同的index中，在filebeat的配置里，加入fields的自定义字段，然后通过%{[]}获取对应的值 ，如下： if [fields][fieldname] == &quot;string&quot; 为了以后不再踩坑，建议参考安装包中自带的样例配置文件 filebeat.reference.yml 上面将收集到的Log数据传递给Logstash(为了演示，直接将Beats输出到Logstash) 配置Logstash123456789101112131415161718192021222324252627282930313233343536#指定logstash监听filebeat的端口input &#123; beats &#123; port =&gt; &quot;5044&quot; &#125;&#125;# The filter part of this file is commented out to indicate that it is# optional.filter &#123; grok &#123; match =&gt; &#123; &quot;message&quot; =&gt; &quot;%&#123;TIMESTAMP_ISO8601:datetime&#125; \[%&#123;SYSLOGPROG:appName&#125;\] %&#123;LOGLEVEL:level&#125; \s*(?&lt;traceback&gt;([\s\S]*))&quot; &#125; overwrite =&gt; [&quot;message&quot;] &#125; date &#123; match =&gt; [&quot;datetime&quot;, &quot;yyyy-MM-dd HH:mm:ss,SSS&quot;, &quot;UNIX&quot;] target =&gt; &quot;@timestamp&quot; locale =&gt; &quot;cn&quot; &#125; &#125;output &#123; if [fields][log_topics] == &quot;errorlog&quot; &#123; # file &#123; # path =&gt; &quot;/Users/lvshuo/Desktop/tmp/log/errorLog_%&#123;+YYYYMMdd&#125;_%&#123;+HH&#125;.log&quot; # codec =&gt; line &#123; format =&gt; &quot;%&#123;message&#125;&quot;&#125; # &#125; stdout &#123; codec =&gt; rubydebug &#125; elasticsearch &#123; hosts =&gt; [&quot;localhost:9200&quot;] index =&gt; &quot;logstash-%&#123;[fields][log_topics]&#125;-%&#123;+YYYY.MM.dd&#125;&quot; &#125; &#125; &#125; Grok 是 Logstash 最重要的插件，可以在 grok 里预定义好命名正则表达式 下面是一个很好用的Grok Debugger，可以调试grok表达式： http://grokdebug.herokuapp.com/ Grok过滤器插件Grok内置了120多种的正则表达式库，地址:https://github.com/logstash-plugins/logstash-patterns-core/tree/master/patterns 比如,下面这条日志： 155.3.244.1 GET /index.html 15824 0.043 这条日志可切分为5个部分，IP(55.3.244.1)、方法(GET)、请求文件路径(/index.html)、字节数(15824)、访问时长(0.043),对这条日志的解析模式(正则表达式匹配)如下: 1%&#123;IP:client&#125; %&#123;WORD:method&#125; %&#123;URIPATHPARAM:request&#125; %&#123;NUMBER:bytes&#125; %&#123;NUMBER:duration&#125; 写到filter中: 12345filter &#123; grok &#123; match =&gt; &#123; &quot;message&quot; =&gt; &quot;%&#123;IP:client&#125; %&#123;WORD:method&#125; %&#123;URIPATHPARAM:request&#125; %&#123;NUMBER:bytes&#125; %&#123;NUMBER:duration&#125;&quot;&#125; &#125;&#125; 解析后: 12345client: 55.3.244.1method: GETrequest: /index.htmlbytes: 15824duration: 0.043 如果内置的正则表达式不能满足要求，可以按下面步骤解析任意格式日志： 先确定日志的切分原则，也就是一条日志切分成几个部分。 对每一块进行分析，如果Grok中正则满足需求，直接拿来用。如果Grok中没用现成的，采用自定义模式。 学会在Grok Debugger中调试。 下面两条日志: 122017-03-07 00:03:44,373 4191949560 [CASFilter.java:330:DEBUG] entering doFilter()2017-03-16 00:00:01,641 133383049 [UploadFileModel.java:234:INFO] 上报内容准备写入文件 切分原则: 1234562017-03-16 00:00:01,641:时间133383049：编号UploadFileModel.java:java类名234:代码行号INFO：日志级别entering doFilter()：日志内容 前五个字段用Grok中已有的，分别是TIMESTAMP_ISO8601、NUMBER、JAVAFILE、NUMBER、LOGLEVEL，最后一个采用自定义正则的形式，日志级别的]之后的内容不论是中英文，都作为日志信息处理，最后一个字段的内容用info表示，正则如下: (?([\s\S]*)) 上面两条日志对应的完整的正则如下，其中\s*用于剔除空格。 1%&#123;TIMESTAMP_ISO8601:time&#125; %&#123;NUMBER:num&#125; \[\s*%&#123;JAVAFILE:class&#125;\s*\:\s*%&#123;NUMBER:lineNumber&#125;\s*\:%&#123;LOGLEVEL:level&#125;\s*\] (?&lt;info&gt;([\s\S]*)) http://grokdebug.herokuapp.com/ 时间处理插件Datedate 插件可以用来转换你的日志记录中的时间字符串，变成 LogStash::Timestamp 对象，然后转存到 @timestamp 字段里 12345678filter &#123; grok &#123; match =&gt; [&quot;message&quot;, &quot;%&#123;HTTPDATE:logdate&#125;&quot;] &#125; date &#123; match =&gt; [&quot;logdate&quot;, &quot;dd/MMM/yyyy:HH:mm:ss Z&quot;] &#125;&#125; 数据修改插件(Mutate)mutate 插件是 Logstash 另一个重要插件。它提供了丰富的基础类型数据处理能力。包括类型转换，字符串处理和字段处理等。 12345filter &#123; mutate &#123; convert =&gt; [&quot;request_time&quot;, &quot;float&quot;] &#125;&#125; GeoIP 地址查询归类GeoIP 库可以根据 IP 地址提供对应的地域信息，包括国别，省市，经纬度等，对于可视化地图和区域统计非常有用。 启动首先启动log所在机器上部署的Filebeat agent mac:12sudo chown root filebeat.yml sudo ./filebeat -e -c filebeat.yml -d &quot;publish&quot; windows:1PS C:\Program Files\Filebeat&gt; Start-Service filebeat 然后启动Logstash 12bin/logstash -f first-pipeline.conf --config.test_and_exit 测试配置文件 bin/logstash -f first-pipeline.conf --config.reload.automatic 自动加载配置文件的修改 可以在Kibana中看到]]></content>
      <tags>
        <tag>Elasticsearch</tag>
        <tag>ELK</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark应用运行流程(转)]]></title>
    <url>%2F2018%2F08%2F17%2FSpark%E5%BA%94%E7%94%A8%E8%BF%90%E8%A1%8C%E6%B5%81%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[Spark核心技术原理透视一（Spark运行原理） Spark专业术语定义Spark应用程序指的是用户编写的Spark应用程序，包含了Driver功能代码和分布在集群中多个节点上运行的Executor代码。Spark应用程序，由一个或多个作业JOB组成，如下图所示。 Driver：驱动程序Spark 中的 Driver 即运行上述 Application 的 Main() 函数并且创建 SparkContext，其中创建 SparkContext 的目的是为了准备 Spark 应用程序的运行环境。在 Spark 中由 SparkContext 负责和 ClusterManager 通信，进行资源的申请、任务的分配和监控等；当 Executor 部分运行完毕后，Driver 负责将 SparkContext 关闭。通常 SparkContext 代表 Driver，如下图所示。 Cluster Manager：资源管理器指的是在集群上获取资源的外部服务，常用的有：Standalone，Spark 原生的资源管理器，由 Master 负责资源的分配；Haddop Yarn，由 Yarn 中的 ResearchManager 负责资源的分配；Messos，由 Messos 中的 Messos Master 负责资源管理，如下图所示。 Executor：执行器Application 运行在 Worker 节点上的一个进程，该进程负责运行 Task，并且负责将数据存在内存或者磁盘上，每个 Application 都有各自独立的一批 Executor，如下图所示。 Worker：计算节点集群中任何可以运行 Application 代码的节点，类似于 Yarn 中的 NodeManager 节点。在Standalone模式中指的就是通过Slave文件配置的Worker节点，在Spark on Yarn模式中指的就是NodeManager节点，在Spark on Messos模式中指的就是Messos Slave节点，如下图所示。 RDD：弹性分布式数据集Resillient Distributed Dataset，Spark的基本计算单元，可以通过一系列算子进行操作（主要有Transformation和Action操作），如下图所示。 窄依赖父RDD每一个分区最多被一个子RDD的分区所用；表现为一个父RDD的分区对应于一个子RDD的分区，或两个父RDD的分区对应于一个子RDD 的分区。如图所示。 宽依赖父RDD的每个分区都可能被多个子RDD分区所使用，子RDD分区通常对应所有的父RDD分区。如图所示。 常见的窄依赖有：map、filter、union、mapPartitions、mapValues、join（父RDD是hash-partitioned ：如果JoinAPI之前被调用的RDD API是宽依赖(存在shuffle), 而且两个join的RDD的分区数量一致，join结果的rdd分区数量也一样，这个时候join api是窄依赖）。 常见的宽依赖有groupByKey、partitionBy、reduceByKey、join（父RDD不是hash-partitioned ：除此之外的，rdd 的join api是宽依赖）。 DAG：有向无环图Directed Acycle graph，反应RDD之间的依赖关系，如图所示。 DAGScheduler：有向无环图调度器基于 DAG 划分 Stage 并以 TaskSet 的形势把 Stage 提交给 TaskScheduler；负责将作业拆分成不同阶段的具有依赖关系的多批任务；最重要的任务之一就是：计算作业和任务的依赖关系，制定调度逻辑。在 SparkContext 初始化的过程中被实例化，一个 SparkContext 对应创建一个 DAGScheduler。 TaskScheduler：任务调度器将 Taskset 提交给 worker（集群）运行并回报结果；负责每个具体任务的实际物理调度。如图所示。 Job：作业由一个或多个调度阶段所组成的一次计算作业；包含多个Task组成的并行计算，往往由Spark Action催生，一个JOB包含多个RDD及作用于相应RDD上的各种Operation。如图所示。 Stage：调度阶段一个任务集对应的调度阶段；每个Job会被拆分很多组Task，每组任务被称为Stage，也可称TaskSet，一个作业分为多个阶段；Stage分成两种类型ShuffleMapStage、ResultStage。如图所示。 TaskSet：任务集由一组关联的，但相互之间没有Shuffle依赖关系的任务所组成的任务集。如图所示。 一个Stage创建一个TaskSet； 为Stage的每个Rdd分区创建一个Task,多个Task封装成TaskSet Task：任务被送到某个Executor上的工作任务；单个分区数据集上的最小处理流程单元。如图所示 总体如图所示： Spark运行基本流程 Spark运行架构特点Spark核心原理透视计算流程 从代码构建DAG图12345678val lines1 = sc.textFile(inputPath1).map(···)).map(···)val lines2 = sc.textFile(inputPath2).map(···)val lines3 = sc.textFile(inputPath3)val dtinone1 = lines2.union(lines3)val dtinone = lines1.join(dtinone1)dtinone.saveAsTextFile(···)dtinone.filter(···).foreach(···) Spark的计算发生在RDD的Action操作，而对Action之前的所有Transformation，Spark只是记录下RDD生成的轨迹，而不会触发真正的计算。 Spark内核会在需要计算发生的时刻绘制一张关于计算路径的有向无环图，也就是DAG。 将DAG划分为Stage核心算法Application多个job多个Stage：Spark Application中可以因为不同的Action触发众多的job，一个Application中可以有很多的job，每个job是由一个或者多个Stage构成的，后面的Stage依赖于前面的Stage，也就是说只有前面依赖的Stage计算完毕后，后面的Stage才会运行。 划分依据：Stage划分的依据就是宽依赖，何时产生宽依赖，reduceByKey, groupByKey等算子，会导致宽依赖的产生。 核心算法：从后往前回溯，遇到窄依赖加入本stage，遇见宽依赖进行Stage切分。Spark内核会从触发Action操作的那个RDD开始从后往前推，首先会为最后一个RDD创建一个stage，然后继续倒推，如果发现对某个RDD是宽依赖，那么就会将宽依赖的那个RDD创建一个新的stage，那个RDD就是新的stage的最后一个RDD。然后依次类推，继续继续倒推，根据窄依赖或者宽依赖进行stage的划分，直到所有的RDD全部遍历完成为止。 将DAG划分为Stage剖析从HDFS中读入数据生成3个不同的RDD，通过一系列transformation操作后再将计算结果保存回HDFS。可以看到这个DAG中只有join操作是一个宽依赖，Spark内核会以此为边界将其前后划分成不同的Stage. 同时我们可以注意到，在图中Stage2中，从map到union都是窄依赖，这两步操作可以形成一个流水线操作，通过map操作生成的partition可以不用等待整个RDD计算结束，而是继续进行union操作，这样大大提高了计算的效率。]]></content>
      <categories>
        <category>Big Data</category>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Big Data</tag>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RDD详解]]></title>
    <url>%2F2018%2F08%2F17%2FRDD%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[RDD API 实操]]></title>
    <url>%2F2018%2F08%2F15%2FRDD-API-%E5%AE%9E%E6%93%8D%2F</url>
    <content type="text"><![CDATA[RDD是Spark最核心的数据抽象，全称叫Resilient Distributed Datasets(弹性分布式数据集)。org.apache.spark.rdd.RDD是一个抽象类，定义了RDD的基本操作和属性 1234567891011121314// 计算某个分区的数据，返回Iteratordef compute(split: Partition, context: TaskContext): Iterator[T]// 获取RDD的分区列表protected def getPartitions: Array[Partition]// 获取RDD的依赖列表protected def getDependencies: Seq[Dependency[_]] = deps// 获取某一个分区数据所在的机器protected def getPreferredLocations(split: Partition): Seq[String] = Nil// 分区器@transient val partitioner: Option[Partitioner] = None 创建RDDRDD可以有三种创建方式： 从存储系统中创建，例如HDFS、文件等等 从已经存在的RDD中创建，即使用Transform操作 从内存中的列表数据创建 下面是在内存中创建RDD的例子 1234val rdd01 = sc.makeRDD(List(1, 2, 3, 4, 5, 6))// 两个分区val rdd02 = sc.parallelize(List(1, 2, 3, 4, 5, 6), 2) RDD支持两类操作： 转换(Transform) 行动(Action) 当RDD执行转换操作时候，实际计算并没有被执行，只有当RDD执行行动操作时候才会触发计算任务提交，执行相应的计算操作。 首先准备RDD 12345val rddInt:RDD[Int] = sc.makeRDD(List(1,2,3,4,5,6,2,5,1))val rddStr:RDD[String] = sc.parallelize(Array("a","b","c","d","b","a"), 2)val rddFile:RDD[String] = sc.textFile("word_count.text", 2)val rdd01:RDD[Int] = sc.makeRDD(List(1,3,5,3))val rdd02:RDD[Int] = sc.makeRDD(List(2,4,5,1)) 转换Transform下面是几个简单的Transform操作 12345678910111213141516171819/* map操作 */rddInt.map(x =&gt; x + 1).collect()/* mapPartitions操作 */rddInt.mapPartitions操作(x =&gt; x + 1).collect()/* filter操作 */rddInt.filter(x =&gt; x &gt; 4).collect()/* flatMap操作 */rddFile.flatMap &#123; x =&gt; x.split(" ") &#125;.take(5)/* distinct去重操作 */rddInt.distinct().collect()rddStr.distinct().collect()/* union操作 */rdd01.union(rdd02).collect()/* intersection操作 */rdd01.intersection(rdd02).collect()/* subtract操作 */println(rdd01.subtract(rdd02).collect()/* cartesian操作 */rdd01.cartesian(rdd02).collect() map(func)将函数应用于 RDD 中的每个元素，返回值构成新的 RDD flatMap(func)类似于 map，但是每个输入项可以映射为0个输出项或更多输出项（打散） filter(func)将函数应用于 RDD 中的每个元素，返回 func 函数的值为true的元素形成一个新的 RDD distinct() 去重union(otherRDD) 并集生成一个包含两个 RDD 中所有元素的 RDD。如果输入的 RDD 中有重复数据，union() 操作也会包含这些重复的数据． intersection(otherRDD)求两个 RDD 共同的元素的 RDD。 intersection() 在运行时也会去掉所有重复的元素 subtract(otherRDD) 差集subtract 接受另一个 RDD 作为参数，返回一个由只存在第一个 RDD 中而不存在第二个 RDD 中的所有元素组成的 RDD 下面的Transform操作应用在Key-Value的RDD上。 groupByKey() 分组根据键值对 key 进行分组。 在（K，V）键值对的数据集上调用时，返回（K，Iterable ）键值对的数据集 note: 基于combineByKeyWithClassTag 12345val rdd:RDD[(String,Int)] = sc.makeRDD(List(("k01",3),("k02",6),("k03",2),("k01",26)))val other:RDD[(String,Int)] = sc.parallelize(List(("k01",29)), 1)val rddGroup:RDD[(String,Iterable[Int])] = rdd.groupByKey()// (k01,CompactBuffer(3, 26)),(k03,CompactBuffer(2)),(k02,CompactBuffer(6)) note: 如果分组是为了在每个 key 上执行聚合（如求总和或平均值）则使用 reduceByKey 或 aggregateByKey 会有更好的性能。 reduceByKey(func, [numTasks]) 根据key聚合当在（K，V）键值对的数据集上调用时，返回（K，V）键值对的数据集，使用给定的reduce函数 func 聚合每个键的值，该函数类型必须是（V，V）=&gt; V note: 基于combineByKeyWithClassTag 12val rddReduce:RDD[(String,Int)] = rdd.reduceByKey((x,y) =&gt; x + y)// (k01,29),(k03,2),(k02,6) aggregateByKey(zeroValue)(seqOp, combOp, [numPartitions])sortByKey([ascending], [numPartitions]) 根据key排序在（K，V）键值对的数据集调用，其中 K 实现 Ordered 接口，按照升序或降序顺序返回按键排序的（K，V）键值对的数据集 12val rddSortAsc:RDD[(String,Int)] = rdd.sortByKey(true, 1)// (k01,3),(k01,26),(k02,6),(k03,2) 行动Action下面是常见的Action操作 123456789101112131415/* count操作 */rddInt.count()/* countByValue操作 */rddInt.countByValue()/* reduce操作 */rddInt.reduce((x ,y) =&gt; x + y)/* fold操作 */rddInt.fold(0)((x ,y) =&gt; x + y))/* aggregate操作 */val res:(Int,Int) = rddInt.aggregate((0,0))((x,y) =&gt; (x._1 + x._2,y),(x,y) =&gt; (x._1 + x._2,y._1 + y._2))println(res._1 + "," + res._2)/* foeach操作 */rddStr.foreach &#123; x =&gt; println(x) &#125; reduce(func)接收一个函数作为参数，这个函数要操作两个相同元素类型的RDD并返回一个同样类型的新元素 1rddInt.reduce((x ,y) =&gt; x + y) collect()将整个RDD的内容返回 take(n)返回 RDD 中的n个元素，并且尝试只访问尽量少的分区，因此该操作会得到一个不均衡的集合 saveAsTextFile(path)将数据集的元素写入到本地文件系统，HDFS 或任何其他 Hadoop 支持的文件系统中的给定目录的文本文件（或文本文件集合）中 saveAsSequenceFile(path)将数据集的元素写入到本地文件系统，HDFS 或任何其他 Hadoop 支持的文件系统中的给定路径下的 Hadoop SequenceFile中。这在实现 Hadoop 的 Writable 接口的键值对的 RDD 上可用 foreach(func)在数据集的每个元素上运行函数 func Key-Value RDDSpark里创建键值对RDD只可以从内存里读取。所有从文件中读取的RDD都是一般的RDD对象，需要进行转化。 对于Pair RDD常见的聚合操作如：reduceByKey，foldByKey，groupByKey，combineByKey，这些API的定义在 PairRDDFunctions类中。 常见的Key-Value RDD转换操作： 1234567891011121314reduceByKey：合并具有相同键的值；groupByKey：对具有相同键的值进行分组；keys：返回一个仅包含键值的RDD；values：返回一个仅包含值的RDD；sortByKey：返回一个根据键值排序的RDD；flatMapValues：针对Pair RDD中的每个值应用一个返回迭代器的函数，然后对返回的每个元素都生成一个对应原键的键值对记录；mapValues：对Pair RDD里每一个值应用一个函数，但是不会对键值进行操作；combineByKey：使用不同的返回类型合并具有相同键的值；subtractByKey：操作的RDD我们命名为RDD1，参数RDD命名为参数RDD，剔除掉RDD1里和参数RDD中键相同的元素；join：对两个RDD进行内连接；rightOuterJoin：对两个RDD进行连接操作，第一个RDD的键必须存在，第二个RDD的键不再第一个RDD里面有那么就会被剔除掉，相同键的值会被合并；leftOuterJoin：对两个RDD进行连接操作，第二个RDD的键必须存在，第一个RDD的键不再第二个RDD里面有那么就会被剔除掉，相同键的值会被合并；cogroup：将两个RDD里相同键的数据分组在一起sample：对RDD采样； 常见的行动操作： 12345678countByKey：对每个键的元素进行分别计数；collectAsMap：将结果变成一个map；lookup：在RDD里使用键值查找数据take(num):返回RDD里num个元素，随机的；top(num):返回RDD里最前面的num个元素，这个方法实用性还比较高；takeSample：从RDD里返回任意一些元素；sample：对RDD里的数据采样；takeOrdered：从RDD里按照提供的顺序返回最前面的num个元素 combineByKeycombineByKey是Spark中一个比较核心的高级函数，其他一些高阶键值对函数底层都是用它实现的。例如groupByKey,reduceByKey等等 1234567def combineByKey[C]( createCombiner: V =&gt; C, mergeValue: (C, V) =&gt; C, mergeCombiners: (C, C) =&gt; C, partitioner: Partitioner, mapSideCombine: Boolean = true, serializer: Serializer = null) 其中主要的前三个参数如下： createCombiner 函数: 组合器函数，用于将RDD[K,V]中的V转换成一个新的值C1；在找到给定分区中第一次碰到的key（在RDD元素中）时被调用。此方法为这个key初始化一个累加器。 mergeValue 函数：合并值函数，将一个C1类型值和一个V类型值合并成一个C2类型，输入参数为(C1,V)，输出为新的C2; 当累加器已经存在的时候（也就是上面那个key的累加器）调用。 mergeCombiners 函数：合并组合器函数，用于将两个C2类型值合并成一个C3类型，输入参数为(C2,C2)，输出为新的C3。如果哪个key跨多个分区，该参数就会被调用。 举个例子说明计算的流程： 现在有一个Key-Value的Pair RDD，两个分区 12val pairStrRDD = sc.parallelize[(String, Int)](Seq(("coffee", 1), ("coffee", 2), ("tea", 3), ("coffee", 9)), 2)pairStrRDD.glom().collect() 这个RDD有两个分区，第一个分区是((“Coffee”, 1), (“Coffee”, 2))，第二个分区是((“Tea”, 3), (“Coffee”,9)) 然后定义combineByKey的前三个参数 12345678910// 找到给定分区中第一次碰到的key（在RDD元素中）时被调用def createCombiner = (value: Int) =&gt; (value, 1)// 当累加器已经存在的时候（也就是上面那个key的累加器）调用def mergeValue = (acc: (Int, Int), value: Int) =&gt; (acc._1 + value, acc._2 + 1)// 如果哪个key跨多个分区，该参数就会被调用def mergeCombiners = (acc1: (Int, Int), acc2: (Int, Int)) =&gt; (acc1._1 + acc2._1, acc1._2 + acc2._2) 在分区1中，第一次遇到key “Coffee”，createCombiner函数被调用，为”Coffee”产生一个累加器，coffee的值为1，出现次数为1。(1, 1)；第二次遇到key “Coffee”，调用函数mergeValue函数，上个累加器的第一个元素加上这次遇到的value，上个累加器的第二个元素加上1作为次数。(1+2, 1+1)。在分区2中，同理得到两个累加器tea: (3, 1) coffee: (9, 1) “Coffee”这个key值跨越两个分区，函数mergeCombiners被调用，coffee: (3+9, 2+1) 所以最后得到的RDD为 coffee (12, 3), tea (3, 1) 12val testCombineByKeyRDD = pairStrRDD.combineByKey(createCombiner, mergeValue, mergeCombiners)testCombineByKeyRDD.collect() groupByKey的实现上面说过，例如reduceByKey、groupByKey等键值对RDD的转换，都是基于combineByKey的 源码中，可以看出定义了combineByKey的三个参数 createCombiner 将原RDD中的K类型转换为Iterable[V]类型，实现为CompactBuffer mergeValue 将原RDD的元素追加到CompactBuffer中，即将追加操作(+=)视为合并操作 mergeCombiners 针对每个key值所对应的Iterable[V]，提供合并功能(c1 ++= c2) groupByKey函数针对PairRddFunctions的RDD[(K, V)]按照key对value进行分组 RDD Programming Guide 关于Shuffle大多数 Spark 作业的性能主要就是消耗在了 shuffle 环节，因为该环节包含了大量的磁盘IO、序列化、网络数据传输等操作 RDD缓存persist() or cache()]]></content>
      <categories>
        <category>Big Data</category>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Big Data</tag>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka Connect进行数据同步]]></title>
    <url>%2F2018%2F07%2F31%2FKafka-Connect%E8%BF%9B%E8%A1%8C%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%2F</url>
    <content type="text"><![CDATA[Kafka Connect 是一个可扩展、可靠的在 Kafka 和其他系统之间流传输的数据工具。它可以通过 Connectors （连接器）简单、快速的将大集合数据导入和导出 Kafka，数据的导入导出可能有以下几种选择： flume kafka connector kafka 生产者/消费者API 商业ETL工具 Kafka Connect可以将完整的数据库注入到Kafka的Topic中，或者将服务器的系统监控指标注入到Kafka，然后像正常的Kafka流处理机制一样进行数据流处理。 使用Kafka客户端API，是需要在应用程序里面进行开发的，可以根据不同的需求进行数据的操作；而如果需要Kafka连接数据存储系统，则使用例如Connect这种可插拔式的连接器，就会很方便，因为不需要修改系统代码。 构建数据管道需要考虑的问题ETL，现在流行的叫法叫做数据管道，构建一个好的数据管道可能要考虑一下几个方面： 及时性、数据频率要求 可靠性，避免单点故障，并能够自动从各种故障中快速恢复； 高吞吐两盒动态吞吐量 数据格式，需要支持各种不同的数据类型 转换，ETL/ELT 安全性 故障处理能力 耦合性和灵活性 Kafka Connect的工作模式 Standalone：在standalone模式中，所有的worker都在一个独立的进程中完成。 Distributed：distributed模式具有高扩展性，以及提供自动容错机制。你可以使用一个group.ip来启动很多worker进程，在有效的worker进程中它们会自动的去协调执行connector和task，如果你新加了一个worker或者挂了一个worker，其他的worker会检测到然后在重新分配connector和task。 运行Kafka Connect下面以Standalone模式举例，通过下面的命令运行Connect： 1bin/connect-standalone.sh config/connect-standalone.properties Connector1.properties [Connector2.properties ...] 第一个参数是worker的配置，这包括Kafka连接的参数设置，序列化格式，以及频繁地提交offset。 Connect进程有以下几个重要的配置参数： bootstrap.server group.id key.converter/val.converter 其余的参数是 Connector（连接器）配置文件 下面以Elasticsearch-sink作为例子，配置一个自己的connector 123456789name=elasticsearch-sinkconnector.class=io.confluent.connect.elasticsearch.ElasticsearchSinkConnectortasks.max=1topics=logstopic.index.map=logs:logs_indexkey.ignore=trueschema.ignore=trueconnection.url=http://localhost:9200type.name=log name： 连接器唯一的名称，不能重复。 Connector.calss：连接器的Java类。 tasks.max：连接器创建任务的最大数。 Connector.class 配置支持多种格式：全名或连接器类的别名。比如连接器是 org.apache.kafka.connect.file.FileStreamSinkConnector，你可以指定全名，也可以使用 FileStreamSink或FileStreamSinkConnector topics：作为连接器的输入的 topic 列表。 加载Elasticsearch-Sink Connector1confluent load elasticsearch-sink REST API管理ConnectorConfluent提供了一个用于管理 Connector 的 REST API。默认情况下，此服务的端口是8083 GET /Connectors：返回活跃的 Connector 列表 POST /Connectors：创建一个新的 Connector；请求的主体是一个包含字符串name字段和对象 config 字段（Connector 的配置参数）的 JSON 对象。 GET /Connectors/{name}：获取指定 Connector 的信息 GET /Connectors/{name}/config：获取指定 Connector 的配置参数 PUT /Connectors/{name}/config：更新指定 Connector 的配置参数 GET /Connectors/{name}/status：获取 Connector 的当前状态，包括它是否正在运行，失败，暂停等。 GET /Connectors/{name}/tasks：获取当前正在运行的 Connector 的任务列表。 GET /Connectors/{name}/tasks/{taskid}/status：获取任务的当前状态，包括是否是运行中的，失败的，暂停的等， PUT /Connectors/{name}/pause：暂停连接器和它的任务，停止消息处理，直到 Connector 恢复。 PUT /Connectors/{name}/resume：恢复暂停的 Connector（如果 Connector 没有暂停，则什么都不做） POST /Connectors/{name}/restart：重启 Connector（Connector 已故障） POST /Connectors/{name}/tasks/{taskId}/restart：重启单个任务 (通常这个任务已失败) DELETE /Connectors/{name}：删除 Connector, 停止所有的任务并删除其配置 测试使用Connector向Elasticsearch中发送数据首先创建一个Topic：logs 1kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic logs 启动Kafka Connect 1bin/connect-standalone etc/kafka/connect-standalone.properties etc/kafka-connect-elasticsearch/elasticsearch-connect.properties &amp; 向Topic发送一条json数据 123kafka-console-producer --broker-list localhost:9092 --topic logs&gt;&#123;"name":"f1","type":"string"&#125; 1kafka-console-consumer --bootstrap-server localhost:9092 --topic logs --from-beginning 可以看到，刚才的一条数据已经写入到Topic中了 同时，可以看到Elasticsearch中已经存在刚才Topic中的那条数据了 MySQL -&gt; Kafka -&gt; Elasticsearch 准备首先下载数据库的JDBC驱动， MySQL、SQL Server… 启动Confluent 在MySQL中创建测试数据 插入一条测试数据 1insert into sample_data (id, name) values(1,'claus'); 配置Kafka JDBC Source12345678910111213141516171819202122232425262728293031323334353637&#123; "name": "jdbc_source_mysql_sample_01", "config": &#123; "_comment": "The JDBC connector class. Don't change this if you want to use the JDBC Source.", "connector.class": "io.confluent.connect.jdbc.JdbcSourceConnector", "_comment": "How to serialise the value of keys - here use the Confluent Avro serialiser. Note that the JDBC Source Connector always returns null for the key ", "key.converter": "io.confluent.connect.avro.AvroConverter", "_comment": "Since we're using Avro serialisation, we need to specify the Confluent schema registry at which the created schema is to be stored. NB Schema Registry and Avro serialiser are both part of Confluent Open Source.", "key.converter.schema.registry.url": "http://localhost:8081", "_comment": "As above, but for the value of the message. Note that these key/value serialisation settings can be set globally for Connect and thus omitted for individual connector configs to make them shorter and clearer", "value.converter": "io.confluent.connect.avro.AvroConverter", "value.converter.schema.registry.url": "http://localhost:8081", "_comment": " --- JDBC-specific configuration below here --- ", "_comment": "JDBC connection URL. This will vary by RDBMS. Consult your manufacturer's handbook for more information", "connection.url": "jdbc:mysql://localhost:3306/kafka_demo?user=claus&amp;password=LvRaikkonen_0306", "_comment": "Which table(s) to include", "table.whitelist": "sample_data", "_comment": "Pull all rows based on an timestamp column. You can also do bulk or incrementing column-based extracts. For more information, see http://docs.confluent.io/current/connect/connect-jdbc/docs/source_config_options.html#mode", "mode": "timestamp", "_comment": "Which column has the timestamp value to use? ", "timestamp.column.name": "update_ts", "_comment": "If the column is not defined as NOT NULL, tell the connector to ignore this ", "validate.non.null": "false", "_comment": "The Kafka topic will be made up of this prefix, plus the table name ", "topic.prefix": "mysql-" &#125;&#125; 加载Connector并将MySQL数据写入Topic中加载刚才创建的Connector 1bin/confluent load jdbc_source_mysql_sample_01 -d etc/kafka-connect-jdbc/kafka-connect-mysql-jdbc-source.json 可以看见，connector已经被加载并且正在运行 运行Avro Consumer查看Topic中的数据 1kafka-avro-console-consumer --bootstrap-server localhost:9092 --property schema.registry.url=http://localhost:8081 --property print.key=true --from-beginning --topic mysql-sample_data Kafka Connect使用Topic connect-offsets来跟踪数据的偏移量，这里的offset是通过sample_data表的update_ts字段来跟踪并提交的，所以connector即使关闭，下次也可以继续追踪新增数据。 配置ES-sink并将Topic数据导入到ES中12345678910111213141516171819202122232425&#123; "name": "es-sink-mysql-sampleData-01", "config": &#123; "_comment": "-- standard converter stuff -- this can actually go in the worker config globally --", "connector.class": "io.confluent.connect.elasticsearch.ElasticsearchSinkConnector", "value.converter": "io.confluent.connect.avro.AvroConverter", "key.converter": "io.confluent.connect.avro.AvroConverter", "key.converter.schema.registry.url": "http://localhost:8081", "value.converter.schema.registry.url": "http://localhost:8081", "_comment": "--- Elasticsearch-specific config ---", "_comment": "Elasticsearch server address", "connection.url": "http://localhost:9200", "_comment": "Elasticsearch mapping name. Gets created automatically if doesn't exist ", "type.name": "type.name=kafka-connect", "_comment": "Which topic to stream data from into Elasticsearch", "topics": "mysql-sample_data", "_comment": "If the Kafka message doesn't have a key (as is the case with JDBC source) you need to specify key.ignore=true. If you don't, you'll get an error from the Connect task: 'ConnectException: Key is used as document id and can not be null.", "key.ignore": "true" &#125;&#125; 可以看到ES中已经创建名为mysql-sample_data的Index，并且Topic中的4条数据已经导入到ES中了。 开发ConnectorTBD]]></content>
      <categories>
        <category>Big Data</category>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka消费者基础使用]]></title>
    <url>%2F2018%2F07%2F25%2FKafka%E6%B6%88%E8%B4%B9%E8%80%85%E5%9F%BA%E7%A1%80%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[Kafka消费者相关概念消费者与消费者组消费者读取过程 创建消费者对象 -&gt; 订阅主题 -&gt; 读取消息 -&gt; 验证消息 -&gt; 保存消息 Kafka消费者从属于消费者群组。一个群组里的消费者订阅的是同一个主题，每个消费者接收主题一部分分区的消息。消费者加入群组或者由于关闭或崩溃导致消费者离开群组时候，分区的所有权需要从一个消费者转移到另一个消费者，这样的行为被称为再均衡 群组协调器 对于每一个消费者组，都会从所有的broker中选取一个作为消费者组协调器(group coordinator)，负责维护和管理这个消费者组的状态，它的主要工作是当一个consumer加入、一个consumer离开（挂掉或者手动停止等）或者topic的partition改变时重新进行partition分配(再均衡) Offset偏移量管理提交：更新分区当前读取位置的操作叫做提交 偏移量：消息在分区中的位置，决定了消费者下次开始读取消息的位置 如果提交偏移量小于当前处理的消息位置，则两个之间的消息会被再次处理； 如果提交偏移量大于当前处理的消息位置，则两个之间的消息会丢失。 Last Committed Offset：这是 group 最新一次 commit 的 offset，表示这个 group 已经把 Last Committed Offset 之前的数据都消费成功了； Current Position：group 当前消费数据的 offset，也就是说，Last Committed Offset 到 Current Position 之间的数据已经拉取成功，可能正在处理，但是还未 commit； Log End Offset：Producer 写入到 Kafka 中的最新一条数据的 offset； High Watermark：已经成功备份到其他 replicas 中的最新一条数据的 offset，也就是说 Log End Offset 与 High Watermark 之间的数据已经写入到该 partition 的 leader 中，但是还未成功备份到其他的 replicas 中，这部分数据被认为是不安全的，是不允许 Consumer 消费的 __consumer_offsets 是 Kafka 内部使用的一个 topic，专门用来存储 group 消费的情况，默认情况下有50个 partition，每个 partition 三副本，而具体 group 的消费情况要存储到哪一个 partition 上，是根据 abs(GroupId.hashCode()) % NumPartitions 来计算（其中，NumPartitions 是__consumer_offsets 的 partition 数，默认是50个）的 消费者的初始化和配置下面是一个消费者的最基本配置 123456Properties props = new Properties();props.put("bootstrap.servers", "localhost:9092"); // 通过其中的一台broker来找到group的coordinator，并不需要列出所有的brokerprops.put("group.id", "consumer-group-simple");props.put("key.deserializer", StringDeserializer.class.getName());props.put("value.deserializer", StringDeserializer.class.getName());KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props); // consumer实例 订阅Topic创建好消费者之后，接下来调用subscribe() 方法来订阅Topic 1consumer.subscribe(Collections.singleton("topicList")); 轮询请求数据轮询是消费者API的核心，在循环中调用poll方法，轮询就会处理所有的细节，包括群组协调、分区再均衡、发送心跳和获取数据。 123456789try &#123; while (running) &#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(1000); for (ConsumerRecord&lt;String, String&gt; record : records) System.out.println(record.offset() + ": " + record.value()); &#125;&#125; finally &#123; consumer.close();&#125; poll()方法返回一个记录列表。每条记录都包含了记录所属主题的信息、记录所在分区的信息、记录在分区里的偏移量，以及记录的键值对。 偏移量管理当一个消费者群组刚开始被创建的时候，最初的offset是通过auto.offset.reset配置项来进行设置的。一旦消费者开始处理数据，它根据应用的需要来定期地对offset进行commit。在每一次的再均衡之后，群组会将这个offset设置为Last Committed Offset 消费者API提供了很多种方式来提交偏移量 自动提交如果enable.auto.commit被设为true，那么每过5s(这个时长可以通过auto.commit.interval.ms来进行配置)，消费者会自动把从poll() 方法接收到的最大偏移量提交上去。 12props.put("enable.auto.commit", "true"); // 自动commitprops.put("auto.commit.interval.ms", "1000"); // 自动commit的间隔 在使用自动提交时，每次调用轮询方法都会把上一次调用返回的偏移量提交上去，它并不知道具体哪些消息已经被处理了。如果使用默认的自动commit机制，系统是保证at least once消息处理，因为offset是在这些messages被应用处理后才进行commit的 手动提交把auto.commit.offset设为false，让应用程序决定何时提交偏移量。使用commitSync()同步提交偏移量，API也提供了commitAsync()方法异步提交偏移量。这个API 会提交由poll() 方法返回的最新偏移量，提交成功后马上返回，如果提交失败就抛出异常 1234567891011121314try &#123; while (running) &#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(1000); for (ConsumerRecord&lt;String, String&gt; record : records) System.out.println(record.offset() + ": " + record.value()); try &#123; consumer.commitSync(); &#125; catch (CommitFailedException e) &#123; // application specific failure handling &#125; &#125;&#125; finally &#123; consumer.close();&#125; 从特定偏移量处开始读取Kafka提供了用于查找特定偏移量的API，通过seek()来指定分区位移开始消费 1234567891011121314151617181920212223242526272829303132public class SaveOffsetsOnRebalance implements ConsumerRebalanceListener &#123; public void onPartitionsRevoked(Collection&lt;TopicPartition&gt; partitions) &#123; //在消费者负责的分区被回收前提交数据库事务，保存消费的记录和位移 commitDBTransaction(); &#125; public void onPartitionsAssigned(Collection&lt;TopicPartition&gt; partitions) &#123; //在开始消费前，从数据库中获取分区的位移，并使用seek()来指定开始消费的位移 for(TopicPartition partition: partitions) consumer.seek(partition, getOffsetFromDB(partition)); &#125; &#125; consumer.subscribe(topics, new SaveOffsetOnRebalance(consumer)); //在subscribe()之后poll一次，并从数据库中获取分区的位移，使用seek()来指定开始消费的位移 consumer.poll(0); for (TopicPartition partition: consumer.assignment()) consumer.seek(partition, getOffsetFromDB(partition)); while (true) &#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); for (ConsumerRecord&lt;String, String&gt; record : records) &#123; processRecord(record); //保存记录结果 storeRecordInDB(record); //保存位移 storeOffsetInDB(record.topic(), record.partition(), record.offset()); &#125; //提交数据库事务，保存消费的记录以及位移 commitDBTransaction(); &#125;]]></content>
      <categories>
        <category>Big Data</category>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(转)Kafka生产者发送消息流程]]></title>
    <url>%2F2018%2F07%2F24%2FKafka%E7%94%9F%E4%BA%A7%E8%80%85%E5%8F%91%E9%80%81%E6%B6%88%E6%81%AF%E6%B5%81%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[参考Kafka 源码解析之 Producer 发送模型（一） 一个简单的生产者下面的代码是一个简单的生产者向Kafka中发送消息的例子： 1234567891011121314151617181920212223import org.apache.kafka.clients.producer.KafkaProducer;import org.apache.kafka.clients.producer.Producer;import org.apache.kafka.clients.producer.ProducerRecord;import java.util.Properties;public class SimpleProducer &#123; public static void main(String[] args)&#123; Properties props = new Properties(); props.put("bootstrap.servers", "localhost:9092"); props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer"); props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer"); Producer&lt;String, String&gt; producer = new KafkaProducer&lt;&gt;(props); for (int i = 0; i &lt; 10; i++)&#123; String msg = "This is message " + i; ProducerRecord&lt;String, String&gt; pr = new ProducerRecord&lt;&gt;("testTopic", msg); producer.send(pr); &#125; producer.close(); &#125;&#125; Kafka提供了生产者API，使用时候需要实例化KafkaProducer，然后调用send方法发送数据。 生产者数据发送流程下面的流程图展示了消息的发送流程 首先创建一个ProducerRecord对象，包含了目标主题和要发送的内容。然后数据被发送给序列化器，将键和值对象序列化成字节数据。然后数据发送给分区器，如果ProducerRecord里面指定了分区，分区器不做任何操作，否则根据ProducerRecord对象的键来选择一个分区。然后上面的记录被追加到一个记录批次里面，有一个独立的线程将记录批次发送到相应的broker上。 实例化了生产者对象之后，调用send方法发送数据 12345678910111213// 异步向Topic发送数据@Overridepublic Future&lt;RecordMetadata&gt; send(ProducerRecord&lt;K, V&gt; record) &#123; return send(record, null);&#125;// 异步向Topic发送数据，发送确认后调用回调函数@Overridepublic Future&lt;RecordMetadata&gt; send(ProducerRecord&lt;K, V&gt; record, Callback callback) &#123; // intercept the record, which can be potentially modified; this method does not throw exceptions ProducerRecord&lt;K, V&gt; interceptedRecord = this.interceptors.onSend(record); return doSend(interceptedRecord, callback);&#125; doSend的具体实现12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182/** * Implementation of asynchronously send a record to a topic. */private Future&lt;RecordMetadata&gt; doSend(ProducerRecord&lt;K, V&gt; record, Callback callback) &#123; TopicPartition tp = null; try &#123; // 1.确认数据要发送到的topic的 metadata 是可用的 ClusterAndWaitTime clusterAndWaitTime = waitOnMetadata(record.topic(), record.partition(), maxBlockTimeMs); long remainingWaitMs = Math.max(0, maxBlockTimeMs - clusterAndWaitTime.waitedOnMetadataMs); Cluster cluster = clusterAndWaitTime.cluster; // 2.序列化key和value byte[] serializedKey; try &#123; serializedKey = keySerializer.serialize(record.topic(), record.headers(), record.key()); &#125; catch (ClassCastException cce) &#123; throw new SerializationException("Can't convert key of class " + record.key().getClass().getName() + " to class " + producerConfig.getClass(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG).getName() + " specified in key.serializer", cce); &#125; byte[] serializedValue; try &#123; serializedValue = valueSerializer.serialize(record.topic(), record.headers(), record.value()); &#125; catch (ClassCastException cce) &#123; throw new SerializationException("Can't convert value of class " + record.value().getClass().getName() + " to class " + producerConfig.getClass(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG).getName() + " specified in value.serializer", cce); &#125; // 3.获取消息的partition值 int partition = partition(record, serializedKey, serializedValue, cluster); tp = new TopicPartition(record.topic(), partition); setReadOnly(record.headers()); Header[] headers = record.headers().toArray(); int serializedSize = AbstractRecords.estimateSizeInBytesUpperBound(apiVersions.maxUsableProduceMagic(), compressionType, serializedKey, serializedValue, headers); ensureValidRecordSize(serializedSize); long timestamp = record.timestamp() == null ? time.milliseconds() : record.timestamp(); log.trace("Sending record &#123;&#125; with callback &#123;&#125; to topic &#123;&#125; partition &#123;&#125;", record, callback, record.topic(), partition); // producer callback will make sure to call both 'callback' and interceptor callback Callback interceptCallback = new InterceptorCallback&lt;&gt;(callback, this.interceptors, tp); if (transactionManager != null &amp;&amp; transactionManager.isTransactional()) transactionManager.maybeAddPartitionToTransaction(tp); // 4.向累加器中追加数据 RecordAccumulator.RecordAppendResult result = accumulator.append(tp, timestamp, serializedKey, serializedValue, headers, interceptCallback, remainingWaitMs); // 5.如果batch满了，唤醒sender线程发送数据 if (result.batchIsFull || result.newBatchCreated) &#123; log.trace("Waking up the sender since topic &#123;&#125; partition &#123;&#125; is either full or getting a new batch", record.topic(), partition); this.sender.wakeup(); &#125; return result.future; // handling exceptions and record the errors; // for API exceptions return them in the future, // for other exceptions throw directly &#125; catch (ApiException e) &#123; log.debug("Exception occurred during message send:", e); if (callback != null) callback.onCompletion(null, e); this.errors.record(); this.interceptors.onSendError(record, tp, e); return new FutureFailure(e); &#125; catch (InterruptedException e) &#123; this.errors.record(); this.interceptors.onSendError(record, tp, e); throw new InterruptException(e); &#125; catch (BufferExhaustedException e) &#123; this.errors.record(); this.metrics.sensor("buffer-exhausted-records").record(); this.interceptors.onSendError(record, tp, e); throw e; &#125; catch (KafkaException e) &#123; this.errors.record(); this.interceptors.onSendError(record, tp, e); throw e; &#125; catch (Exception e) &#123; // we notify interceptor about all exceptions, since onSend is called before anything else in this method this.interceptors.onSendError(record, tp, e); throw e; &#125;&#125; 上面doSend方法中，发送数据总共分5步： 确认数据要发送到的 topic 的 metadata 是可用的 序列化key和value 获取消息的partition值 向累加器中追加数据，数据先进行缓存 如果batch满了，唤醒sender线程发送数据 序列化生产者端对数据的key和value进行序列化操作，消费者端再进行相应的反序列化操作，下面是Kafka提供的序列化器和反序列化器 对于自带的序列化器不能满足需求的情况，可以使用例如Avro、Thrift或者Protobuf等序列化框架或者使用自定义的序列化器。 下面使用Avro序列化记录 首先使用Avro命令生成自定义对象，.avsc文件通过JSON来描述数据的schema 1234567891011121314&#123; &quot;type&quot;: &quot;record&quot;, &quot;namespace&quot;: &quot;com.example&quot;, &quot;name&quot;: &quot;Customer&quot;, &quot;version&quot;: &quot;1&quot;, &quot;fields&quot;: [ &#123; &quot;name&quot;: &quot;first_name&quot;, &quot;type&quot;: &quot;string&quot;, &quot;doc&quot;: &quot;First Name of Customer&quot; &#125;, &#123; &quot;name&quot;: &quot;last_name&quot;, &quot;type&quot;: &quot;string&quot;, &quot;doc&quot;: &quot;Last Name of Customer&quot; &#125;, &#123; &quot;name&quot;: &quot;age&quot;, &quot;type&quot;: &quot;int&quot;, &quot;doc&quot;: &quot;Age at the time of registration&quot; &#125;, &#123; &quot;name&quot;: &quot;height&quot;, &quot;type&quot;: &quot;float&quot;, &quot;doc&quot;: &quot;Height at the time of registration in cm&quot; &#125;, &#123; &quot;name&quot;: &quot;weight&quot;, &quot;type&quot;: &quot;float&quot;, &quot;doc&quot;: &quot;Weight at the time of registration in kg&quot; &#125;, &#123; &quot;name&quot;: &quot;automated_email&quot;, &quot;type&quot;: &quot;boolean&quot;, &quot;default&quot;: true, &quot;doc&quot;: &quot;Field indicating if the user is enrolled in marketing emails&quot; &#125; ]&#125; 生成自定义的数据对象后，将schema保存在注册表中，这样消费者在读取数据的时候就知道用什么schema来反序列化记录。 下面是使用avro的KafkaAvroSerializer来序列化对象，需要制定schema注册表的位置http://127.0.0.1:8081 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455import io.confluent.kafka.serializers.KafkaAvroSerializer;import org.apache.kafka.clients.producer.Callback;import org.apache.kafka.clients.producer.KafkaProducer;import org.apache.kafka.clients.producer.ProducerRecord;import org.apache.kafka.clients.producer.RecordMetadata;import org.apache.kafka.common.serialization.StringSerializer;import java.util.Properties;public class KafkaAvroProducer &#123; public static void main(String[] args)&#123; Properties properties = new Properties(); properties.setProperty("bootstrap.servers", "localhost:9092"); properties.setProperty("acks", "1"); properties.setProperty("retries", "10"); properties.setProperty("key.serializer", StringSerializer.class.getName()); properties.setProperty("value.serializer", KafkaAvroSerializer.class.getName()); properties.setProperty("schema.registry.url", "http://127.0.0.1:8081"); String topic = "customer-avro"; KafkaProducer&lt;String, Customer&gt; kafkaProducer = new KafkaProducer&lt;String, Customer&gt;(properties); Customer customer = Customer.newBuilder() .setFirstName("Mike") .setLastName("Tom") .setAge(30) .setHeight(171.0f) .setWeight(120.5f) .setAutomatedEmail(false) .build(); ProducerRecord&lt;String, Customer&gt; producerRecord = new ProducerRecord&lt;String, Customer&gt;( topic, customer ); kafkaProducer.send(producerRecord, new Callback() &#123; @Override public void onCompletion(RecordMetadata recordMetadata, Exception e) &#123; if (e == null)&#123; System.out.println("Success"); System.out.println(recordMetadata.toString()); &#125;else &#123; e.printStackTrace(); &#125; &#125; &#125;); kafkaProducer.flush(); kafkaProducer.close(); &#125;&#125; 更具体的Avro序列化，查看Avro的相关文档。 获取分区看一下partition方法的实现： 1234private int partition(ProducerRecord&lt;K, V&gt; record, byte[] serializedKey, byte[] serializedValue, Cluster cluster) &#123; Integer partition = record.partition(); return partition != null ? partition.intValue() : this.partitioner.partition(record.topic(), record.key(), serializedKey, record.value(), serializedValue, cluster);&#125; 如果record指定了分区则指定的分区会被使用，如果没有则使用partitioner分区器来选择分区 123456789101112131415161718192021222324252627282930313233public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) &#123; List&lt;PartitionInfo&gt; partitions = cluster.partitionsForTopic(topic); int numPartitions = partitions.size(); // key为空 if (keyBytes == null) &#123; // 根据topic名获取上次计算分区时使用的一个整数并加一 int nextValue = this.nextValue(topic); List&lt;PartitionInfo&gt; availablePartitions = cluster.availablePartitionsForTopic(topic); if (availablePartitions.size() &gt; 0) &#123; // 如果大于0则使用获取的nextValue的值和可用分区数进行取模操作 int part = Utils.toPositive(nextValue) % availablePartitions.size(); return ((PartitionInfo)availablePartitions.get(part)).partition(); &#125; else &#123; return Utils.toPositive(nextValue) % numPartitions; &#125; &#125; else &#123; // 如果消息的key不为null，就根据hash算法murmur2就算出key的hash值，然后和分区数进行取模运算。 return Utils.toPositive(Utils.murmur2(keyBytes)) % numPartitions; &#125;&#125;private int nextValue(String topic) &#123; AtomicInteger counter = (AtomicInteger)this.topicCounterMap.get(topic); if (null == counter) &#123;// 第一次调用时，随机产生 counter = new AtomicInteger(ThreadLocalRandom.current().nextInt()); AtomicInteger currentCounter = (AtomicInteger)this.topicCounterMap.putIfAbsent(topic, counter); if (currentCounter != null) &#123; counter = currentCounter; &#125; &#125; return counter.getAndIncrement();&#125; 向累加器追加数据生产者向累加器中追加数据，当batch满了，唤醒sender线程发送数据 Producer 通过RecordAccumulator实例追加数据，每个 TopicPartition 都会对应一个 Deque，当添加数据时，会向其 topic-partition 对应的这个 queue 最新创建的一个 RecordBatch 中添加 record，而发送数据时，则会先从 queue 中最老的那个 RecordBatch 开始发送。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263public RecordAppendResult append(TopicPartition tp, long timestamp, byte[] key, byte[] value, Header[] headers, Callback callback, long maxTimeToBlock) throws InterruptedException &#123; // We keep track of the number of appending thread to make sure we do not miss batches in // abortIncompleteBatches(). appendsInProgress.incrementAndGet(); ByteBuffer buffer = null; if (headers == null) headers = Record.EMPTY_HEADERS; try &#123; // 获取该 topic-partition 对应的 queue Deque&lt;ProducerBatch&gt; dq = getOrCreateDeque(tp);// 每个 topicPartition 对应一个 queue synchronized (dq) &#123; if (closed) throw new IllegalStateException("Cannot send after the producer is closed."); // 向 queue 中追加数据, 先获取 queue 中最新加入的那个 RecordBatch, // 如果不存在或者存在但剩余空余不足以添加本条 record 则返回 null RecordAppendResult appendResult = tryAppend(timestamp, key, value, headers, callback, dq); if (appendResult != null) return appendResult; &#125; // we don't have an in-progress record batch try to allocate a new batch byte maxUsableMagic = apiVersions.maxUsableProduceMagic(); // 为 topic-partition 创建一个新的 RecordBatch 缓冲区, 需要初始化相应的 RecordBatch，要为其分配的大小是: max（batch.size, 加上头文件的本条消息的大小） int size = Math.max(this.batchSize, AbstractRecords.estimateSizeInBytesUpperBound(maxUsableMagic, compression, key, value, headers)); log.trace("Allocating a new &#123;&#125; byte message buffer for topic &#123;&#125; partition &#123;&#125;", size, tp.topic(), tp.partition()); buffer = free.allocate(size, maxTimeToBlock); synchronized (dq) &#123; // Need to check if producer is closed again after grabbing the dequeue lock. if (closed) throw new IllegalStateException("Cannot send after the producer is closed."); RecordAppendResult appendResult = tryAppend(timestamp, key, value, headers, callback, dq); if (appendResult != null) &#123; // Somebody else found us a batch, return the one we waited for! Hopefully this doesn't happen often... return appendResult; &#125; // 给 topic-partition 创建一个 RecordBatch MemoryRecordsBuilder recordsBuilder = recordsBuilder(buffer, maxUsableMagic); ProducerBatch batch = new ProducerBatch(tp, recordsBuilder, time.milliseconds()); // 向新的 RecordBatch 中追加数据 FutureRecordMetadata future = Utils.notNull(batch.tryAppend(timestamp, key, value, headers, callback, time.milliseconds())); dq.addLast(batch); incomplete.add(batch); // Don't deallocate this buffer in the finally block as it's being used in the record batch buffer = null; // 如果 dp.size()&gt;1 就证明这个 queue 有一个 batch 是可以发送了 return new RecordAppendResult(future, dq.size() &gt; 1 || batch.isFull(), true); &#125; &#125; finally &#123; if (buffer != null) free.deallocate(buffer); appendsInProgress.decrementAndGet(); &#125;&#125; 获取该 topic-partition 对应的 queue，没有的话会创建一个空的 queue； 向 queue 中追加数据，先获取 queue 中最新加入的那个 RecordBatch，如果不存在或者存在但剩余空余不足以添加本条 record 则返回 null，成功写入的话直接返回结果，写入成功； 如果不存在现有的RecordBatch，创建一个新的 RecordBatch，初始化内存大小根据 max(batch.size, Records.LOG_OVERHEAD + Record.recordSize(key, value)) 来确定（防止单条 record 过大的情况）； 向新建的 RecordBatch 写入 record，并将 RecordBatch 添加到 queue 中，返回结果，写入成功。 发送RecordBatch到Broker当 record 写入成功后，如果发现 RecordBatch 已满足发送的条件（通常是 queue 中有多个 batch，那么最先添加的那些 batch 肯定是可以发送了），那么就会唤醒 sender 线程，发送 RecordBatch 123456789101112131415161718192021222324252627282930313233343536373839404142private void sendProduceRequests(Map&lt;Integer, List&lt;ProducerBatch&gt;&gt; collated, long now) &#123; for (Map.Entry&lt;Integer, List&lt;ProducerBatch&gt;&gt; entry : collated.entrySet()) sendProduceRequest(now, entry.getKey(), acks, requestTimeout, entry.getValue());&#125;private void sendProduceRequest(long now, int destination, short acks, int timeout, List&lt;ProducerBatch&gt; batches) &#123; if (batches.isEmpty()) return; Map&lt;TopicPartition, MemoryRecords&gt; produceRecordsByPartition = new HashMap&lt;&gt;(batches.size()); final Map&lt;TopicPartition, ProducerBatch&gt; recordsByPartition = new HashMap&lt;&gt;(batches.size()); // find the minimum magic version used when creating the record sets byte minUsedMagic = apiVersions.maxUsableProduceMagic(); for (ProducerBatch batch : batches) &#123; if (batch.magic() &lt; minUsedMagic) minUsedMagic = batch.magic(); &#125; for (ProducerBatch batch : batches) &#123; TopicPartition tp = batch.topicPartition; MemoryRecords records = batch.records(); if (!records.hasMatchingMagic(minUsedMagic)) records = batch.records().downConvert(minUsedMagic, 0, time).records(); produceRecordsByPartition.put(tp, records); recordsByPartition.put(tp, batch); &#125; ProduceRequest.Builder requestBuilder = ProduceRequest.Builder.forMagic(minUsedMagic, acks, timeout, produceRecordsByPartition, transactionalId); RequestCompletionHandler callback = new RequestCompletionHandler() &#123; public void onComplete(ClientResponse response) &#123; handleProduceResponse(response, recordsByPartition, time.milliseconds()); &#125; &#125;; String nodeId = Integer.toString(destination); ClientRequest clientRequest = client.newClientRequest(nodeId, requestBuilder, now, acks != 0, callback); client.send(clientRequest, now); log.trace("Sent produce request to &#123;&#125;: &#123;&#125;", nodeId, requestBuilder);&#125;]]></content>
      <categories>
        <category>Big Data</category>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据仓库分层]]></title>
    <url>%2F2018%2F06%2F07%2F%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E5%88%86%E5%B1%82%2F</url>
    <content type="text"><![CDATA[在数据仓库实际构建过程中，往往都是快猛糙地直接接入业务系统，订制ETL开发，然后简单进行维度建模，满足业务方的报表分析需求，到这里就建立了数据仓库1.0版本。但是这时候的数据仓库很脆弱，业务方的任何需求变更、以及任何环节的数据质量问题，都会导致整个流程发生剧烈的变动，这时候就需要进行数据分层。数据分层的好处如下： 清晰数据结构：每一个数据分层都有它的作用域，这样我们在使用表的时候能更方便地定位和理解。 数据血缘追踪：简单来讲可以这样理解，我们最终给业务诚信的是一能直接使用的张业务表，但是它的来源有很多，如果有一张来源表出问题了，我们希望能够快速准确地定位到问题，并清楚它的危害范围。 减少重复开发：规范数据分层，开发一些通用的中间层数据，能够减少极大的重复计算。 把复杂问题简单化。讲一个复杂的任务分解成多个步骤来完成，每一层只处理单一的步骤，比较简单和容易理解。而且便于维护数据的准确性，当数据出现问题之后，可以不用修复所有的数据，只需要从有问题的步骤开始修复。 屏蔽原始数据的异常。 屏蔽业务的影响，不必改一次业务就需要重新接入数据。 数据仓库分层架构逻辑上以把数据仓库分为下面三个层：数据运营层(ODS)、数据仓库层(DW)和数据产品层(APP) ODS数据运营层数据运营层，也叫ODS层，是最接近数据源中数据的一层，数据源中的数据，经过抽取、洗净、传输，也就说传说中的 ETL 之后，装入本层。本层的数据，总体上大多是按照源头业务系统的分类方式而分类的。 但是，这一层面的数据却不等同于原始数据。在源数据装入这一层时，要进行诸如去噪（例如有一条数据中人的年龄是 300 岁，这种属于异常数据，就需要提前做一些处理）、去重（例如在个人资料表中，同一 ID 却有两条重复数据，在接入的时候需要做一步去重）、字段命名规范等一系列操作。 DW数据层库层在这里，从 ODS 层中获得的数据按照主题基于维度建模理论建立各种数据模型，DW层的数据是ODS层的数据通过ETL清洗、转换、加载生成的。此外，基于性能、重复计算和使用便捷性考虑， DW层除了保存基于维度建模的最细粒度的事实表和维度表，还会基于它们生成一层汇总数据(DW汇总层)。 APP数据应用层在DW层的基础上，各个业务方可以建立自己的数据集市，主要是提供给数据产品和数据分析使用的数据，一般会存放在 ES、Mysql 等系统中供线上系统使用，也可能会存在 Hive 或者 Druid 中供数据分析和数据挖掘使用。原则上不允许应用层直接访问ODS层。 转自木东居士——如何优雅地设计数据分层]]></content>
      <tags>
        <tag>数据仓库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL Binlog以及Canal简单使用]]></title>
    <url>%2F2018%2F05%2F16%2FMySql-binlog%2F</url>
    <content type="text"><![CDATA[在ETL过程中，如何获取增量数据是完成ETL过程的主要解决问题，前面文章ETL增量数据的捕获中，使用日志对比的方法是一种比较好的获取增量数据流的方式，那篇文章里主要介绍了SQL Server的Change Data Capture这么个功能来完成增量数据的获取任务，这篇文章介绍一下对于MySQL来说，如何通过读取binlog来获取增量数据。 MySQL的日志MySQL 的日志包括错误日志（ErrorLog），更新日志（Update Log），二进制日志（Binlog），查询日志（Query Log），慢查询日志（Slow Query Log）等。在默认情况下，系统仅仅打开错误日志，关闭了其他所有日志。 开启BinlogMySQL 默认没有开启 Binlog，下面要开启Binlog。mac上使用brew安装的mysql，默认安装后的目录是/usr/local/Cellar，在这个目录中没有MySQL的配置文件my.cnf，在系统中查看my.cnf的路径 1mysql --help --verbose | grep my.cnf 可以看到MySQL是按照这个顺序来读取配置文件的，下面在目录/etc/下新建文件my.cnf`，并添加如下来开启Binlog 12345[mysqld]#log_binlog-bin = mysql-bin #开启binlogbinlog-format = ROW #选择row模式server_id = 1 #配置mysql replication需要定义，不能和canal的slaveId重复 重启MySQL服务 1mysql.server restart 查看Binlog是否开启 1show variables like "%log_bin%"; 简单查看Binlog内容首先创建一个表test，然后向表中插入两条数据 123create table test(id int, name varchar(20)) engine=innodb charset=utf8;insert into test(id, name) values(1, "aaa");insert into test(id, name) values(2, "bbb"); 显示当前使用的Binlog及所处位置，然后查看这个二进制文件内容 12show master status;show binlog events in &quot;mysql-bin.000002&quot;; 使用MySQL内置的mysqlbinlog工具查看和操作Binlog1$ mysqlbinlog -v mysql-bin.000002 可以看到刚才创建表和插入数据的操作都被记录下来了。关于Binlog更详细内容，参考MySQL官方文档The Binary Log 使用Canal监听MySQL增量数据Canal是阿里开源的MySQL数据库Binlog的增量订阅&amp;消费组件 MySQL的Binlog有两个重要用途：主从复制和数据恢复，关于主从复制的内容，参考MySQL主备复制原理、实现及异常处理 这里其实利用了MySQL支持主从复制的原理，Master产生Binlog记录增删改语句，通过将Binlog发送到Slave节点从而完成Binlog的解析。下图是原理图 Canal是CS结构 安装配置Canal服务端下载canal releases包，解压之后，编辑conf/example文件夹下的instance.properties： 1234567891011121314151617181920212223242526272829303132################################################### mysql serverIdcanal.instance.mysql.slaveId=1234# position infocanal.instance.master.address=127.0.0.1:3306canal.instance.master.journal.name=canal.instance.master.position=canal.instance.master.timestamp=# table meta tsdb infocanal.instance.tsdb.enable=truecanal.instance.tsdb.dir=$&#123;canal.file.data.dir:../conf&#125;/$&#123;canal.instance.destination:&#125;canal.instance.tsdb.url=jdbc:h2:$&#123;canal.instance.tsdb.dir&#125;/h2;CACHE_SIZE=1000;MODE=MYSQL;#canal.instance.tsdb.url=jdbc:mysql://127.0.0.1:3306/canal_tsdbcanal.instance.tsdb.dbUsername=canalcanal.instance.tsdb.dbPassword=canal#canal.instance.standby.address =#canal.instance.standby.journal.name =#canal.instance.standby.position = #canal.instance.standby.timestamp = # username/passwordcanal.instance.dbUsername=canalcanal.instance.dbPassword=canalcanal.instance.defaultDatabaseName=tutorialscanal.instance.connectionCharset=UTF-8# table regexcanal.instance.filter.regex=.*\\..*# table black regexcanal.instance.filter.black.regex=################################################# address设置为mysql的连接地址，defaultDatabaseName设置为自己要监听的库名，这里是tutorial 在MySQL命令行，创建一个新用户，作为slave，这个用户对应配置文件里的dbUsername 1234CREATE USER canal IDENTIFIED BY 'canal!QAZ2wsx'; GRANT SELECT, REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO 'canal'@'%';-- GRANT ALL PRIVILEGES ON *.* TO 'canal'@'%' ;FLUSH PRIVILEGES; 启动 1sh bin/startup.sh 启动后可以在logs目录下查看日志 编写Canal客户端程序新建一个java maven项目，pom.xml里添加依赖 12345&lt;dependency&gt; &lt;groupId&gt;com.alibaba.otter&lt;/groupId&gt; &lt;artifactId&gt;canal.client&lt;/artifactId&gt; &lt;version&gt;1.0.25&lt;/version&gt;&lt;/dependency&gt; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697import com.alibaba.otter.canal.client.CanalConnector;import com.alibaba.otter.canal.client.CanalConnectors;import com.alibaba.otter.canal.protocol.CanalEntry;import com.alibaba.otter.canal.protocol.Message;import java.net.InetSocketAddress;import java.util.List;/** * @author lvshuo * @create 2018/5/17 * @since 1.0.0 */public class MainApp &#123; public static void main(String[] args) throws Exception &#123; CanalConnector connector = CanalConnectors.newSingleConnector( new InetSocketAddress("127.0.0.1", 11111), "example", "", ""); int batchSize = 1000; int emptyCount = 0; try &#123; connector.connect(); connector.subscribe(".*\\..*"); connector.rollback(); int totalEmptyCount = 120; while (emptyCount &lt; totalEmptyCount) &#123; Message message = connector.getWithoutAck(batchSize); // 获取指定数量的数据 long batchId = message.getId(); int size = message.getEntries().size(); if (batchId == -1 || size == 0) &#123; emptyCount++; System.out.println("empty count : " + emptyCount); try &#123; Thread.sleep(10000); &#125; catch (InterruptedException e) &#123; &#125; &#125; else &#123; emptyCount = 0; // System.out.printf("message[batchId=%s,size=%s] \n", batchId, size); printEntry(message.getEntries ()); &#125; connector.ack(batchId); // 提交确认 // connector.rollback(batchId); // 处理失败, 回滚数据 &#125; System.out.println("empty too many times, exit"); &#125; finally &#123; connector.disconnect(); &#125; &#125; private static void printEntry(List&lt;CanalEntry.Entry&gt; entrys) &#123; for (CanalEntry.Entry entry : entrys) &#123; if (entry.getEntryType() == CanalEntry.EntryType.TRANSACTIONBEGIN || entry.getEntryType() == CanalEntry .EntryType .TRANSACTIONEND) &#123; continue; &#125; CanalEntry.RowChange rowChage = null; try &#123; rowChage = CanalEntry.RowChange.parseFrom(entry.getStoreValue()); &#125; catch (Exception e) &#123; throw new RuntimeException("ERROR ## parser of eromanga-event has an error , data:" + entry.toString(), e); &#125; CanalEntry.EventType eventType = rowChage.getEventType(); System.out.println(String.format("================&gt; binlog[%s:%s] , name[%s,%s] , eventType : %s", entry.getHeader().getLogfileName(), entry.getHeader().getLogfileOffset(), entry.getHeader().getSchemaName(), entry.getHeader().getTableName(), eventType)); for (CanalEntry.RowData rowData : rowChage.getRowDatasList()) &#123; if (eventType == CanalEntry.EventType.DELETE) &#123; printColumn(rowData.getBeforeColumnsList()); &#125; else if (eventType == CanalEntry.EventType.INSERT) &#123; printColumn(rowData.getAfterColumnsList()); &#125; else &#123; System.out.println("-------&gt; before"); printColumn(rowData.getBeforeColumnsList()); System.out.println("-------&gt; after"); printColumn(rowData.getAfterColumnsList()); &#125; &#125; &#125; &#125; private static void printColumn(List&lt;CanalEntry.Column&gt; columns) &#123; for (CanalEntry.Column column : columns) &#123; System.out.println(column.getName() + " : " + column.getValue() + " update=" + column.getUpdated()); &#125; &#125;&#125; 模拟MySQL的变化创建一张新表，插入一条数据，然后更新这条数据，最后删除这条数据，查看一下canal客户端是不是抓取到了增量数据： 123create table test_canal(id int, name varchar(20)) engine=innodb charset=utf8;insert into test_canal(id, name) values(1, "aaa");update test_canal set name="hahaha" where id=1; 可以看到，创建了一个表，然后插入一条新数据，然后更新了这条数据，更新前后的值都可以看到。]]></content>
      <tags>
        <tag>ETL</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ETL增量数据的捕获]]></title>
    <url>%2F2018%2F05%2F07%2FETL%E5%A2%9E%E9%87%8F%E6%95%B0%E6%8D%AE%E7%9A%84%E6%8D%95%E8%8E%B7%2F</url>
    <content type="text"><![CDATA[在日常的数据操作中，有种很常见的需求，就是要捕获上游数据源的变化，来对增量数据进行处理，如何捕捉变化的数据就变成了ETL过程的主要问题，一般对捕获过程有两个要求：1. 要按照一定的频率准确捕获到增量数据， 2. 不能对业务系统造成太大压力。 常见的捕获增量数据的方法有以下几种： 触发器 时间戳 全表对比 日志监控 下面逐个分析一下 触发器在被抽取的源表上建立插入、修改、删除三种触发器，当源表中数据发生变化，相应的触发器将变化的数据写入一个增量日志表，日志表只存储源表名称、更新关键字值和操作类型（insert，update，delete）。ETL先从日志表取源表名称和关键字值，再去源表抽取完整记录，根据操作类型对目标表做处理。这样的缺点很明显，会对业务系统的性能造成极大的影响。 时间戳在源表上增加一个时间戳字段，系统中更新修改表数据的时候，同时修改时间戳字段的值。当进行数据抽取时，通过比较系统时间与时间戳字段的值来决定抽取哪些数据。对不支持时间戳的自动更新的数据库，需要要求业务系统进行额外的更新时间戳操作，业务系统的人可能会不愿意，并且时间戳的方式无法捕获delete操作(可以要求业务系统实现软删除来避免) 全表对比典型的全表比对的方式是采用MD5校验码。ETL工具事先为要抽取的表建立一个结构类似的MD5临时表，该临时表记录源表主键以及根据所有字段的数据计算出来的MD5校验码。每次进行数据抽取时，对源表和MD5临时表进行MD5校验码的比对，从而决定源表中的数据是新增、修改还是删除，同时更新MD5校验码。这种方式性能较差，并且对于没有主键或者有重复数据的表，准确性也较差。 日志对比数据的增删改在数据库中是要记录log的，例如对于MySQL，可以通过Canel，Databus，Puma等工具读binlog来获取MySQL的增删改等操作来获取增量数据。SQL Server2008之后，提供了CDC(Change Data Capture)功能来实现对增量数据的捕获，CDC可以以异步进程读取事务日志进行捕获数据变更，这样对业务系统的影响比较小。 下面用一个例子来实际操作一下CDC 使用CDC捕捉增量数据启用CDC1234567891011121314151617--把dbowner设为sa，否则会提示权限不足EXEC sp_changedbowner 'sa'GO--查看数据库是否启用CDCSELECT name, is_cdc_enabled FROM sys.databases WHERE name = 'AdventureWorks2012'--启用数据库CDCUSE AdventureWorks2012GOEXECUTE sys.sp_cdc_enable_db;GO--检查启用是否成功SELECT is_cdc_enabled,CASE WHEN is_cdc_enabled=0 THEN 'CDC功能禁用' ELSE 'CDC功能启用' END 描述FROM sys.databasesWHERE NAME = 'AdventureWorks2012' 这时候发现数据库的用户多了一个叫cdc的用户，并且多了一个cdc的schema 对目标表启用CDC1234567891011121314151617181920212223242526272829303132333435--创建测试表USE CDC_DBGOCREATE TABLE [dbo].[Department]( [DepartmentID] [smallint] IDENTITY(1,1) NOT NULL, [Name] [nvarchar](200) NULL, [GroupName] [nvarchar](50) NOT NULL, [ModifiedDate] [datetime] NOT NULL, [AddName] [nvarchar](120) NULL, CONSTRAINT [PK_Department_DepartmentID] PRIMARY KEY CLUSTERED ( [DepartmentID] ASC) ON [PRIMARY]) ON [PRIMARY]GO--对表启用捕获EXEC sys.sp_cdc_enable_table @source_schema= 'dbo', @source_name = 'Department', @role_name = N'cdc_Admin', @capture_instance = DEFAULT, @supports_net_changes = 1, @index_name = NULL, @captured_column_list = NULL, @filegroup_name = DEFAULT--检查是否成功SELECT name, is_tracked_by_cdc , CASE WHEN is_tracked_by_cdc = 0 THEN 'CDC功能禁用' ELSE 'CDC功能启用' END 描述FROM sys.tablesWHERE OBJECT_ID= OBJECT_ID('dbo.Department')--返回某个表的变更捕获配置信息EXEC sys.sp_cdc_help_change_data_capture 'dbo', 'Department' 创建一个测试表，对表行变更启用捕获，为表Department启用CDC，首先会在系统表中创建[cdc].[dbo_Department_CT]，会在Agent中创建两个作业，cdc.CDC_DB_capture和cdc.CDC_DB_cleanup，启用表变更捕获需要开启SQL Server Agent服务，不然会报错。每对一个表启用捕获就会生成一个向对应的记录表。 测试插入、更新、删除12345678910111213141516--测试插入数据INSERT INTO dbo.Department( Name , GroupName , ModifiedDate)VALUES('Marketing','Sales and Marketing',GETDATE())--测试更新数据UPDATE dbo.Department SET Name = 'Marketing Group',ModifiedDate = GETDATE()WHERE Name = 'Marketing'--测试删除数据DELETE FROM dbo.Department WHERE Name='Marketing Group'--查询捕获数据SELECT * FROM cdc.dbo_Department_CT 对于insert/delete操作，会有对应的一行记录，而对于update，会有两行记录。__$operation列： 1 = 删除 2 = 插入 3 = 更新（旧值） 4 = 更新（新值） 可以从结果中看出：刚才的语句插入了一条、更新前后的数据、删除一条数据。 ETL查询指定时间范围的增量数据12345678910111213141516171819202122232425262728SELECT sys.fn_cdc_map_time_to_lsn('smallest greater than or equal', '2018-05-07 09:00:30') AS BeginLSNSELECT sys.fn_cdc_map_time_to_lsn('largest less than or equal', '2018-05-08 23:59:59') AS EndLSN/******* 查看某时间段所有CDC记录*******/DECLARE @FromLSN binary(10) =sys.fn_cdc_map_time_to_lsn('smallest greater than or equal' , '2018-05-07 09:00:30')DECLARE @ToLSN binary(10) =sys.fn_cdc_map_time_to_lsn('largest less than or equal' , '2018-05-08 23:59:59')SELECT CASE [__$operation] WHEN 1 THEN 'DELETE' WHEN 2 THEN 'INSERT' WHEN 3 THEN 'Before UPDATE' WHEN 4 THEN 'After UPDATE' END Operation,[__$operation],[__$update_mask],DepartmentId,Name,GroupName,ModifiedDate,AddNameFROM [cdc].[fn_cdc_get_all_changes_dbo_Department](@FromLSN, @ToLSN, N'all update old')/*all 其中的update，只包含新值all update old 包含新值和旧值*/ 使用SSIS的CDC控件实现增量数据处理准备首先创建一个测试表dbo.DimCustomer_CDC以CustomerKey作为主键，并在表中插入500条数据，这个表作为源表。 12345678910SELECT * into [TestDB].dbo.DimCustomer_CDCFROM AdventureWorksDW2014.[dbo].[DimCustomer]WHERE CustomerKey &lt; 11500IF NOT EXISTS (SELECT * FROM sys.indexes WHERE object_id = OBJECT_ID(N'[dbo].[DimCustomer_CDC]') AND name = N'PK_DimCustomer_CDC') ALTER TABLE [dbo].[DimCustomer_CDC] ADD CONSTRAINT [PK_DimCustomer_CDC] PRIMARY KEY CLUSTERED( [CustomerKey] ASC)GO 然后在这个源表上开启CDC 123456789101112131415161718192021EXEC sp_changedbowner 'sa'GOUSE TestDBGOEXECUTE sys.sp_cdc_enable_db;GO-- CheckSELECT is_cdc_enabled,CASE WHEN is_cdc_enabled=0 THEN N'CDC功能禁用' ELSE N'CDC功能启用' END 描述FROM sys.databasesWHERE NAME = 'TestDB' EXEC sys.sp_cdc_enable_table@source_schema = N'dbo',@source_name = N'DimCustomer_CDC',@role_name = N'cdc_admin',@supports_net_changes = 1 GO 创建一个目标表dbo.DimCustomer_Destination 123SELECT TOP 0 * INTO DimCustomer_DestinationFROM DimCustomer_CDC 到此， CDC的监控准备工作就做好了 全量加载这个包只需要执行一次，将源表的所有数据加载到目标表，并且记录下起始/结束LSN 使用CDC Control Task标记起始LSN，并将CDC状态写入到cdc_state表中 然后创建数据流任务，将源表数据全量导入目标表 最后，创建新的CDC Control Task标记结束LSN 运行这个包，发现源表数据已经全部导入到目标表，并且在cdc_state表中存储了当前CDC的状态 增量加载这个包可以随时加载源表中的增量数据，每次运行这个包都会记录每次的CDC状态。 首先，创建两个stage临时表缓存更新数据和已删的数据 然后创建CDC Control Task查询上次的CDC状态 接下来增量数据流任务里面，查找出增量数据 CDC Source本质上是去CDC创建的系统表cdc.dbo_DimCustomer_CDC_CT中查询变化数据，其中__$operation列： 1 = 删除 2 = 插入 3 = 更新（旧值） 4 = 更新（新值） CDC Split能够根据上面的__$operation列值，自动分出Insert、Update和Delete，下面就是将这三个output放入指定的表即可。 接下来，通过缓存表来更新或者删除目标表 1234567891011121314151617181920-- batch updateUPDATE destSET dest.FirstName = stg.FirstName, dest.MiddleName = stg.MiddleName, dest.LastName = stg.LastName, dest.YearlyIncome = stg.YearlyIncomeFROM [DimCustomer_Destination] dest, [stg_DimCustomer_UPDATES] stgWHERE stg.[CustomerKey] = dest.[CustomerKey] -- batch deleteDELETE FROM [DimCustomer_Destination] WHERE[CustomerKey] IN( SELECT [CustomerKey] FROM [dbo].[stg_DimCustomer_DELETES]) 接下来创建一个新的CDC Control Task来标记这次增量抽取的范围 最终，增量加载的包如下： 运行增量加载首先，将源表的数据进行新增和更新 12345678910111213141516-- Transfer the remaining customer rowsINSERT INTO DimCustomer_CDCSELECT *FROM AdventureWorksDW2014.[dbo].[DimCustomer]WHERE CustomerKey &gt;= 11500 -- give 10 people a raiseUPDATE DimCustomer_CDCSET YearlyIncome = YearlyIncome + 10WHERE CustomerKey &gt; 11000 AND CustomerKey &lt;= 11010-- delete 10 customerDELETE DimCustomer_CDCWHERE CustomerKey &gt; 11110 AND CustomerKey &lt;= 11120 此时运行增量加载的包，可以发现包已经获取到了新增、更新以及删除的数据： 并且CDC状态也已经更新 _NOTE: CDC_STATE的含义_]]></content>
      <tags>
        <tag>SQL Server</tag>
        <tag>ETL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flume1.8安装和入门实例]]></title>
    <url>%2F2018%2F04%2F20%2FFlume1-8%E5%AE%89%E8%A3%85%E5%92%8C%E5%85%A5%E9%97%A8%E5%AE%9E%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[Flume简介Flume是一个分布式的，可靠的，可用的，可以非常有效率的对大数据的日志数据进行收集、聚集、转移。 每一个flume进程都有一个agent层，agent层包含有source、channel、sink： source：采集日志数据，并发送给channel channel：管道，用于连接source和sink，它是一个缓冲区，从source传来的数据会以event的形式在channel中排成队列，然后被sink取走。 sink：获取channel中的数据，并存储到目标源中，目标源可以是HDFS和Hbase 安装与配置下载 Flume 1.8.0 解压缩，在flume-env.sh配置java环境变量，复制一份默认的flume-conf.properties 查看flume的版本 123456$ bin/flume-ng versionFlume 1.8.0Source code repository: https://git-wip-us.apache.org/repos/asf/flume.gitRevision: 99f591994468633fc6f8701c5fc53e0214b6da4fCompiled by denes on Fri Sep 15 14:58:00 CEST 2017From source with checksum fbb44c8c8fb63a49be0a59e27316833d 通过设置agent的配置文件，可以进行不同类型的数据收集，配置文件格式： 12345678910# list the sources, sinks and channels for the agent&lt;Agent&gt;.sources = &lt;Source&gt;&lt;Agent&gt;.sinks = &lt;Sink&gt;&lt;Agent&gt;.channels = &lt;Channel1&gt; &lt;Channel2&gt;# set channel for source&lt;Agent&gt;.sources.&lt;Source&gt;.channels = &lt;Channel1&gt; &lt;Channel2&gt; ...# set channel for sink&lt;Agent&gt;.sinks.&lt;Sink&gt;.channel = &lt;Channel1&gt; OK，可以运行实例了 发送一个文件给Flume新建配置文件 avro.conf123456789101112131415161718192021a1.sources = r1a1.sinks = k1a1.channels = c1# Describe/configure the sourcea1.sources.r1.type= avroa1.sources.r1.channels = c1a1.sources.r1.bind = 0.0.0.0a1.sources.r1.port = 4141# Describe the sinka1.sinks.k1.type= logger# Use a channel which buffers events in memorya1.channels.c1.type= memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100# Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1 启动Flume代理1bin/flume-ng agent -c /Users/lvshuo/bigdata/flume/conf/ -f /Users/lvshuo/bigdata/flume/conf/avro.conf -n a1 -Dflume.root.logger=INFO,console 最后控制台上出现 Avro source r1 started. 表示agent a1启动成功。 使用avro-client发送文件首先创建一个log文件 1echo "Hello World" &gt; test.log 发送这个文件到上面配置文件设置的localhost:4141 1bin/flume-ng avro-client -c /Users/lvshuo/bigdata/flume/conf/ -H localhost -p 4141 -F /Users/lvshuo/test.log Flume控制台接受数据发送文件之后，可以在控制台上看到刚才创建文件的内容： Spool监测配置目录中的新文件Spool监测配置的目录下新增的文件，并将文件中的数据读取出来。 需要注意两点： 1) 拷贝到spool目录下的文件不可以再打开编辑。 2) spool目录下不可包含相应的子目录 创建agent的配置文件 spool.conf新建一个文件夹，存放被监测的log文件 123456789101112131415161718192021a1.sources = r1a1.sinks = k1a1.channels = c1# Describe/configure the sourcea1.sources.r1.type = spooldira1.sources.r1.channels = c1a1.sources.r1.spoolDir =/Users/lvshuo/bigdata/logsa1.sources.r1.fileHeader = true# Describe the sinka1.sinks.k1.type = logger# Use a channel which buffers events in memorya1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100# Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1 启动代理 agent a11bin/flume-ng agent -c /Users/lvshuo/bigdata/flume/conf/ -f /Users/lvshuo/bigdata/flume/conf/spool.conf -n a1 -Dflume.root.logger=INFO,console 模拟产生log文件追加文件到被监测的文件夹中(/Users/lvshuo/bigdata/logs) 1cp /Users/lvshuo/test.log /Users/lvshuo/bigdata/logs Flume控制台接受数据观察到控制台输出log中的内容： Flume在传完文件之后，将会修改文件的后缀，变为.COMPLETED Flume整合Kafka上面的案例里面，Flume将监测到的Log文件内容输出到了控制台。在实际的项目中，经常使用Kafka作为数据中间件，来同时支持离线批处理和在线流式计算 对于Kafka来说，Flume既能作为生产者，又能作为消费者 Flume作为Kafka消费者： Flume作为Kafka生产者： 下面的例子，Flume作为Kafka的生产者，将监控到的Log文件写入到Kafka的相关Topic中 Flume传输Log到Kafka中下面是Flume的配置，  1234567891011121314151617181920212223242526272829# Name component of agenta1.sources = r1a1.sinks = sample a1.channels = sample-channel# # Describe/configure the sourcea1.sources.r1.type = execa1.sources.r1.command = tail -f /Users/lvshuo/bigdata/logs/my_log_file.log a1.sources.r1.logStdErr = true# sink typea1.sinks.sample.type = logger## buffers events in memeory to channela1.channels.sample-channel.type = memorya1.channels.sample-channel.capacity = 1000a1.channels.sample-channel.transactionCapacity = 100# bind source and sink to the channela1.sources.r1.channels.selector.type = replicatinga1.sources.r1.channels = sample-channel## kafka configa1.sinks.sample.type = org.apache.flume.sink.kafka.KafkaSinka1.sinks.sample.kafka.topic = flumeLogTopica1.sinks.sample.kafka.bootstrap.servers = 127.0.0.1:9092a1.sinks.sample.kafka.producer.acks = 1a1.sinks.sample.kafka.flumeBatchSize = 20a1.sinks.sample.channel = sample-channel 相关Flume配置参考 Flume User Guide(Exec Source) Flume User Guide(Kafka Sink) 启动Flume代理a1 1bin/flume-ng agent -c /Users/lvshuo/bigdata/flume/conf/ -f /Users/lvshuo/bigdata/flume/conf/flume-kafka.conf -n a1 -Dflume.root.logger=INFO,console 查看FlumeLogTopic中的数据，发现log中的数据已经发送到Topic中，可以供下游消费者进行消费 1kafka-console-consumer.sh --zookeeper localhost:2181 --topic flumeLogTopic --from-beginning]]></content>
      <categories>
        <category>Big Data</category>
        <category>Flume</category>
      </categories>
      <tags>
        <tag>Flume</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka入门]]></title>
    <url>%2F2018%2F04%2F18%2FKafka%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[Kafka是一个分布式消息发布订阅系统，Kafka可以用于以下场景： 消息系统， 例如ActiveMQ 和 RabbitMQ. 站点的用户活动追踪。 用来记录用户的页面浏览，搜索，点击等。 操作审计。 用户/管理员的网站操作的监控。 日志聚合。收集数据，集中处理。 流处理等 基本概念 Producer：消息生产者，就是向kafka broker发消息的客户端。 Consumer：消息消费者，是消息的使用方，负责消费Kafka服务器上的消息。 Topic：主题，由用户定义并配置在Kafka服务器，用于建立Producer和Consumer之间的订阅关系。生产者发送消息到指定的Topic下，消息者从这个Topic下消费消息。 Partition：消息分区，一个topic可以分为多个 partition，每个partition是一个有序的队列。partition中的每条消息都会被分配一个有序的id（offset）。 note: 一个topic的一个partition只能被一个consumer group中的一个consumer消费，多个consumer消费同一个partition中的数据是不允许的，但是一个consumer可以消费多个partition中的数据 Broker：一台kafka服务器就是一个broker。一个集群由多个broker组成。一个broker可以容纳多个topic。 Consumer Group：消费者分组，用于归组同类消费者。每个consumer属于一个特定的consumer group，多个消费者可以共同消费一个Topic下的消息，每个消费者消费其中的部分消息，这些消费者就组成了一个分组，拥有同一个分组名称，通常也被称为消费者集群。 快速实践Kafka下载kafka和zookeeper的安装包，解压，配置环境变量，启动zookeeper服务，启动kafka守护进程 新建一个topic使用kafka自带的命令行工具kafka-topic.sh创建一个新的topic 1bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test 发送一个消息到topic123bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test This is a messageThis is another message 读取刚创建的消息1bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic test --from-beginning 可以看到刚才创建的两条消息被输出来了。 使用python的Kafka客户端实现生产者消费者实现上面命令行的生产者消费者的例子： 生产者12345678910111213from kafka import KafkaProducerimport timeproducer = KafkaProducer(bootstrap_servers='localhost:9092') i = 0while True: ts = int(time.time() * 1000) producer.send(topic="myfirsttopic", value=bytes(str(i).encode('utf-8')), timestamp_ms=ts) producer.flush() print(i) i += 1 time.sleep(1) 消费者1234567891011121314from kafka import KafkaConsumerKAFKA_HOST = 'localhost'KAFKA_PORT = 9092KAFKA_TOPIC = 'myfirsttopic'consumer = KafkaConsumer(KAFKA_TOPIC, bootstrap_servers='localhost:9092')try: for message in consumer: print(message)except KeyboardInterrupt: print("Catch keyboard interrupt") 可以看到，生产者产生的消息被消费者消费了 追踪偏移量offsetkafka允许consumer将当前消费的消息的offset提交到kafka中，这样如果consumer因异常退出后，下次启动仍然可以从上次记录的offset开始向后继续消费消息。 修改一下刚才的consumer代码，把enable_auto_commit设为false，让应用程序决定何时提交偏移 1234567891011121314151617181920212223from kafka import KafkaConsumer, TopicPartition, OffsetAndMetadataKAFKA_HOST = 'localhost'KAFKA_PORT = 9092kafka_topic = TopicPartition("myfirsttopic", 0)consumer = KafkaConsumer( bootstrap_servers=['localhost:9092'], group_id="testgroup", auto_offset_reset="earliest", enable_auto_commit=False)consumer.assign([tp])print("Start offset is: " + str(consumer.position(tp)))try: for message in consumer: print(message) consumer.seek(tp, message.offset+1) consumer.commit()except KeyboardInterrupt: print("Catch keyboard interrupt") 在offset为249时候终止消费者，然后再次启动消费者，可以看出是从上次停止的地方(250)继续消费： 默认情况下auto.commit.enable等于true，这也就意味着consumer会定期的commit offset Kafka消息发送分区选择Kafka发送消息的流程图如下： 那么，发送消息的时候，是如何选择分区的呢？KafkaProducer对象通过send方法，将记录发给kafka 1producer.send(new ProducerRecord&lt;String, String&gt;(topicName, Integer.toString(i), Integer.toString(i))); 在KafkaProducer中，是通过内部的私有方法doSend来发送消息 1int partition = this.partition(record, serializedKey, serializedValue, cluster); 看一下partition方法的实现： 1234private int partition(ProducerRecord&lt;K, V&gt; record, byte[] serializedKey, byte[] serializedValue, Cluster cluster) &#123; Integer partition = record.partition(); return partition != null ? partition.intValue() : this.partitioner.partition(record.topic(), record.key(), serializedKey, record.value(), serializedValue, cluster);&#125; 如果record指定了分区则指定的分区会被使用，如果没有则使用partitioner分区器来选择分区 partitioner的实现： 1234567891011121314151617181920212223242526272829public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) &#123; List&lt;PartitionInfo&gt; partitions = cluster.partitionsForTopic(topic); int numPartitions = partitions.size(); if (keyBytes == null) &#123; int nextValue = this.nextValue(topic); List&lt;PartitionInfo&gt; availablePartitions = cluster.availablePartitionsForTopic(topic); if (availablePartitions.size() &gt; 0) &#123; int part = Utils.toPositive(nextValue) % availablePartitions.size(); return ((PartitionInfo)availablePartitions.get(part)).partition(); &#125; else &#123; return Utils.toPositive(nextValue) % numPartitions; &#125; &#125; else &#123; return Utils.toPositive(Utils.murmur2(keyBytes)) % numPartitions; &#125;&#125;private int nextValue(String topic) &#123; AtomicInteger counter = (AtomicInteger)this.topicCounterMap.get(topic); if (null == counter) &#123; counter = new AtomicInteger(ThreadLocalRandom.current().nextInt()); AtomicInteger currentCounter = (AtomicInteger)this.topicCounterMap.putIfAbsent(topic, counter); if (currentCounter != null) &#123; counter = currentCounter; &#125; &#125; return counter.getAndIncrement();&#125; 如果key为null，则先根据topic名获取上次计算分区时使用的一个整数并加一。然后判断topic的可用分区数是否大于0，如果大于0则使用获取的nextValue的值和可用分区数进行取模操作。 如果topic的可用分区数小于等于0，则用获取的nextValue的值和总分区数进行取模操作。 如果消息的key不为null，就根据hash算法murmur2就算出key的hash值，然后和分区数进行取模运算。 Kafka ConnectKafka Connect 是Kafka 的一部分，它为在Kafka 和外部数据存储系统之间移动数据提供了一种可靠且可伸缩的方式。 Kafka Connnect有两个核心概念：Source和Sink。 Source负责导入数据到Kafka，Sink负责从Kafka导出数据，它们都被称为Connector。 Kafka Connect简单实例下面这个例子使用Kafka Connect将数据从文件导入到Kafka，然后再将Topic中数据导入到文件中 创建文件，写入测试数据 12cd /Users/lvshuo/bigdata/kafkaecho "Hello World" &gt; test.txt topic的偏移量存储在/tmp/connect.offsets这个文件中,在config/connect-standalone.properties配置，每次connect启动的时候会根据connector的name获得topic偏移量,然后在继续读取或者写入数据 下面配置source和sink：一个用于将文件数据导入Kafka，一个用于将Topic数据导出到文件，下面是配置文件connect-console-source.properties的内容： 1234name=local-console-sourceconnector.class=org.apache.kafka.connect.file.FileStreamSourceConnectortasks.max=1topic=connect-test connect-console-sink.properties文件内容： 12345name=local-console-sinkconnector.class=org.apache.kafka.connect.file.FileStreamSinkConnectortasks.max=1topics=connect-testfile=test.sink.txt 启动两个单点的connector 1bin/connect-standalone.sh config/connect-standalone.properties config/connect-file-source.properties config/connect-file-sink.properties 可以看到生成了test.sink.txt的文件 查看一下connect-test主题中的数据 更多的Connector可以去confluent平台去看。 参考 Apache Kafka Doc Kafka Connect Building a Real-Time Streaming ETL Pipeline in 20 Minutes]]></content>
      <categories>
        <category>Big Data</category>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[记录配置伪分布式Hadoop环境遇到的坑]]></title>
    <url>%2F2018%2F04%2F17%2F%E8%AE%B0%E5%BD%95%E9%85%8D%E7%BD%AE%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8FHadoop%E7%8E%AF%E5%A2%83%E9%81%87%E5%88%B0%E7%9A%84%E5%9D%91%2F</url>
    <content type="text"><![CDATA[问题本地mac搭了个Hadoop3.0的伪分布式环境，HDFS运行正常、MapReduce单机模式也正常，JPS后台守护进程都正常。但是写了个MapReduce任务，想去跑一下，结果卡在了Map 100% Reduce 0% 去stackoverflow查了一圈，基本都在说运行环境没有分配足够的内存导致的，从container的syslog里面只有下面的ERROR： 1234567891011121314Error: org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in fetcher#1 at org.apache.hadoop.mapreduce.task.reduce.Shuffle.run(Shuffle.java:134) at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:377) at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1965) at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)Caused by: java.io.IOException: Exceeded MAX_FAILED_UNIQUE_FETCHES; bailing-out. at org.apache.hadoop.mapreduce.task.reduce.ShuffleSchedulerImpl.checkReducerHealth(ShuffleSchedulerImpl.java:396) at org.apache.hadoop.mapreduce.task.reduce.ShuffleSchedulerImpl.copyFailed(ShuffleSchedulerImpl.java:311) at org.apache.hadoop.mapreduce.task.reduce.Fetcher.openShuffleUrl(Fetcher.java:291) at org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyFromHost(Fetcher.java:330) at org.apache.hadoop.mapreduce.task.reduce.Fetcher.run(Fetcher.java:198) 内存问题解决方法歪个楼，看了不少的关于内存不足的分析，也学了不少，记在下面以备以后需要。 引用自MapReduce任务Shuffle Error错误 shuffle过程的输入是：map任务的输出文件，它的输出接收者是：运行reduce任务的机子上的内存buffer，并且shuffle过程以并行方式运行参数mapreduce.reduce.shuffle.input.buffer.percent控制运行reduce任务的机子上多少比例的内存用作上述buffer(默认值为0.70)，参数mapreduce.reduce.shuffle.parallelcopies控制shuffle过程的并行度(默认值为5)那么”mapreduce.reduce.shuffle.input.buffer.percent” *“mapreduce.reduce.shuffle.parallelcopies” 必须小于等于1，否则就会出现如上错误因此，我将mapreduce.reduce.shuffle.input.buffer.percent设置成值为0.1，就可以正常运行了（设置成0.2，还是会抛同样的错） 另外，可以发现如果使用两个参数的默认值，那么两者乘积为3.5，大大大于1了，为什么没有经常抛出以上的错误呢？ 首先，把默认值设为比较大，主要是基于性能考虑，将它们设为比较大，可以大大加快从map复制数据的速度其次，要抛出如上异常，还需满足另外一个条件，就是map任务的数据一下子准备好了等待shuffle去复制，在这种情况下，就会导致shuffle过程的“线程数量”和“内存buffer使用量”都是满负荷的值，自然就造成了内存不足的错误；而如果map任务的数据是断断续续完成的，那么没有一个时刻shuffle过程的“线程数量”和“内存buffer使用量”是满负荷值的，自然也就不会抛出如上错误另外，如果在设置以上参数后，还是出现错误，那么有可能是运行Reduce任务的进程的内存总量不足，可以通过mapred.child.java.opts参数来调节，比如设置mapred.child.java.opts=-Xmx2024mreduce会在map执行到一定比例启动多个fetch线程去拉取map的输出结果，放到reduce的内存、磁盘中，然后进行merge。当数据量大时，拉取到内存的数据就会引起OOM，所以此时要减少fetch占内存的百分比，将fetch的数据直接放在磁盘上。 mapreduce.reduce.shuffle.memory.limit.percent：每个fetch取到的map输出的大小能够占的内存比的大小。默认是0.25。因此实际每个fetcher的输出能放在内存的大小是reducer的java heapsize0.90.25 我的问题解决言归正传，按照上面的解决办法，问题依然存在，然后我删掉了Hadoop重新配置了一个全新的，跑一下伪分布式的hadoop-example，依然是这样，大脑都快炸了，啊啊啊 后来发现有人说是本机host的问题，把计算机名加入到host里面 1234127.0.0.1 localhost-- added127.0.0.1 my_computername... 好了~~~内牛满面啊 Hadoop伪分布式环境配置core-site.xml1234567891011121314&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://localhost:9000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;file:/Users/lvshuo/bigdata/hadoop/tmp&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;io.file.buffer.size&lt;/name&gt; &lt;value&gt;131702&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; hdfs-site.xml123456789101112131415&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:/Users/lvshuo/bigdata/hadoop/tmp/hdfs/namenode&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:/Users/lvshuo/bigdata/hadoop/tmp/hdfs/datanode&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; yarn-site.xml1234&lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt;&lt;/property&gt; mapred-site.xml123456789101112131415161718192021222324&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.child.java.opts&lt;/name&gt; &lt;value&gt;-Xmx4096m&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.reduce.shuffle.memory.limit.percent&lt;/name&gt; &lt;value&gt;0.1&lt;/value&gt; &lt;description&gt;Expert: Maximum percentage of the in-memory limit that a single shuffle can consume&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.application.classpath&lt;/name&gt; &lt;value&gt; /Users/lvshuo/bigdata/hadoop/etc/hadoop:/Users/lvshuo/bigdata/hadoop/share/hadoop/common/lib/*:/Users/lvshuo/bigdata/hadoop/share/hadoop/common/*:/Users/lvshuo/bigdata/hadoop/share/hadoop/hdfs:/Users/lvshuo/bigdata/hadoop/share/hadoop/hdfs/lib/*:/Users/lvshuo/bigdata/hadoop/share/hadoop/hdfs/*:/Users/lvshuo/bigdata/hadoop/share/hadoop/mapreduce/*:/Users/lvshuo/bigdata/hadoop/share/hadoop/yarn:/Users/lvshuo/bigdata/hadoop/share/hadoop/yarn/lib/*:/Users/lvshuo/bigdata/hadoop/share/hadoop/yarn/* &lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt;]]></content>
      <categories>
        <category>Big Data</category>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[拉链表的应用]]></title>
    <url>%2F2018%2F04%2F13%2F%E6%8B%89%E9%93%BE%E8%A1%A8%E7%9A%84%E5%BA%94%E7%94%A8%2F</url>
    <content type="text"><![CDATA[数据仓库和事务数据库不一样的地方，在于数据仓库需要对历史数据进行分析。而在数据仓库海量的数据里，如何既节约存储空间，又能满足对历史变更数据的查询，就成为一个要解决的问题。这时候就可以使用拉链表来完成这种需求。 什么是拉链表以订单这种场景为例，原始订单表orders有如下几个字段：订单ID、创建时间、修改时间、订单状态 4月1号当天的订单 orderid createtime modifiedtime status 1 2018-04-01 2018-04-01 create 2 2018-04-01 2018-04-01 create 3 2018-04-01 2018-04-01 create 4月2号当天的订单表数据 orderid createtime modifiedtime status 1 2018-04-01 2018-04-02 paid 2 2018-04-01 2018-04-02 finish-closed 3 2018-04-01 2018-04-02 paid 4 2018-04-02 2018-04-02 create 这一天，订单1、3由create变成paid状态，订单2由create变为paid，然后变为finish状态(假设现在只关心订单的最新状态)，订单4为新订单。 这时候，在数据仓库中，可以设计一张表来保存每天的订单变化情况。增加两个字段：dw_start_date和dw_end_date，dw_start_date表示该条记录的生命周期开始时间，dw_end_date表示该条记录的生命周期结束时间。现在表就变成了下面这个样子： orderid createtime modifiedtime status dw_start_date dw_end_date 1 2018-04-01 2018-04-01 create 2018-04-01 2018-04-01 1 2018-04-01 2018-04-02 paid 2018-04-02 9999-12-31 2 2018-04-01 2018-04-01 create 2018-04-01 2018-04-01 2 2018-04-01 2018-04-02 finish-closed 2018-04-02 9999-12-31 3 2018-04-01 2018-04-01 create 2018-04-01 2018-04-01 3 2018-04-01 2018-04-02 paid 2018-04-02 9999-12-31 4 2018-04-02 2018-04-02 create 2018-04-02 9999-12-31 这个表就叫拉链表 orders_his 如果要查询当前有效的记录，那么如下： 12SELECT * FROM orders_his WHERE dw_end_date='9999-12-31'; 如果要查询4月1号的历史快照，如下： 12SELECT * FROM orders_his WHERE dw_start_date &lt;= '2018-04-01' AND dw_end_date &gt;= '2018-04-01' 实现拉链表准备现在有三张表：原系统订单表orders、数据仓库ODS层增量数据表orders_inc、数据仓库DW层拉链表orders_his 123456CREATE TABLE orders ( orderid INT, createtime STRING, modifiedtime STRING, status STRING) stored AS textfile; 12345678CREATE TABLE orders_inc ( orderid INT, createtime STRING, modifiedtime STRING, status STRING)PARTITIONED BY (day STRING)stored AS textfile; 12345678CREATE TABLE orders_his ( orderid INT, createtime STRING, modifiedtime STRING, status STRING, dw_start_date STRING, dw_end_date STRING) stored AS textfile; 其中，增量表可以有如下几个方式获取： 监控MySQL的数据变化，例如使用Canal、或者ChangeDataCapture，获取最后一条更新的数据 每天获取一份切片数据，然后对比两天的切片数据获得增量 源数据库的流水表，例如XXX_log等 下面是如何实现拉链表的步骤 1. 全量初始化假设数据仓库从4月1号起启用，那么首先需要在4月1号这一天做全量初始化，需要将4月1号以前(含)的所有数据都抽取并刷新到数据仓库中。 1234INSERT overwrite TABLE orders_inc PARTITION (day = '2018-04-01')SELECT orderid, createtime, modifiedtime, statusFROM ordersWHERE createtime &lt;= '2018-04-01'; 然后从ODS层抽取到DW层： 123456INSERT overwrite TABLE orders_hisSELECT orderid,c reatetime, modifiedtime, status, createtime AS dw_start_date, '9999-12-31' AS dw_end_dateFROM orders_incWHERE day = '2018-04-01'; 到此，全量初始化的工作完成，现在DW拉链表的数据如下， orderid createtime modifiedtime status dw_start_date dw_end_date 1 2018-04-01 2018-04-01 create 2018-04-01 9999-12-31 2 2018-04-01 2018-04-01 create 2018-04-01 9999-12-31 3 2018-04-01 2018-04-01 create 2018-04-01 9999-12-31 2. 增量抽取4月2号了， 从源系统订单表中，将前一天的增量数据抽取到ODS层的增量数据表。这里的增量数据使用订单表中的创建时间和修改时间来确定。 note：订单表中数据同一天有多次状态更新，取每天的最后一个状态为当天的最终状态 12345-- $&#123;day&#125; = '2018-04-02'INSERT overwrite TABLE orders_inc PARTITION (day = '$&#123;day&#125;')SELECT orderid, createtime, modifiedtime, statusFROM ordersWHERE createtime = '$&#123;day&#125;' OR modifiedtime = '$&#123;day&#125;'; 3. 放到拉链表中先将增量数据放到临时表中，然后插入DW拉链表 12345678910111213141516171819202122232425262728293031323334353637DROP TABLE IF EXISTS orders_his_tmp; CREATE TABLE orders_his_tmp AS SELECT orderid, createtime, modifiedtime, status, dw_start_date, dw_end_date FROM ( -- updated order SELECT a.orderid, a.createtime, a.modifiedtime, a.status, a.dw_start_date, CASE WHEN b.orderid IS NOT NULL AND a.dw_end_date = '9999-12-31' THEN '2018-04-01' ELSE a.dw_end_date END AS dw_end_date FROM orders_his a LEFT JOIN (SELECT * FROM orders_inc WHERE day = '2018-04-02') b ON (a.orderid = b.orderid) UNION ALL -- new order SELECT orderid, createtime, modifiedtime, status, modifiedtime AS dw_start_date, '9999-12-31' AS dw_end_date FROM orders_inc WHERE day = '2018-04-02' ) x ORDER BY orderid,dw_start_date; INSERT overwrite TABLE dw_orders_his SELECT * FROM orders_his_tmp; 拉链表就是如下了： orderid createtime modifiedtime status dw_start_date dw_end_date 1 2018-04-01 2018-04-01 create 2018-04-01 2018-04-01 1 2018-04-01 2018-04-02 paid 2018-04-02 9999-12-31 2 2018-04-01 2018-04-01 create 2018-04-01 2018-04-01 2 2018-04-01 2018-04-02 finish-closed 2018-04-02 9999-12-31 3 2018-04-01 2018-04-01 create 2018-04-01 2018-04-01 3 2018-04-01 2018-04-02 paid 2018-04-02 9999-12-31 4 2018-04-02 2018-04-02 create 2018-04-02 9999-12-31 查询性能优化数据仓库随着时间发展，拉链表也会越来越大，为了查询性能不会受影响，需要定期对拉链表中历史数据进行归档。另外，除了增加两个起始时间和结束时间外，还可以增加一个当前行状态字段isActive字段，可以迅速找到可用的状态行。 参考下面的一些思路参考 木东居士 使用拉链表的时候可以不加t_end_date，即失效日期，但是加上之后，能优化很多查询。 可以加上当前行状态标识，能快速定位到当前状态。 在拉链表的设计中可以加一些内容，因为我们每天保存一个状态，如果我们在这个状态里面加一个字段，比如当天修改次数，那么拉链表的作用就会更大。]]></content>
      <tags>
        <tag>数据仓库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Homebrew 管理后台服务]]></title>
    <url>%2F2018%2F04%2F11%2FHomebrew-%E7%AE%A1%E7%90%86%E5%90%8E%E5%8F%B0%E6%9C%8D%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[在MacOS上，Homebrew是一个特别好的软件包管理的工具。以前只是拿来安装软件，偶然一次Mac重启了，好多后台服务就停了，这时候想起来Homebrew还能管理它所安装的软件的后台服务。下面内容转自Starting and Stopping Background Services with Homebrew 作为备忘。 如果你使用 Homebrew 安装过 mysql 那么下面的安装后提示你可能比较熟悉 123456To have launchd start mysql at login: ln -sfv /usr/local/opt/mysql/*.plist ~/Library/LaunchAgentsThen to load mysql now: launchctl load ~/Library/LaunchAgents/homebrew.mxcl.mysql.plistOr, if you don&apos;t want/need launchctl, you can just run: mysql.server start 如果按上面的说明操作的话，未免太麻烦了，而且也很难记住 plist 的位置。还好 Homebrew 提供了一个易用的接口来管理 plist，然后你就不用再纠结什么 ln，launchctl，和 plist 的位置了。 brew services首先安装 brew services 命令1brew tap gapple/services 下面举个例子12$ brew services start mysql==&gt; Successfully started `mysql` (label: homebrew.mxcl.mysql) 在后台，brew services start 其实执行了最上面的安装后消息里面提到的所有命令，比如首先运行 ln -sfv ...，然后 launchctl load ~/Library/LaunchAgents/homebrew.mxcl.mysql.plist 。 假设突然 MySQL 出毛病了，我们要重启一下，那么执行下面的命令就行了1234brew services restart mysqlStopping `mysql`... (might take a while)==&gt; Successfully stopped `mysql` (label: homebrew.mxcl.mysql)==&gt; Successfully started `mysql` (label: homebrew.mxcl.mysql) 想看所有的已启用的服务的话：123456$ brew services listredis started 442 /Users/gabe/Library/LaunchAgents/homebrew.mxcl.redis.plistpostgresql started 443 /Users/gabe/Library/LaunchAgents/homebrew.mxcl.postgresql.plistmongodb started 444 /Users/gabe/Library/LaunchAgents/homebrew.mxcl.mongodb.plistmemcached started 445 /Users/gabe/Library/LaunchAgents/homebrew.mxcl.memcached.plistmysql started 87538 /Users/gabe/Library/LaunchAgents/homebrew.mxcl.mysql.plist 要注意的是，这里不止显示通过 brew services 加载的服务，也包含 launchctl load 加载的。 如果你卸载了 MySQL 但是 Homebrew 没把 plist 文件删除的话，你可以 12$ brew services cleanupRemoving unused plist /Users/gabe/Library/LaunchAgents/homebrew.mxcl.mysql.plist]]></content>
      <categories>
        <category>备忘</category>
      </categories>
      <tags>
        <tag>Mac</tag>
        <tag>Homebrew</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MapReduce 实现倒排索引]]></title>
    <url>%2F2018%2F04%2F08%2FMapReduce-%E5%AE%9E%E7%8E%B0%E5%80%92%E6%8E%92%E7%B4%A2%E5%BC%95%2F</url>
    <content type="text"><![CDATA[概念倒排索引是一种索引方法，被用来存储在全文搜索下某个单词在一个文档或者一组文档中的存储位置的映射，常被应用于搜索引擎和关键字查询的问题中。 以英文为例，下面是要被索引的文本： 123T0 = &quot;it is what it is&quot; T1 = &quot;what is it&quot; T2 = &quot;it is a banana&quot; 有两种不同的反向索引形式： 一条记录的水平反向索引（或者反向档案索引）包含每个引用单词的文档的列表。 一个单词的水平反向索引（或者完全反向索引）又包含每个单词在一个文档中的位置 我们就能得到下面的反向文件索引：12345&quot;a&quot;: &#123;2&#125;&quot;banana&quot;: &#123;2&#125;&quot;is&quot;: &#123;0, 1, 2&#125;&quot;it&quot;: &#123;0, 1, 2&#125;&quot;what&quot;: &#123;0, 1&#125; 检索的条件”what”,”is”和”it”将对应集合的交集。检索的条件”what”, “is” 和 “it” 将对应这个集合： ${\displaystyle {0,1}\cap {0,1,2}\cap {0,1,2}={0,1}}$。 下面得到的是第二种倒排索引，包含有文档数量和单词结果在文档中的位置组成的的成对数据。 12345&quot;a&quot;: &#123;(2, 2)&#125;&quot;banana&quot;: &#123;(2, 3)&#125;&quot;is&quot;: &#123;(0, 1), (0, 4), (1, 1), (2, 1)&#125;&quot;it&quot;: &#123;(0, 0), (0, 3), (1, 2), (2, 0)&#125; &quot;what&quot;: &#123;(0, 2), (1, 0)&#125; 实现第一种倒排索引在Map端，把单词和文件名作为key值，把单词词频作为value值。在Combine阶段，需要把单词设成key，文件名和词频作为value值，将相同单词的所有记录发送给同一个Reducer处理 在Reduce端，生成文档列表。 Mapper 123456789101112131415161718192021public static class InvertedIndexMapper extends Mapper&lt;Object, Text, Text, Text&gt;&#123; private Text word_filename_key = new Text(); private Text word_frequency = new Text(); private FileSplit split; // split对象 @Override protected void map(Object key, Text value, Context context) throws IOException, InterruptedException &#123; //获得&lt;key,value&gt;对所属的FileSplit对象 split = (FileSplit) context.getInputSplit(); StringTokenizer itr = new StringTokenizer(value.toString()); while (itr.hasMoreTokens())&#123; word_filename_key.set(itr.nextToken() + ":" + split.getPath().toString()); word_frequency.set("1"); context.write(word_filename_key, word_frequency); &#125; &#125; //map output: I:1.txt list&#123;1,1,1&#125;&#125; Combiner 123456789101112131415161718192021222324252627// Combiner merge word frequency public static class InvertedIndexCombiner extends Reducer&lt;Text, Text, Text, Text&gt;&#123; private Text new_value = new Text(); @Override public void reduce(Text key, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException &#123; int sum = 0; for (Text value: values)&#123; sum += Integer.parseInt(value.toString()); &#125; int splitIndex = key.toString().indexOf(":"); // 文件名和词频作为value值 new_value.set(key.toString().substring(splitIndex + 1) + ":" + sum); // 单词设成key key.set(key.toString().substring(0, splitIndex)); context.write(key, new_value); System.out.println("key: " + key); System.out.println("value: " + new_value); &#125; // combiner output: I 1.txt:3&#125; Reducer 12345678910111213141516// 生成单词的文档列表public static class InvertedIndexReducer extends Reducer&lt;Text, Text, Text, Text&gt;&#123; private Text result = new Text(); @Override public void reduce(Text key, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException &#123; String fileList = ""; for (Text value: values)&#123; fileList += " " + value.toString() + ";"; &#125; result.set(fileList); context.write(key, result); &#125;&#125; 实现第二种倒排索引使用自定义的数据类型]]></content>
      <categories>
        <category>Big Data</category>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Big Data</tag>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MapReduce原理]]></title>
    <url>%2F2018%2F04%2F04%2FMapReduce%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[MapReduce Paper MapReduce是Google提出的一个软件架构，用于大规模数据集（大于1TB）的并行运算。MapReduce最本质的两个过程就是Map和Reduce，思想来源于函数式编程。 初学MapReduce，写了WordCount入门程序后，觉得编写MapReduce程序只需要实现Map和Reduce函数就可以了，后来觉得，这个框架隐藏的细节还是需要好好了解一下的。下面这个图基本描述了MapReduce的整个过程： 其中，Map阶段、Reduce阶段比较好理解，但是Shuffle阶段的这个细节还是很神奇的。下面简单介绍下MapReduce各个阶段。 InputFormat类该类的作用是将输入的文件和数据分割成许多小的split文件，并将split的每个行通过LineRecorderReader解析成&lt;Key,Value&gt;,通过job.setInputFromatClass()函数来设置，默认的情况为类TextInputFormat，其中Key默认为字符偏移量，value是该行的值。 Mapper类&lt;k1, v1&gt; –map–&gt; &lt;k2, v2&gt; 根据输入的&lt;Key,Value&gt;对生成中间结果，默认的情况下使用Mapper类，该类将输入的&lt;Key,Value&gt;对进行映射并作为中间结果输出，通过job.setMapperClass()实现。提供了Mapper的抽象类，需要实现map方法 Combiner类实现combine函数，该类的主要功能是合并相同的key键，通过job.setCombinerClass()方法设置，默认为null，不合并中间结果 Partition类Map的结果会通过partition分发到指定的Reducer上，哪个key到哪个Reducer的分配过程是由Partitioner规定的。可以通过job.setPartitionerClass()方法进行设置，partition是分割map每个节点的结果，按照key分别映射给不同的reduce，也是可以自定义的，需要实现getPartition函数。 1getPartition(Text key, Text value, int numPartitions) MapReduce默认使用hashPartitioner，计算方法如下： 1reducer = (key.hashCode() &amp; Integer.MAX_VALUE) % numReduceTasks Reducer类&lt;k2, value-list&gt; –reduce–&gt; &lt;k3, v3&gt; 将中间结果合并，得到中间结果。通过job.setReduceCalss()方法进行设置，默认使用Reducer类，实现reduce方法。 OutputFormat类该类负责输出结果的格式。可以通过job.setOutputFormatClass()方法进行设置。默认使用TextOUtputFormat类，得到&lt;Key,value&gt;对。 整个mapreduce任务的基本流程如下程序所示： 1234567891011public static void main(String[] args) throws IOException &#123; Configuration conf = new Configuration(); Job job = new Job(conf); job.setInputFormatClass(TextInputFormat.class); job.setMapperClass(Mapper.class); job.setCombinerClass(null); job.setPartitionerClass(HashPartitioner.class); job.setReducerClass(Reducer.class); job.setOutputFormatClass(TextOutFormat.class); &#125;&#125; Shuffle 过程Shuffle过程是MapReduce的核心，Shuffle过程的性能与整个MapReduce的性能直接相关。Shuffle过程包含在Map和Reduce两端中。 Map端的Shuffle过程是对Map的结果进行分区(partition)、排序(sort)、和溢写(spill)，然后将属于同一个划分的输出合并在一起并写在磁盘上，同时按照不同的划分结果发送给对应的Reduce任务。 Reduce端会将各个Map发送来的属于同一个划分的输出进行合并、然后对合并结果进行排序，最后交给Reduce处理。 Map 端Map端的Shuffle过程，包含了切分(Partition)过程、溢写(Sort&amp;Combine)过程。和合并(Merge)过程。 Partition分区 mapreduce提供Partitioner接口，它的作用就是根据key或value及reduce的数量来决定当前的这对输出数据最终应该交由哪个reducetask处理（分区）。MapRedcuce默认使用hashPartitioner。 注意：虽然Partitioner接口会计算出一个值来决定某个输出会交给哪个reduce去处理，但是在缓冲区中并不会实现物理上的分区，而是将结果加载key-value后面。物理上的分区实在磁盘上进行的。 环形缓冲区 map在内存中有一个环形缓冲区(字节数组实现)，用于存储任务的输出。默认是100M，这其中80%的容量用来缓存，当这部分容量满了的时候会启动一个溢出线程进行溢出操作，写入磁盘形成溢写文件；在溢出的过程中剩余的20%对新生产的数据继续缓存。【简单来说就是别读边写】但如果再次期间缓冲区被填满，map会阻塞直到写磁盘过程完成。 阈值是可以设置的，但一般默认就可以了。 1）环形缓冲区大小:mapred-site.xml中设置mapreduce.task.io.sort.mb的值 2）环形缓冲区溢写的阈值:mapred-site.xml中设置mapreduce.map.sort.spill.percent的值 作用：为什么要分区呢？？由于map()处理后的数据量可能会非常大，所以如果由一个reduce()处理效率不高，为了解决这个问题可以用分布式的思想，一个reduce()解决不了，就用多个reduce节点。一般来说有几类分区就对应有几个reduce节点，把相同分区交给一个reduce节点处理。 Spill溢写sort排序 缓冲区的数据写到磁盘前，会对它进行一个二次快速排序，首先根据数据所属的partition （分区）排序，然后每个partition中再按Key 排序。输出包括一个索引文件和数据文件。如果设定了Combiner，将在排序输出的基础上运行。【Combiner】就是一个简单Reducer操作，它在执行Map 任务的节点本身运行，先对Map 的输出做一次简单Reduce，使得Map的输出更紧凑，更少的数据会被写入磁盘和传送到Reducer。临时文件会在map任务结束后删除。 merge文件合并每次溢写会在磁盘上生成一个溢写文件，如果map的输出结果很大，有多次这样的溢写发生，磁盘上相应的就会有多个溢写文件存在。因为最终的文件只有一个，所以需要将这些溢写文件归并到一起，这个过程就叫做Merge。【merge就是多个溢写文件合并到一个文件】所以可能也有相同的key存在，在这个过程中如果client设置过Combiner，也会使用Combiner来合并相同的key。 map端就处理完了，接下来就是reduce端了。 Reduce 端Map端合并最终生成的这个文件也存放在TaskTracker够得着的某个本地目录内，每个reduce task不断地从JobTracker那里获取map task是否完成的信息，如果reduce task得到通知，获知某台TaskTracker上的map task执行完成，Shuffle的后半段过程开始启动。 copy复制 reduce端默认有5个数据复制线程从map端复制数据，其通过Http方式得到Map对应分区的输出文件。reduce端并不是等map端执行完后将结果传来，而是直接去map端去Copy输出文件。 Merge合并 reduce端的shuffle也有一个环形缓冲区，它的大小要比map端的灵活（由JVM的heapsize设置），由Copy阶段获得的数据，会存放的这个缓冲区中，同样，当到达阀值时会发生溢写到磁盘操作，这个过程中如果设置了Combiner也是会执行的，这个过程会一直执行直到所有的map输出都被复制过来，如果形成了多个磁盘文件还会进行合并，最后一次合并的结果作为reduce的输入而不是写入到磁盘中。 reduce执行 当Reducer的输入文件确定后，整个Shuffle操作才最终结束。之后就是Reducer的执行了，最后Reducer会把结果存到HDFS上。 案例：分区函数使用模拟数据： aa 1 2 bb 2 22 cc 11 dd 1 ee 99 99999 ff 12 23123 gg 12 41441 442 hh 11 41435 412 现在需要根据字段的个数分区，字段大于3个的为long，小于3个的为short，等于3个的为right。 下面是map、getPartition和reduce函数的实现： 1234567891011121314151617181920212223242526272829303132333435363738394041public static class MyPartitionerMapper extends Mapper&lt;LongWritable, Text, Text, Text&gt;&#123; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; String [] arr_value = value.toString().split(" "); if (arr_value.length &gt; 3)&#123; context.write(new Text("long"), value); &#125; else if (arr_value.length &lt; 3)&#123; context.write(new Text("short"), value); &#125; else &#123; context.write(new Text("right"), value); &#125; &#125; &#125;public static class MyPartitionerPartitioner extends Partitioner&lt;Text, Text&gt;&#123; @Override public int getPartition(Text key, Text value, int numPartitions) &#123; int result = 0; if (key.toString().equals("long"))&#123; result = 0 % numPartitions; &#125; else if (key.toString().equals("short"))&#123; result = 1 % numPartitions; &#125; else&#123; result = 2 % numPartitions; &#125; return result; &#125; &#125;public static class MyPartitionerReducer extends Reducer&lt;Text, Text, Text, Text&gt;&#123; @Override protected void reduce(Text key, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException &#123; // just output data from map for (Text val: values)&#123; context.write(key, val); &#125; &#125; &#125;]]></content>
      <categories>
        <category>Big Data</category>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Big Data</tag>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flask 从入门到放弃6: 网站结构最佳实践]]></title>
    <url>%2F2017%2F08%2F28%2FFlask%20%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E6%94%BE%E5%BC%836%3A%20%E7%BD%91%E7%AB%99%E7%BB%93%E6%9E%84%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5%2F</url>
    <content type="text"><![CDATA[来自狗书第七章：大型程序的结构 项目结构下图显示多文件的Flask程序的基本结构 这个结构有四个顶层目录： Flask应用一般放置在名为app的目录下 migrations目录包含数据库迁移脚本 单元测试放置在test目录下 venv目录包含Python虚拟环境 还有一些新的文件： requirements.txt列出一些依赖包，这样就可以很容易的在不同的计算机上部署一个相同的虚拟环境。 config.py存储了一些配置设置。 manage.py用于启动应用程序和其他应用程序任务。 配置选项在这里配置应用相关的配置，例如Form用到的SECRET_KEY, 数据库配置的SQLALCHEMY_DATABASE_URI等配置信息。也可以在这里配置开发、测试和生产环境所需要的不同配置等。 config.py文件： 12345678910111213141516171819202122232425262728293031323334353637383940import osbasedir = os.path.abspath(os.path.dirname(__file__))class Config: SECRET_KEY = os.environ.get('SECRET_KEY') or 'hard to guess string' SQLALCHEMY_COMMIT_ON_TEARDOWN = True FLASKY_MAIL_SUBJECT_PREFIX = '[Flasky]' FLASKY_MAIL_SENDER = 'Flasky Admin &lt;flasky@example.com&gt;' FLASKY_ADMIN = os.environ.get('FLASKY_ADMIN') @staticmethod def init_app(app): passclass DevelopmentConfig(Config): DEBUG = True MAIL_SERVER = 'smtp.googlemail.com' MAIL_PORT = 587 MAIL_USE_TLS = True MAIL_USERNAME = os.environ.get('MAIL_USERNAME') MAIL_PASSWORD = os.environ.get('MAIL_PASSWORD') SQLALCHEMY_DATABASE_URI = os.environ.get('DEV_DATABASE_URL') or \ 'sqlite:///' + os.path.join(basedir, 'data-dev.sqlite')class TestingConfig(Config): TESTING = True SQLALCHEMY_DATABASE_URI = os.environ.get('TEST_DATABASE_URL') or \ 'sqlite:///' + os.path.join(basedir, 'data-test.sqlite')class ProductionConfig(Config): SQLALCHEMY_DATABASE_URI = os.environ.get('DATABASE_URL') or \ 'sqlite:///' + os.path.join(basedir, 'data.sqlite')config = &#123; 'development': DevelopmentConfig, 'testing': TestingConfig, 'production': ProductionConfig, 'default': DevelopmentConfig&#125; 程序包应用程序包放置了所有应用程序代码、模板(templates)和静态文件(static)。数据库模型和电子邮件支持功能也要置入到这个包中，以app/models.py和app/email.py形式存入自己的模块当中 使用工厂函数创建程序实例单个文件中开发程序，创建程序实例在全局作用域中，无法动态修改配置，对于单元测试来说很重要，必须在不同的配置环境中运行程序。 这里把创建程序实例的过程放到可显示调用的工厂函数中。(app.__init__.py) 12345678910111213141516171819202122from flask import Flaskfrom flask_bootstrap import Bootstrapfrom flask_sqlalchemy import SQLAlchemyfrom config import configbootstrap = Bootstrap()db = SQLAlchemy()def create_app(config_name): app = Flask(__name__) app.config.from_object(config[config_name]) config[config_name].init_app(app) bootstrap.init_app(app) mail.init_app(app) moment.init_app(app) db.init_app(app) ## route and errorhandler return app 工厂函数返回创建的Flask程序实例。create_app()函数就是程序的工厂函数，接收一个参数就是程序所使用的配置名。这时候，是动态获得了程序实例了，但是定义路由需要在调用工厂函数之后才能使用app.route装饰器定义路由。这时候就引入蓝图(Blueprint)来进行路由的定义。 蓝图中实现程序功能蓝图定义的路由处于休眠状态，直到蓝图注册到应用实例上后，路由才真正成为程序的一部分，蓝图的使用对于大型程序的模块化开发提供了方便。 app/main/__init__.py 12345from flask import Blueprintmain = Blueprint('main', __name__)from . import views, errors 这里定义蓝图main，程序的路由在views.py中，错误处理程序在errors.py中 在工厂函数create_app()中将蓝图注册到程序实例上 12345678def create_app(config_name): app = Flask(__name__) # ... from .main import main as main_blueprint app.register_blueprint(main_blueprint) return app 启动脚本在顶级文件夹中的manage.py文件用于启动应用程序 manage.py 123456789101112131415161718192021#!/usr/bin/env pythonimport osfrom app import create_app, dbfrom app.models import User, Rolefrom flask_script import Manager, Shellfrom flask_migrate import Migrate, MigrateCommandapp = create_app(os.getenv('FLASK_CONFIG') or 'default')manager = Manager(app)migrate = Migrate(app, db)def make_shell_context(): return dict(app=app, db=db, User=User, Role=Role)manager.add_command("shell", Shell(make_context=make_shell_context))manager.add_command('db', MigrateCommand)if __name__ == '__main__': manager.run() 需求文件包含一个requirement.txt文件，用于记录所有依赖包以及精准的版本号，pip使用下面命令生成需求文件 1(venv) $ pip freeze &gt; requirements.txt 在一个新环境中，使用下面的命令创建一个相同的环境 1(venv) $ pip install -r requirements.txt]]></content>
      <categories>
        <category>Flask从入门到放弃</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Flask</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flask 从入门到放弃5: 数据库操作]]></title>
    <url>%2F2017%2F08%2F28%2FFlask%20%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E6%94%BE%E5%BC%835%3A%20%E6%95%B0%E6%8D%AE%E5%BA%93%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[Flask-SQLAlchemy是一个Flask扩展，它简化了在Flask应用程序中对SQLAlchemy的使用。SQLAlchemy是一个强大的关系数据库框架，支持一些数据库后端。提供高级的ORM和底层访问数据库的本地SQL功能。 通过pip安装Flask-SQLAlchemy： 1(venv) $ pip install flask-sqlalchemy 对于一个Flask应用，我们需要先创建Flask应用，选择加载配置，然后创建SQLAlchemy对象时候把Flask应用传递给它作为参数。 12345678910111213from flask_sqlalchemy import SQLAlchemyfrom config import configdb = SQLAlchemy()def create_app(config_name): app = Flask(__name__) app.config.from_object(config[config_name]) config[config_name].init_app(app) db.init_app(app) return app 配置下面是Flask-SQLAlchemy中常用的配置值。Flask-SQLAlchemy从Flask主配置(config.py)中加载这些值。 SQLALCHEMY_DATABASE_URI：用于数据库的连接，例如sqlite:////tmp/test.db SQLALCHEMY_TRACK_MODIFICATIONS：如果设置成True(默认情况)，Flask-SQLAlchemy将会追踪对象的修改并且发送信号。这需要额外的内存，如果不必要的可以禁用它。 SQLALCHEMY_COMMIT_ON_TEARDOWN：每次request自动提交db.session.commit() 更多的配置键请参考Flask-SQLAlchemy官方文档。 常见数据库的连接URI格式如下所示： Database URI Postgres postgresql://scott:tiger@localhost/mydatabase MySQL mysql://scott:tiger@localhost/mydatabase Oracle oracle://scott:tiger@127.0.0.1:1521/sidname SQLite sqlite:////absolute/path/to/foo.db 模型定义在ORM中，模型一般是一个Python类，类中的属性对应为数据表中的列 (app/models.py) 12345678910from . import dbclass User(db.Model): __tablename__ = 'users' id = db.Column(db.Integer, primary_key=True) username = db.Column(db.String(64), unique=True, index=True) def __repr__(self): return '&lt;User %r&gt;' % self.username 这个模型创建了两个字段，他们是类db.Column的实例，id和username。db.Column 类构造函数的第一个参数是数据库列和模型属性的类型，下面列出了一些常见的列类型以及在模型中使用的Python类型： Integer：普通整数，一般是32bit String：变长字符串 Text：变长字符串，对较长或不限长度的字符做了优化 Boolean：布尔值 Date：日期 DateTime：日期和时间 db.Column 中其余的参数指定属性的配置选项。下面列出了一些常用选项： primary_key：如果设置为True，这列就是表的主键 unique：如果设置为True，这列不允许出现重复的值 index：如果设置为True，为这列创建索引，提升查询效率 default：为这列定义默认值 一对多关系关系型数据库使用主外键关系把不同表中的行联系起来。关系可能分为：一对一、一对多、多对多等。关系使用db.relationship()函数表示，外键使用db.ForeignKey来单独声明。 123456789101112131415161718class Role(db.Model): __tablename__ = 'roles' id = db.Column(db.Integer, primary_key=True) name = db.Column(db.String(64), unique=True) users = db.relationship('User', backref='role', lazy='dynamic') def __repr__(self): return '&lt;Role %r&gt;' % self.nameclass User(db.Model): __tablename__ = 'users' id = db.Column(db.Integer, primary_key=True) username = db.Column(db.String(64), unique=True, index=True) role_id = db.Column(db.Integer, db.ForeignKey('roles.id')) def __repr__(self): return '&lt;User %r&gt;' % self.username user表中的db.ForeignKey(&#39;roles.id&#39;)属性说明，user表的role_id列被定义为外键去关联roles表的id列。在role表中添加的users属性将返回与角色相关联的用户组成的列表(代表这个关系的面向对象视角) 常用的配置选项如下所示： backref：在关系的另一个模型中添加反向引用 primaryjoin：明确指定两个模型之间使用的联结条件。只在模棱两可的关系中需要指定 lazy：决定了SQLAlchemy什么时候从数据库中加载数据。可选值有 select(首次访问时按需加载)、immediate(源对象加载后就加载)、 joined(加载记录，但使用联结)、 subquery (立即加载，但使用子查询)，noload(永不加载)和 dynamic(不加载记录，但提供加载记录的查询) 多对多关系在学习数据库时，处理多对多关系的方法是用到第三张表，也就是关系表。 数据库基本操作在创建好Flask应用、配置好，创建了SQLAlchemy对象之后，就可以对定义好的数据库进行操作了，主要操作就是增删改查四项。 创建表1db.create_all() 这时候在程序目录中创建了一个.sqlite的文件，这个就是数据库文件了。 插入数据12345user_john=User(username='john')#添加到数据库会话db.session.add(user_john)#提交db.session.commit() 数据库会话(db.session)和Flask的session没有什么关系，数据库会话就是关系型数据库的事务，事务能够保证数据库的一致性，提交操作使用原子方式把会话中的对象全部写入数据库，如果发生错误，可以回滚(db.session.rollback())。 修改数据123admin_role = 'new role name'db.session.add(admin_role)db.session.commit() 删除数据数据库会话也有删除操作 123#删除行db.session.delete(user_john)db.session.commit() 查询数据Flask-SQLAlchemy 在Model类上提供了 query 属性。访问它，会得到一个新的所有记录的查询对象。 12#查询行User.query.all() 使用过滤器可以配置query对象进行更精确的数据库查询: filter()：把过滤器添加到原查询上，返回一个新查询 filter_by()：把等值过滤器添加到原查询上，返回一个新查询 limit()：使用指定的值限制原查询返回的结果数量，返回一个新查询 offset()：偏移原查询返回的结果，返回一个新查询 order_by()：根据指定条件对原查询结果进行排序，返回一个新查询 group_by()：根据指定条件对原查询结果进行分组，返回一个新查询 在查询上应用指定的过滤器后，通过调用all()执行查询，以列表的形式返回结果。除了all()之外，还有其他方法能触发查询执行。下面列出常用的执行查询方法： all()：以列表形式返回查询的所有结果 first()：返回查询的第一个结果，如果没有结果，则返回 None first_or_404()：返回查询的第一个结果，如果没有结果，则终止请求，返回 404 错误响应 get()：返回指定主键对应的行，如果没有对应的行，则返回 None get_or_404()：返回指定主键对应的行，如果没找到指定的主键，则终止请求，返回 404 错误响应 count()：返回查询结果的数量 paginate()：返回一个 Paginate 对象，它包含指定范围内的结果]]></content>
      <categories>
        <category>Flask从入门到放弃</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Flask</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flask 从入门到放弃4: Web表单]]></title>
    <url>%2F2017%2F08%2F15%2FFlask%20%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E6%94%BE%E5%BC%834%3A%20Web%E8%A1%A8%E5%8D%95%2F</url>
    <content type="text"><![CDATA[HTML表单用于搜集不同类型的用户输入，是Web应用和用户交互的一种HTML元素。关于表单的基础知识可以去 W3School HTML 表单 去复习一下。 先歪个楼回顾一下HTTP请求的基础知识 HTTP 方法：GET 对比 POST Flask中的request对象可以存储来自客户端的所有信息，其中可以通过 request.form 来获得POST请求所提交的表单数据。在处理表单的时候，像构建表单、表单数据验证是重复并且很繁琐的。可以使用 Flask-WTF 来简化相关操作。 1(venv) $ pip install flask-wtf Flask-WTF是一个集成了WTForms的Flask扩展，使用它你可以在python文件里创建表单类，然后在HTML使用它提供的函数渲染表单。 创建Flask-WTF表单跨站请求伪造保护CSRFFlask-WTF默认支持CSRF（跨站请求伪造）保护，只需要在程序中设置一个密钥。Flask-WTF使用这个密钥生成加密令牌，再用令牌验证表单中数据的真伪。 12app = Flask(__name__)app.config['SECRET_KEY'] = 'DontTellAnyone' 表单类从Flask-WTF导入FlaskForm类，再从WTForms导入表单字段和验证函数 123from flask_wtf import FlaskFormfrom wtforms import StringField, PasswordField, SubmitFieldfrom wtforms.validators import DataRequired, Length, Email, AnyOf 每个表单都用一个继承自FlaskForm的类表示，每个字段都用一个对象表示，每个对象可以附加多个验证函数。常见的验证函数有Required()，Length()，Email()等。 下面创建自己的表单类： 1234class LoginForm(FlaskForm): email = StringField(u'邮箱', validators=[DataRequired(message=u'邮箱不能为空'), Length(1, 64), Email(message=u'请输入有效的邮箱地址，比如：username@domain.com')]) password = PasswordField(u'密码', validators=[DataRequired(message=u'密码不能为空'), Length(min=5, max=13), AnyOf(['secret', 'password'])]) submit = SubmitField(u'登录') 自定义的表单类里面包含了三个元素：email文本框、password密码框和提交按钮。其中文本框和密码框引入了一些验证函数。 表单渲染在视图函数中引入自定义的表单类的实例，然后在返回模版的时候传入这个实例： 12345678910@app.route('/login', methods=['GET', 'POST'])def login(): form = LoginForm() if form.validate_on_submit(): email = form.email.data password = form.password.data print "email: %s password: %s" % (email, password) print 'Form Successfully Submitted!' flash(u'登录成功，欢迎回来！', 'info') return render_template('wtf.html', form=form) 在模板中使用下面方式渲染模版：(wtf.html) 123456&lt;form class="form" method="POST"&gt; &#123;&#123; form.hidden_tag() &#125;&#125; &#123;&#123; form.email.label &#125;&#125;&#123;&#123; form.email() &#125;&#125; &#123;&#123; form.password.label &#125;&#125;&#123;&#123; form.password() &#125;&#125; &#123;&#123; form.submit() &#125;&#125;&lt;/form&gt; 使用Bootstrap渲染表单上篇渲染模版，提到了使用Flask-Bootstrap提供比较美观的模版，现在就用这种方式渲染表单。 wtf.html模版首先继承bootstrap的基模版，然后再引入bootstrap对wtf支持的文件。 123456789101112131415161718&#123;% extends "bootstrap/base.html" %&#125;&#123;% import 'bootstrap/wtf.html' as wtf %&#125;&#123;% block title %&#125;WTForm extends Bootstrap&#123;% endblock %&#125;&#123;% block content %&#125;&lt;form class="form" method="POST"&gt; &lt;dl&gt; &#123;&#123; form.csrf_token &#125;&#125; &#123;&#123; wtf.form_field(form.email) &#125;&#125; &#123;&#123; wtf.form_field(form.password) &#125;&#125; &lt;button type="submit" class="btn btn-primary"&gt;Sign in&lt;/button&gt; &lt;/dl&gt;&lt;/form&gt;&#123;% endblock %&#125; 到这里，一个简单的表单就已经创建好了，当用户第一次访问到这个页面的时候，服务器接收到一个GET方法请求，validate_on_submit() 返回False，if分支内的内容会跳过；当用户通过POST方法来提交请求时，validate_on_submit()调用Required()来验证name属性，如验证通过，if内的逻辑会被执行。而模板内容最后会被渲染到页面上。 处理表单数据表单数据验证当点击了submit提交按钮的时候，form.validate_on_submit() 方法会判断： 通过is_submitted()来判断是否提交了表单 通过WTForms提供的validate()验证方法验证表单数据是否符合规则。 当然，在自定义的表单类中也可以写自己的验证方法，比如从数据库中查找用户名是否被占用：(class LoginForm) 123def validate_username(self, field): if User.query.filter_by(username=field.data).first(): raise ValidationError(u'用户名已被注册，换一个吧。') 在表单数据验证通过后，使用 form..data来访问表单的单个值 存储表单数据当接收POST请求的时候，可以从form..data中获取数据，请求结束后数据就丢失了，这时候可以使用session来存储起来，提供给以后的请求使用，在下面的代码中，就把POST提交之后的 12345678form = NameForm()if form.validate_on_submit(): old_name = session.get('name') if old_name is not None and old_name != form.name.data: flash("Looks like you have changed your name!") session['name'] = form.name.data return redirect(url_for('hello_somebody'))return render_template('userform_index.html', form=form, name=session.get('name'))]]></content>
      <categories>
        <category>Flask从入门到放弃</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Flask</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flask 从入门到放弃3: 渲染模版]]></title>
    <url>%2F2017%2F08%2F14%2FFlask%20%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E6%94%BE%E5%BC%833%3A%20%E6%B8%B2%E6%9F%93%E6%A8%A1%E7%89%88%2F</url>
    <content type="text"><![CDATA[MVCMVC：Model-View-Controller，中文名“模型-视图-控制器” Model（模型）是应用程序中用于处理应用程序数据逻辑的部分。通常模型对象负责在数据库中存取数据。 View（视图）是应用程序中处理数据显示的部分。通常视图是依据模型数据创建的。 Controller（控制器）是应用程序中处理用户交互的部分。通常控制器负责从视图读取数据，控制用户输入，并向模型发送数据。 Flask支持MVC模型，Flask默认使用Jinjia2模板引擎，对模版进行渲染，最终生成HTML文件。视图方法有两个作用：处理业务逻辑（比如操作数据库）和 返回响应内容。模板起到了将两者分开管理的作用。 下面介绍自动生成HTML的方法：模版渲染 模版默认情况下,Flask 在程序文件夹中的 templates 子文件夹中寻找模板。 一个模版文件 user.html： 12345678&lt;html&gt; &lt;head&gt; &lt;title&gt;&#123;&#123; title &#125;&#125; - microblog&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt;Hello, &#123;&#123; user_name &#125;&#125;!&lt;/h1&gt; &lt;/body&gt;&lt;/html&gt; 调用模版 123456789from flask import Flask, render_template@app.route('/')def index(): return render_template('index.html')@app.route('/user/&lt;name&gt;')def user(name): return render_template('user.html', user_name=name) render_template()第一个参数是模板的名称，然后是 键/值 对，user_name=name左边表示模板中的占位符，右边是当前视图中的变量。 变量类型模板中不仅能使用字符串数字等简单的数据类型，还能接收复杂的数据结构，比如dict、list、obj等。 1234&lt;p&gt;A value from a dictionary: &#123;&#123; mydict['key'] &#125;&#125;.&lt;/p&gt;&lt;p&gt;A value from a list: &#123;&#123; mylist[3] &#125;&#125;.&lt;/p&gt;&lt;p&gt;A value from a list, with a variable index: &#123;&#123; mylist[myintvar] &#125;&#125;.&lt;/p&gt;&lt;p&gt;A value from an object's method: &#123;&#123; myobj.somemethod() &#125;&#125;.&lt;/p&gt; 控制结构Jinjia2能够使用常见的控制流，如下是常用的几种控制流： if else for 12345&#123;% if user %&#125; Hello, &#123;&#123;user&#125;&#125;&#123;% else %&#125; Hello, stranger&#123;% endif %&#125; 12345&lt;ul&gt; &#123;% for comment in comments%&#125; &lt;li&gt;&#123;&#123; comment &#125;&#125;&lt;/li&gt; &#123;% endfor %&#125;&lt;/ul&gt; 模版继承和类继承的方式类似，如果多个页面的大部分内容相同，可以定义一个父模板，包含相同的内容，然后子模板继承内容，并根据需要进行部分修改。block标签定义的元素可以在衍生模板中修改。 123456789101112131415&lt;!DOCTYPE html&gt;&lt;html lang="en"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &#123;% block head%&#125; &lt;title&gt; &#123;% block title%&#125;&#123;% endblock%&#125;- My Application &lt;/title&gt; &#123;% endblock %&#125;&lt;/head&gt;&lt;body&gt; &#123;% block body%&#125; &#123;% endblock%&#125;&lt;/body&gt;&lt;/html&gt; 父模版定义了head, title和body三个块，可以在子模板中进行修改。 123456789101112&#123;% extends 'base.html'%&#125;&#123;% block title%&#125; Index&#123;% endblock %&#125;&#123;% block head%&#125; &#123;&#123; super() &#125;&#125; &lt;style&gt; &lt;/style&gt;&#123;% endblock%&#125;&#123;% block body%&#125; &lt;h1&gt;Helll, World!&lt;/h1&gt;&#123;% endblock%&#125; 继承父模版，extends命令声明这个模版继承自base.html。其中，super()这个命令调用父模版的内容。 例子：Jinjia2集成BootstrapBootstrap 是腿特开源的一个开源框架，可以使用这个框架创建一个比较漂亮的网页。这里使用叫做Flask-Bootstrap的Flask扩展。 1(venv) $ pip install flask-bootstrap 安装好之后，就可以在命名空间中导入了。 123from flask_bootstrap import Bootstrap# ...bootstrap = Bootstrap(app) 创建UI的父模版页面整体可以分为两部分：导航条和页面主体。 extends &quot;bootstrap/base.html&quot; 表明这个模版继承自Bootstrap中的bootstrap/base.html。在这个模板中定义了title, navbar, content和page_content这几个块。 自定义404页面UI的框架已经定义好了，下面就可以自定义一个模版，用来显示404页面。 404.html 123456789&#123;% extends "base.html" %&#125;&#123;% block title %&#125;Flasky - Page not found&#123;% endblock %&#125;&#123;% block page_content %&#125;&lt;div class="page-header"&gt; &lt;h1&gt;Not Found&lt;/h1&gt;&lt;/div&gt;&#123;% endblock %&#125; 这个页面继承自上面定义好的父模版，在404.html模版页里面，对title和page_content这两个块的内容进行修改。 现在页面的UI就从丑陋变得稍微美观一点了。]]></content>
      <categories>
        <category>Flask从入门到放弃</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Flask</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 浅复制与深复制]]></title>
    <url>%2F2017%2F07%2F31%2FPython%20%E6%B5%85%E5%A4%8D%E5%88%B6%E4%B8%8E%E6%B7%B1%E5%A4%8D%E5%88%B6%2F</url>
    <content type="text"><![CDATA[Python中，万物皆对象。 在介绍Python的浅复制和深复制之前，先来歪个楼，说明一下Python的可变对象和不可变对象。提到这里，有两个坑不得不拿出来说一下。 坑1：可变对象作为函数默认值先介绍一个Python里面常见的坑： 1234567def append_to_list(value, def_list=[]): def_list.append(value) return def_listmy_list = append_to_list(1)my_other_list = append_to_list(2)print my_other_list 这时候的输出是什么呢？ 1[1, 2] 意不意外？惊不惊喜？ :) 为什么呢？这是因为这个默认值是在函数建立的时候就生成了, 每次调用都是用了这个对象的”缓存”。下面图表示第二次调用函数 append_to_list时候的引用状况： 这就是一条避坑指南：不要使用可变对象作为函数默认值 12345def append_to_list(value, def_list=None): if def_list is None: def_list = [] def_list.append(value) return def_list 坑2：list += 的不同行为123456789a1 = range(3)a2 = a1a2 += [3]print a1, a2a1 = range(3)a3 = a1a3 = a3 + [3]print a1, a3 你会发现第一段代码的结果a1和a2都为[0,1,2,3]，而第二段的代码a1为[0,1,2] a3为[0,1,2,3]。为什么两次会不同呢？上学时候老师不是说 a+=b 等价于 a=a+b 的吗？ 可变和不可变数据类型Python中，对象分为可变(mutable)和不可变(immutable)两种类型 字典型(dictionary)和列表型(list)的对象是可变对象 元组（tuple)、数值型（number)、字符串(string)均为不可变对象 下面验证一下这两种类型，可变对象一旦创建之后还可改变但是地址不会发生改变，即该变量指向的还是原来的对象。而不可变对象则相反，创建之后不能更改，如果更改则变量会指向一个新的对象。 12345678910111213s = 'abc' # 不可变对象print(id(s))# 45068120s += 'd'print(id(s))# 75648536l = ['a','b','c']print(id(l)) # 可变对象# 74842504l += 'd'print(id(l))# 74842504 这个案列也就解释了上面坑2的原因，原因：对于可变对象，例子中的list， += 操作调用 __iadd__ 方法，相当于 a1 = a1.__iadd__([3])，是直接在 a2(a1的引用)上面直接更新。而对于 +操作来说，会调用 __add__ 方法，返回一个新的对象。所以对于可变对象来说 a+=b 是不等价于 a=a+b 的。 好了，下面进入正题，如何复制一个对象。 浅复制Python中对象之间的赋值是按引用传递的 (敲黑板！) 标识一个对象唯一身份的是：对象的id(内存地址)，对象类型，对象值，而浅拷贝就是创建一个具有相同类型，相同值但不同id的新对象 如果需要复制对象，可以使用标准库中的copy模块，copy.copy是浅复制，只会复制父对象，而不会复制对象内部的子对象； 对于list对象，使用 list() 构造方法或者切片的方式，做的是浅复制，复制了外层容器，副本中的元素其实是源容器中元素的 引用 1234567l1 = [3, [55, 44], (7, 8, 9)]l2 = list(l1)print(id(l1), id(l2))print(id(l1[1]), id(l2[1]))# 2148206816520 2148206718344# 2148206717768 2148206717768 深复制copy.deepcopy是深复制，会复制对象及其子对象。 12345678910import copyorigin_list = [0, 1, 2, [3, 4]]copy_list = copy.copy(origin_list)deepcopy_list = copy.deepcopy(origin_list)origin_list.append('hhh')origin_list[3].append('aaa')print(origin_list, copy_list, deepcopy_list) 看了下面的引用关系，结果就猜不错了 在原列表后面添加一个元素，不会对复制的两个列表有影响；浅复制列表中最后一个元素是原列表最后一个元素的引用，所以添加一个元素也影响浅复制的列表。结果为 123origin_list: [0, 1, 2, [3, 4, &apos;aaa&apos;], &apos;hhh&apos;]copy_list: [0, 1, 2, [3, 4, &apos;aaa&apos;]]deepcopy_list: [0, 1, 2, [3, 4]]]]></content>
      <tags>
        <tag>Python</tag>
        <tag>Reference</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flask 从入门到放弃2: 深入理解@app.route()]]></title>
    <url>%2F2017%2F07%2F26%2FFlask%20%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E6%94%BE%E5%BC%832%3A%20%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3%40app.route()%2F</url>
    <content type="text"><![CDATA[下面这段代码是Flask的主页上给出的，这是一段Hello World级别的代码段，但是里面包含的概念可一点都不简单。 123456from flask import Flaskapp = Flask(__name__)@app.route("/")def hello(): return "Hello World!" 这里面的 `@app.route(‘/‘)` 到底是什么意思呢，具体又是如何实现的呢？很多初学者都是很迷茫。我在集中精力理解了装饰器之后，慢慢的就对app.route 这个装饰器的原理以及目的有了了解了。 以前写过一篇文章，详细说明了装饰器的概念：搞懂Python装饰器 要是忘了可以随时复习一下。 给装饰器传参数需要外嵌一个工厂函数，调用这个函数，然后返回函数的装饰器 1234567891011121314151617def decorator_factory(enter_message, exit_message): # return this decorator print "In decorator_factory" def simple_deco(func): def wrapper(): print enter_message func() print exit_message return wrapper return simple_deco@decorator_factory("Start", "End")def hello(): print "Hello World"hello() 注意，这里使用 @decorator_factory(&quot;Start&quot;, &quot;End&quot;) 的时候，实际调用的是 decorator_factory 函数。相当于如下调用： 1decorator_factory(&quot;Start&quot;, &quot;End&quot;)(hello) 输出结果为： 创建自己的Flask类现在我们已经有了足够的装饰器的背景知识，可以模拟一下Flask对象里面的内容。 创建route装饰器我们知道，Flask是一个类，而类方法也可以被用作装饰器。 1234567891011121314class MyFlask(): # decorator_factory def route(self, route_str): def decorator(func): return func return decoratorapp = MyFlask()@app.route("/")def hello(): return "Hello World" 这里不想修改被装饰函数的行为，只是想获取被装饰函数的引用，以便后面注册这个函数用。 添加一个存储路由的字典现在，需要一个变量去存储路由和其关联的函数 123456789101112131415161718192021222324class MyFlask(): def __init__(self): self.routes = &#123;&#125; # decorator_factory def route(self, route_str): def decorator(func): self.routes[route_str] = func return func return decorator # access register variable def serve(self, path): view_function = self.routes.get(path) if view_function: return view_function() else: raise ValueError('Route "&#123;&#125;" has not been registered'.format(path))app = MyFlask()@app.route("/")def hello(): return "Hello World" 当给定的路径被注册过则返回函数运行结果，当路径尚未注册时则抛出一个异常。 解释动态URL形如 `@app.route(“/hello/“)` 这样的路径又是如何解析出参数的呢？Flask使用正则的形式表达路径。这样就可以将路径作为一种模式进行匹配。 使用正则表达式123456import reroute_regex = re.compile(r'^/hello/(?P&lt;username&gt;.+)$')match = route_regex.match("/hello/ains")print match.groupdict() 输出结果为：{&#39;username&#39;: &#39;ains&#39;} 现在需要使用 (pattern, view_function) 这个元组来保存路径编译变成一个正则表达式和注册函数的关系。然后在装饰器中，把编译好的正则表达式和注册函数的元组保存在列表中。 1234567891011121314routes = []def build_route_pattern(route): route_regex = re.sub(r'(&lt;\w+&gt;)', r'(?P\1.+)', route) return re.compile("^&#123;&#125;$".format(route_regex))def route(self, route_str): def decorator(func): route_pattern = build_route_pattern(route_str) routes.append((route_pattern, func)) return func return decorator 接下来，再创建一个访问routes变量的函数，如果匹配上，则返回正则表达式匹配组和注册函数组成的元组。 1234567def get_route_match(path): for route_pattern, view_function in routes: m = route_pattern.match(path) if m: return m.groupdict(), view_function return None 再接下来要找出调用view_function的方法，使用来自正则表达式匹配组字典的正确参数 1234567def serve(path): route_match = get_route_match(path) if route_match: kwargs, view_function = route_match return view_function(**kwargs) else: raise ValueError('Route "&#123;&#125;"" has not been registered'.format(path)) 改好的MyFlask类如下： 123456789101112131415161718192021222324252627282930313233class MyFlask(): def __init__(self): self.routes = [] @staticmethod def build_route_pattern(route): route_regex = re.sub(r'(&lt;\w+&gt;)', r'(?P\1.+)', route) return re.compile("^&#123;&#125;$".format(route_regex)) def route(self, route_str): def decorator(func): route_pattern = self.build_route_pattern(route_str) self.routes.append((route_pattern, func)) return func return decorator def get_route_match(self, path): for route_pattern, view_function in self.routes: m = route_pattern.match(path) if m: return m.groupdict(), view_function return None def serve(self, path): route_match = self.get_route_match(path) if route_match: kwargs, view_function = route_match return view_function(**kwargs) else: raise ValueError('Route "&#123;&#125;"" has not been registered'.format(path)) 运行一段带参数的试试 1234567app = MyFlask()@app.route("/hello/&lt;username&gt;")def hello_user(username): return "Hello &#123;&#125;!".format(username)print app.serve("/hello/ains") 下面是程序运行的引用关系图]]></content>
      <categories>
        <category>Flask从入门到放弃</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Flask</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flask 从入门到放弃1: Hello World]]></title>
    <url>%2F2017%2F07%2F25%2FFlask%20%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E6%94%BE%E5%BC%831%3A%20Hello%20World%2F</url>
    <content type="text"><![CDATA[除了Flask，常见的Python Web框架还有： Django：全能型Web框架； web.py：一个小巧的Web框架； Bottle：和Flask类似的Web框架； Tornado：Facebook的开源异步Web框架。 Flask简介Flask 是一个用于 Python 的微型网络开发框架，依赖两个外部库： Jinja2 模板引擎和 Werkzeug WSGI 套件。Flask也被称为microframework，因为它使用简单的核心，用加载扩展的方式增加其他功能。 Flask 没有默认使用的数据库、窗体验证工具。但是，Flask 保留了扩增的弹性，可以用Flask扩展加入这些功能：ORM、窗体验证工具、文件上传、开放式身份验证技术。 准备环境对于Python来说，有相当数量的外部包，如果管理不当，会让人崩溃，创建一个Flask的Web项目更是这样，所以推荐一个项目一套环境。这里使用 virtualenv 在项目的目录中创建这个项目的虚拟环境。 123456$ sudo pip install virtualenv$ mkdir myproject$ cd myproject$ virtualenv venv$ . venv/bin/activate$ pip install Flask Hello World 应用官方文档上给出的hello world例子很小，但是也基本说明了一个Flask应用都包含了什么 123456789from flask import Flaskapp = Flask(__name__)@app.route('/')def hello_world(): return 'Hello World!'if __name__ == '__main__': app.run() 保存代码为 hello.py，在命令行运行 1python hello.py 这时候访问 http://127.0.0.1:5000/ 可以看见页面输出 Hello World! 程序都干了啥 首先导入了 Flask 类，这个类的实例会是WSGI应用程序 接下来创建了这个类的实例 app 然后，使用 route() 装饰器进行路由绑定，通过路由来绑定URL和Python函数的映射关系。 最后使用 run() 函数来让应用运行在本地服务器上。 其中 if __name__ == &#39;__main__&#39;: 确保服务器只会在该脚本被 Python 解释器直接执行的时候才会运行，而不是作为模块导入的时候。 好了，Ctrl+C关闭服务器。到这里一个麻雀虽小五脏俱全的小Flask应用就创建好了。]]></content>
      <categories>
        <category>Flask从入门到放弃</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Flask</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[搞懂Python装饰器]]></title>
    <url>%2F2017%2F07%2F20%2F%E6%90%9E%E6%87%82Python%E8%A3%85%E9%A5%B0%E5%99%A8%2F</url>
    <content type="text"><![CDATA[装饰器是Python中的一个高阶概念，装饰器是可调用的对象，其参数是另外一个函数。装饰器可能会处理被装饰的函数然后把它返回，或者将其替换成另外一个函数或者可调用对象。 这么介绍装饰器确实很难懂，还是以例子逐步理解更容易些。 装饰器的强大在于它能够在不修改原有业务逻辑的情况下对代码进行扩展，常见的应用场景有：权限校验、用户认证、日志记录、性能测试、事务处理、缓存等。 下面记录一下我逐步理解装饰器的过程。 一等函数在Python中，函数是一等对象，也就是说函数是满足以下条件的程序实体： 在运行时创建 能赋值给变量或者数据结构中的元素 能作为参数传给函数 能作为函数的返回结果 下面先看一个简单函数的定义 12def hello(): print("Hello world!") python解释器遇到这段代码的时候，发生了两件事： 编译代码生成一个函数对象 将名为”hello”的名字绑定到这个函数对象上 Python中函数是一等对象，也就是说函数可以像int、string、float对象一样作为参数、或者作为返回值等进行传递。 函数作为参数123456789101112def foo(bar): return bar + 1print(foo)print(foo(2))print(type(foo))def call_foo_with_arg(foo, arg): return foo(arg)print(call_foo_with_arg(foo, 3)) 函数 call_foo_with_arg 接收两个参数，其中一个是可被调用的函数对象 foo 嵌套函数函数也可以定义在另外一个函数中，作为嵌套函数 1234567891011def parent(): print("Printing from the parent() function.") def first_child(): return "Printing from the first_child() function." def second_child(): return "Printing from the second_child() function." print(first_child()) print(second_child()) first_child 和 second_child 函数是嵌套在 parent 函数中的函数。 当调用 parent 函数时，内嵌的first_child 和 second_child 函数也被调用，但是如果在 parent 函数中并不是调用first_child 和 second_child 函数， 而是返回这两个函数对象呢？ 下面歪个楼，先介绍一下Python的变量作用域的规则。 变量作用域Python是动态语言，Python的变量名解析机制有时称为LEGB法则，当在函数中使用未认证的变量名时，Python搜索4个作用域： local 函数内部作用域 enclosing 函数内部与内嵌函数之间 global 全局作用域 build-in 内置作用域 12345def f1(a): print(a) print(b)f1(3) 这段程序会抛出错误：”NameError: name ‘b’ is not defined”，这是因为在函数体内，Python编译器搜索上面 LEGB 的变量，没有找到。 再下面一个例子： 1234567b = 6def f2(a): print(a) print(b) b = 9f2(3) 在Python中，Python不要求声明变量，但是假定在函数体中被赋值的变量是局部变量，所以在这个函数体中，变量b被判断成局部变量，所以在print(b)调用时会抛出 “UnboundLocalError: local variable ‘b’ referenced before assignment” 的错误。 要想上面的代码运行，就必须手动在函数体内声明变量b为全局变量 12345678b = 6def f2(a): global b print(a) print(b) b = 9f2(3) 闭包有了上面的背景知识，下面就可以介绍闭包了。闭包是指延伸了作用域的函数。 要想理解这个概念还是挺难的，下面还是用例子来说明。 现在有个avg函数，用于计算不断增长的序列的平均值。 12345678910111213def make_averager(): series = [] def averager(new_value): series.append(new_value) total = sum(series) return total / len(series) return averageravg = make_averager()avg(10)avg(11) 调用函数 make_averager 时候，返回一个 averager 函数对象，每次调用 averager 函数，会把参数添加到 series 中，然后计算当前平均值。 series 是 make_averager 函数的局部变量，调用 avg(10) 时， make_averager 函数已经返回，所以本地作用域也就不存在了。但是在 averager 函数中，series 是自由变量 这里的 avg 就是一个闭包，本质上它还是函数，闭包是引用了自由变量(series)的函数(averager) nonlocal声明刚才的例子稍稍改动一下，使用total和count来计算移动平均值 12345678910111213def make_averager(): count = 0 total = 0 def averager(new_value): count += 1 total += new_value return total / count return averageravg = make_averager()avg(10) 这时候会抛出错误 “UnboundLocalError: local variable ‘count’ referenced before assignment”。这是因为：当count为数字或者任何不可变类型时，在函数体定义中 count = count + 1 实际上是为count赋值，所以count就变成了局部变量。为了避免这个问题，python3引入了 nonlocal 声明，作用是把变量标记成 自由变量 12345678910111213141516def make_averager(): count = 0 total = 0 def averager(new_value): nonlocal count, total count += 1 total += new_value return total / count return averageravg = make_averager()print(avg(10))print(avg(11))print(avg(12)) 装饰器了解了闭包之后，下面就可以用嵌套函数实现装饰器了。事实上，装饰器就是一种闭包的应用，只不过传递的是函数。 无参数装饰器下面写一个简单的装饰器的例子 123456789101112131415161718def makebold(fn): def wrapped(): return '&lt;b&gt;' + fn() + '&lt;/b&gt;' return wrappeddef makeitalic(fn): def wrapped(): return '&lt;i&gt;' + fn() + '&lt;/i&gt;' return wrapped@makebold@makeitalicdef hello(): return "Hello World"print(hello()) makeitalic 装饰器将函数 hello 传递给函数 makeitalic，函数 makeitalic 执行完毕后返回被包装后的 hello 函数，而这个过程其实就是通过闭包实现的 装饰器有一个语法糖@,直接@my_new_decorator就把上面一坨代码轻松化解了，这就是Pythonic的代码，简洁高效，使用语法糖其实等价于下面显式使用闭包 12hello_bold = makebold(hello)hello_italic = makeitalic(hello) 装饰器是可以叠加使用的，对于Python中的”@”语法糖，装饰器的调用顺序与使用 @ 语法糖声明的顺序相反，上面案例中叠加装饰器相当于如下包装顺序： 1hello = makebold(makeitalic(hello)) 被装饰的函数带参数再来一个例子 12345678910111213141516171819202122232425262728293031323334353637import timeimport functoolsdef clock(func): @functools.wraps(func) def clocked(*args, **kwargs): """ in wrapper """ t0 = time.time() # execute result = func(*args, **kwargs) elapsed = time.time() - t0 name = func.__name__ arg_lst = [] if args: arg_lst.append(', '.join(repr(arg) for arg in args)) if kwargs: pairs = ['%s=%r' % (k, w) for k, w in sorted(kwargs.items())] arg_lst.append(', '.join(pairs)) arg_str = ', '.join(arg_lst) print('[%0.8fs] %s(%s) -&gt; %r ' % (elapsed, name, arg_str, result)) return result return clocked@clockdef snooze(seconds): """ sleep for seconds """ time.sleep(seconds)@clockdef factorial(n): """ calculate n! """ return 1 if n&lt;2 else n*factorial(n-1) snooze和factorial函数会作为func参数传给clock函数，然后clock函数会返回clocked函数。所以现在factorial保留的是clocked函数的引用。但是这也是装饰器的一个副作用：会把被装饰函数的一些元数据，例如函数名、文档字符串、函数签名等信息覆盖掉。下面会使用functools库中的 @wraps 装饰器来避免这个。 内嵌包装函数 clocked 的参数跟被装饰函数的参数对应，这里使用了 (*args, **kwargs)，是为了适应可变参数。 clocked函数做了以下几件事： 记录初始时间 调用原来的factorial函数，保存结果 计算经过的时间 格式化收集的数据，然后打印出来 返回第2步保存的结果 12print('*'*40, 'Calling factorial(6)')print('6! = ', factorial(6)) 装饰器的典型行为就是：把被装饰的函数体换成新函数，二者接受相同的参数，返回被装饰的函数本该返回的值，同时有额外操作 另外，内嵌包装函数 clocked 添加了functools库中的 @wraps 装饰器，这个装饰器可以把被包装函数的元数据，例如函数名、文档字符串、函数签名等信息保存下来。 123print(snooze(5))print(snooze.__doc__)print('origin func name is:', snooze.__name__) 1234[5.01506114s] snooze(5) -&gt; NoneNone sleep for secondsorigin func name is: snooze 参数化装饰器如果装饰器本身需要传入参数，那就需要编写一个返回decorator的高阶函数，也就是针对装饰器进行装饰。 下面代码来自 Python Cookbook： 123456789101112131415161718192021222324252627282930from functools import wrapsimport loggingdef logged(level, name=None, message=None): """ Add logging to a function. level is the logging level, name is the logger name, and message is the log message. If name and message aren't specified, they default to the function's module and name. """ def decorate(func): logname = name if name else func.__module__ log = logging.getLogger(logname) logmsg = message if message else func.__name__ @wraps(func) def wrapper(*args, **kwargs): log.log(level, logmsg) return func(*args, **kwargs) return wrapper return decorate# Example use@logged(logging.DEBUG)def add(x, y): return x + y@logged(logging.CRITICAL, 'example')def spam(): print('Spam!') 最外层的函数 logged() 接受参数并将它们作用在内部的装饰器函数上面。 内层的函数 decorate() 接受一个函数作为参数，然后在函数上面放置一个包装器。这个装饰器的处理过程相当于： 1spam = logged(x, y)(spam) 首先执行logged(&#39;x&#39;, &#39;y&#39;)，返回的是 decorate 函数，再调用返回的函数，参数是 spam 函数。 装饰器在真实世界的应用更多的装饰器的案例： PythonDecoratorLibrary 1. 给函数调用做缓存像求第n个斐波那契数来说，是个递归算法，对于这种慢速递归，可以把耗时函数的结果先缓存起来，在调用函数之前先查询一下缓存，如果没有才调用函数 12345678910111213141516171819202122from functools import wrapsdef memo(func): cache = &#123;&#125; miss = object() @wraps(func) def wrapper(*args): result = cache.get(args, miss) if result is miss: result = func(*args) cache[args] = result return result return wrapper@memo@clockdef fib(n): if n &lt; 2: return n return fib(n-2) + fib(n-1) 也可以使用下面的functools库里面的 lru_cache 装饰器来实现缓存。 2. LRUCacheLRU就是Least Recently Used，即最近最少使用，是一种内存管理算法。 12345678910import functools@functools.lru_cache()@clockdef fibonacci(n): if n &lt; 2: return n return fibonacci(n-2) + fibonacci(n-1)print(fibonacci(6)) 3. 给函数输出记日志1234567891011121314151617181920212223import timefrom functools import wrapsdef log(func): @wraps(func) def wrapper(*args, **kwargs): print("Function running") ts = time.time() result = func(*args, **kwargs) te = time.time() print("Function = &#123;0&#125;".format(func.__name__)) print("Arguments = &#123;0&#125; &#123;1&#125;".format(args, kwargs)) print("Return = &#123;0&#125;".format(result)) print("time = %.6f seconds" % (te - ts)) return wrapper@logdef sum(x, y): return x + yprint(sum(1, 2)) 4. 数据库连接1234567891011121314151617181920def open_and_close_db(func): def wrapper(*a, **k): conn = connect_db() result = func(conn=conn, *a, **k) conn.commit() conn.close() return result return wrapper@open_and_close_dbdef query_for_dict(sql, conn): cur = conn.cursor() try: cur.execute(sql) conn.commit() entries = [dict(zip([i[0] for i in cur.description], row)) for row in cur.fetchall()] print entries except Exception,e: print e return entries 5. Flask路由拿Flask的 hello world来说： 123456789from flask import Flaskapp = Flask(__name__)@app.route("/")def hello(): return "Hello World!"if __name__ == '__main__': app.run() 到这儿，装饰器的一些基本概念就都清楚了。 参考Python 的闭包和装饰器 Fluent Python]]></content>
      <tags>
        <tag>Python</tag>
        <tag>装饰器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python dict类型的实现]]></title>
    <url>%2F2017%2F07%2F18%2FPython%20dict%E7%B1%BB%E5%9E%8B%E7%9A%84%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[程序员们的经验里面，通常都会认为字典和集合的速度是非常快的，字典的搜索的时间复杂读为O(1)， 为什么能有这么快呢？在于字典和集合的后台实现。 散列表 Hash table散列表是一个稀疏数组，散列表里面的单元叫做表元 bucket， 在dict的散列表中，每个键值对都占用一个表元，每个表元有两个部分：一个对键值的引用，一个对值的引用。因为所有表元大小一致，可以通过偏移量来读取某个表元。由于是稀疏数组，python会设法保证还有大约三分之一的表元是空的，快要到达这个阈值的时候，会把原有的散列表复制到一个更大的空间里面。 如果要把一个对象放入散列表，那么需要先计算这个元素的散列值 12345map(hash, (0, 1, 2, 3))# [0, 1, 2, 3]map(hash, ("namea", "nameb", "namec", "named"))# [6674681622036885098, -1135453951840843879, 3071659021342785694, 5386947181042036450] 常用构造hash函数的方法构造散列函数有多种方式，比如直接寻址法、数字分析法、平方取中法、折叠法、随机数法、除留余数法等。著名的hash算法: MD5 和 SHA-1 是应用最广泛的Hash算法。 直接寻址法取keyword或keyword的某个线性函数值为散列地址。即H(key)=key或H(key) = a*key + b，当中a和b为常数（这样的散列函数叫做自身函数） 数字分析法分析一组数据，比方一组员工的出生年月日，这时我们发现出生年月日的前几位数字大体同样，这种话，出现冲突的几率就会非常大，可是我们发现年月日的后几位表示月份和详细日期的数字区别非常大，假设用后面的数字来构成散列地址，则冲突的几率会明显减少。因此数字分析法就是找出数字的规律，尽可能利用这些数据来构造冲突几率较低的散列地址。 平方取中法取keyword平方后的中间几位作为散列地址。 折叠法将keyword切割成位数同样的几部分，最后一部分位数能够不同，然后取这几部分的叠加和（去除进位）作为散列地址。 随机数法选择一随机函数，取keyword的随机值作为散列地址，通经常使用于keyword长度不同的场合。 除留余数法取keyword被某个不大于散列表表长m的数p除后所得的余数为散列地址。即 H(key) = key MOD p, p&lt;=m。不仅能够对keyword直接取模，也可在折叠、平方取中等运算之后取模。对p的选择非常重要，一般取素数或m，若p选的不好，easy产生同义词。 散列表算法为了获取my_dict[search_key] 的值， Python会调用hash(search_key) 来计算散列值，把这个值的最低几位数字当做偏移量，在散列表里面查找表元，如果摘到的表元是空的，则抛出KeyError异常，若不是空的， 表元里会有一对found_key: found_value，然后Python会检查search_key是否等于found_key，如果相等，就返回found_value。 如果search_key和found_key不匹配的话，就叫做散列冲突。 散列冲突解决方法开放寻址法 Open addressingPython是使用开放寻址法中的二次探查来解决冲突的。如果使用的容量超过数组大小的2/3，就申请更大的容量。数组大小较小的时候resize为4，较大的时候resize2。实际上是用左移的形式。 字典的C数据结构下面的C结构体来存储一个字典项，包括散列值、键和值。 12345typedef struct &#123; Py_ssize_t me_hash; PyObject *me_key; PyObject *me_value;&#125; PyDictEntry; 下面的结构代表了一个字典 12345678910typedef struct _dictobject PyDictObject;struct _dictobject &#123; PyObject_HEAD Py_ssize_t ma_fill; Py_ssize_t ma_used; Py_ssize_t ma_mask; PyDictEntry *ma_table; PyDictEntry *(*ma_lookup)(PyDictObject *mp, PyObject *key, long hash); PyDictEntry ma_smalltable[PyDict_MINSIZE];&#125;; ma_fill 是使用了的slots加 dummy slots的数量和。当一个键值对被移除了时，它占据的那个slot会被标记为dummy。如果添加一个新的 key 并且新 key 不属于dummy，则 ma_fill 增加 1 ma_used 是被占用了（即活跃的）的slots数量 ma_mask 等于数组长度减一，它被用来计算slot的索引。在查找元素的一个 key 时，使用 slot = key_hash &amp; mask 就能直接获得哈希槽序号 ma_table 一个 PyDictEntry 结构体的数组， PyDictEntry 包含 key 对象、value 对象，以及 key 的散列值 ma_lookup 一个用于查找 key 的函数指针 ma_smalltable 是一个初始大小为8的数组。 Cpython中，使用如下算法来进行二次探查序列查找空闲slot 123i = (5 * i + perturb + 1)slot_index = i &amp; ma_maskperturb &gt;&gt;= 5 字典的使用字典初始化第一次创建一个字典，PyDict_New()函数会被调用 123456789returns new dictionary objectfunction PyDict_New: allocate new dictionary object clear dictionary&apos;s table set dictionary&apos;s number of used slots + dummy slots (ma_fill) to 0 set dictionary&apos;s number of active slots (ma_used) to 0 set dictionary&apos;s mask (ma_value) to dictionary size - 1 = 7 set dictionary&apos;s lookup function to lookdict_string return allocated dictionary object 添加项当添加一个新键值对时PyDict_SetItem()被调用，该函数带一个指向字典对象的指针和一个键值对作为参数。它检查该键是否为字符串并计算它的hash值（如果这个键的哈希值已经被缓存了则用缓存值）。然后insertdict()函数被调用来添加新的键/值对，如果使用了的slots和dummy slots的总量超过了数组大小的2/3则重新调整字典的大小。 12345678910arguments: dictionary, key, valuereturn: 0 if OK or -1function PyDict_SetItem: if key&apos;s hash cached: use hash else: calculate hash call insertdict with dictionary object, key, hash and value if key/value pair added successfully and capacity orver 2/3: call dictresize to resize dictionary&apos;s table insertdict() 使用查找函数lookdict_string来寻找空闲的slot，这和寻找key的函数是一样的。lookdict_string()函数利用hash和mask值计算slot的索引，如果它不能在slot索引（=hash &amp; mask）中找到这个key，它便开始如上述伪码描述循环来探测直到找到一个可用的空闲slot。第一次探测时，如果key为空(null)，那么如果找到了dummy slot则返回之 下面一个列子，如何将{‘a’: 1, ‘b’: 2, ‘z’: 26, ‘y’: 25, ‘c’: 5, ‘x’: 24} 键值对添加到字典里面。(字典结构的表大小为8) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748PyDict_SetItem: key=&apos;a&apos;, value = 1 hash = hash(&apos;a&apos;) = 12416037344 insertdict lookdict_string slot index = hash &amp; mask = 12416037344 &amp; 7 = 0 slot 0 is not used so return it init entry at index 0 with key, value and hash ma_used = 1, ma_fill = 1PyDict_SetItem: key=&apos;b&apos;, value = 2 hash = hash(&apos;b&apos;) = 12544037731 insertdict lookdict_string slot index = hash &amp; mask = 12544037731 &amp; 7 = 3 slot 3 is not used so return it init entry at index 3 with key, value and hash ma_used = 2, ma_fill = 2PyDict_SetItem: key=&apos;z&apos;, value = 26 hash = hash(&apos;z&apos;) = 15616046971 insertdict lookdict_string slot index = hash &amp; mask = 15616046971 &amp; 7 = 3 slot 3 is used so probe for a different slot: 5 is free init entry at index 5 with key, value and hash ma_used = 3, ma_fill = 3PyDict_SetItem: key=&apos;y&apos;, value = 25 hash = hash(&apos;y&apos;) = 15488046584 insertdict lookdict_string slot index = hash &amp; mask = 15488046584 &amp; 7 = 0 slot 0 is used so probe for a different slot: 1 is free init entry at index 1 with key, value and hash ma_used = 4, ma_fill = 4PyDict_SetItem: key=&apos;c&apos;, value = 3 hash = hash(&apos;c&apos;) = 12672038114 insertdict lookdict_string slot index = hash &amp; mask = 12672038114 &amp; 7 = 2 slot 2 is not used so return it init entry at index 2 with key, value and hash ma_used = 5, ma_fill = 5PyDict_SetItem: key=&apos;x&apos;, value = 24 hash = hash(&apos;x&apos;) = 15360046201 insertdict lookdict_string slot index = hash &amp; mask = 15360046201 &amp; 7 = 1 slot 1 is used so probe for a different slot: 7 is free init entry at index 7 with key, value and hash ma_used = 6, ma_fill = 6 移除项PyDict_DelItem()被用来删除一个字典项。key的散列值被计算出来作为查找函数的参数，删除后这个slot就成为了dummy slot。 参考Python dictionary implementation Fluent Python]]></content>
      <tags>
        <tag>Python</tag>
        <tag>dict</tag>
        <tag>hash</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MongoDB索引操作]]></title>
    <url>%2F2017%2F06%2F29%2FMongoDB%E7%B4%A2%E5%BC%95%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[上面那篇MongoDB Documentation关于查询优化的案例，数据只有10条，看不出来性能有多少提升，这篇再尝试一个例子。 准备数据首先先插入1千万条测试数据 123456use testfor(var i = 0; i &lt; 10000000; i++)&#123; var rand = parseInt(i * Math.random()); db.person.insert(&#123;name: "hxc"+i, age: i&#125;)&#125; 数据插入需要好久，这个在以后也是个需要优化的地方。 使用 explain函数分析查询性能现在在person这个collection上面没有创建任何索引，这里使用MongoDB提供的 explain工具分析一下一个查询的性能 1db.person.find(&#123; name: "hxc"+10000&#125;).explain() 上面查询使用的是COLLSCAN，就是表扫描，totalDocsExamined为全部1千万，nReturn为1，最终返回一个文档， executionTimeMillisEstimate为4637，预计耗时4637毫秒。 创建索引试一试在name字段上创建一个索引呢？ 12db.person.ensureIndex(&#123; name: 1&#125;)db.person.find(&#123; name: "hxc"+10000&#125;).explain() 再来看看查询的性能，这回使用的是IDXSCAN，mongodb在后台使用B树结构来存放索引，这里使用的索引名字是name_1，只浏览了一个文档就返回这一个文档，executionTimeMillisEstimate预计耗时0毫秒。 note: 在创建索引的时候，会占用一个写锁，如果数据量很大的话，会产生很多问题，所以建议用background方式为大表创建索引。 1db.person.ensureIndex(&#123; name: 1&#125;, &#123;background: 1&#125;) 创建组和索引有时候查询不是单条件的，可能是多条件，这时候可以创建组合索引来加速查询 1234db.person.ensureIndex(&#123; name: 1, birthday: 1&#125;)db.person.ensureIndex(&#123; birthday: 1, name: 1&#125;)db.person.getIndexes() 下面就是创建好的所有索引 其中第一个索引是在创建collection的时候系统自动创建的一个唯一性索引，key值为 _id。 最后两个是刚才所创建的组合索引，这两个组和索引使用的字段虽然是一样的，但是这是两个完全不同的索引 下面分析一下下面的查询使用的到底是哪个索引 1db.person.find(&#123; birthday: "1989-05-01", name: "mary"&#125;).explain() 可以看出，最终优化器选择了使用name_1_birthday_1这个索引。查询优化器会使用我们建立的这些索引来创建查询方案，优化器会从中选择最优的执行查询，同时也可以看到被优化器拒绝的查询计划。当然如果非要用自己指定的查询方案，这也是可以的，在mongodb中给我们提供了hint方法让我们可以暴力执行。]]></content>
      <tags>
        <tag>MongoDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MongoDB 分析查询计划]]></title>
    <url>%2F2017%2F06%2F29%2FMongoDB%20%E5%88%86%E6%9E%90%E6%9F%A5%E8%AF%A2%E8%AE%A1%E5%88%92%2F</url>
    <content type="text"><![CDATA[查询计划和传统的关系型数据库的执行计划类似，MongoDB也提供了查询计划。MongoDB的查询优化器处理查询语句，并且从生成的执行计划中选择最优的来执行查询过程。下图显示了MongoDB查询计划的逻辑步骤： 分析查询的性能MongoDB 提供了一个 explain 命令让我们获知系统如何处理查询请求。利用 explain 命令，我们可以很好地观察系统如何使用索引来加快检索，同时可以针对性优化索引。 分析实例准备数据123456789101112db.inventory.insertMany([&#123; "_id" : 1, "item" : "f1", type: "food", quantity: 500 &#125;,&#123; "_id" : 2, "item" : "f2", type: "food", quantity: 100 &#125;,&#123; "_id" : 3, "item" : "p1", type: "paper", quantity: 200 &#125;,&#123; "_id" : 4, "item" : "p2", type: "paper", quantity: 150 &#125;,&#123; "_id" : 5, "item" : "f3", type: "food", quantity: 300 &#125;,&#123; "_id" : 6, "item" : "t1", type: "toys", quantity: 500 &#125;,&#123; "_id" : 7, "item" : "a1", type: "apparel", quantity: 250 &#125;,&#123; "_id" : 8, "item" : "a2", type: "apparel", quantity: 400 &#125;,&#123; "_id" : 9, "item" : "t2", type: "toys", quantity: 50 &#125;,&#123; "_id" : 10, "item" : "f4", type: "food", quantity: 75 &#125;]) 无索引的查询在这个collection没有索引的情况下，写一个查询，查询quantity字段的值在100和200之间的文档 1db.inventory.find(&#123; quantity: &#123; $gte: 100, $lte: 200&#125;&#125;) 这时候，我们想知道这个查询语句所选择的查询计划是怎么样的： 123db.inventory.find( &#123; quantity: &#123;$gte: 100, $lte: 200&#125;&#125;).explain("executionStats") 下图为返回的结果： queryPlanner.winningPlan.stage 显示的是 COLLSCAN集合扫描，也就是关系型数据库的全表扫描，看到这个说明性能肯定不好 nReturned为3，符合的条件的返回为3条 totalKeysExamined为0，没有使用index。 totalDocsExamined为10，扫描了所有记录。 优化的方向也很明显， 就是如何减少检查的文档数量。 创建索引在quantity字段上创建索引： 1db.inventory.createIndex( &#123;quantity: 1&#125;) 再次执行上面的命令查看查询计划 queryPlanner.winningPlan.inputStage.stage 显示IXSCAN说明使用了索引 executionStats.nReturned 有3条文档符合条件返回 executionStats.totalKeysExamined 扫描了3个索引 executionStats.totalDocsExamined 一共扫描了3个文档 参考资料： MongoDB Documentation: Analyze Query Performance]]></content>
      <tags>
        <tag>MongoDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MongoDB CRUD 快速复习]]></title>
    <url>%2F2017%2F06%2F28%2FMongoDB%20CRUD%20%E5%BF%AB%E9%80%9F%E5%A4%8D%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[提到数据库的基本操作，无论关系型还是非关系型，首先想到的肯定是数据的增删改查 (Create, Read, Update, Delete)，下面记录一下MongoDB里面的CRUD操作。 INSERT 操作 MongoDB里面数据叫做文档Document，其实就是JSON对象，常见的插入操作分为单条插入 insertOne() 和多条插入 insertMany() ，单条插入传入一个JSON对象，多条插入传入一个多个JSON对象的数组 FIND 操作 Query操作是这4中操作最重要的部分， 查询的语法如下： query criteria，可选。表示集合的查询条件。可指定条件查询参数，或一个空对象({}) projection，可选。表示查询时所要返回的字段，省略此参数将返回全部字段。其格式如下：{ field1: , field2: … } 返回查询文档的游标，即：执行find()方法时其返回的文档，实际是对文档引用的一个游标。当指定projection参数时，返回值仅包含指定的字段和_id字段，也可以指定不返回_id字段 查询参数下面这些在SQL的WHERE子句中的操作符，在MongoDB中都有实现。 SQL : 12&gt;, &gt;=, &lt;, &lt;=, !=And，OR，In，NotIn MongoDB: 12&quot;$gt&quot;, &quot;$gte&quot;, &quot;$lt&quot;, &quot;$lte&quot;, &quot;$ne&quot;&quot;$and&quot;, &quot;$or&quot;, &quot;$in&quot;，&quot;$nin&quot; 举几个查询的例子： 1234567891011121314db.inventory.find( &#123; status: &#123; $in: [ "A", "D" ] &#125; &#125; )# SELECT * FROM inventory WHERE status in ("A", "D")db.inventory.find( $and: [ &#123; status: "A", qty: &#123; $lt: 30 &#125; &#125; ] )# SELECT * FROM inventory WHERE status = "A" AND qty &lt; 30db.inventory.find( &#123; $or: [ &#123; status: "A" &#125;, &#123; qty: &#123; $lt: 30 &#125; &#125; ] &#125; )# SELECT * FROM inventory WHERE status = "A" OR qty &lt; 30db.inventory.find( &#123; status: "A", $or: [ &#123; qty: &#123; $lt: 30 &#125; &#125;, &#123; item: /^p/ &#125; ]&#125; )# SELECT * FROM inventory WHERE status = "A" AND ( qty &lt; 30 OR item LIKE "p%") 如果查询使用的是嵌套的文档的属性，那就使用 &quot;field.nestedField&quot; UPDATE 操作UPDATE 的语法如下： 12345678910db.collection.update( &lt;query&gt;, &lt;update&gt;, &#123; upsert: &lt;boolean&gt;, multi: &lt;boolean&gt;, writeConcern: &lt;document&gt;, collation: &lt;document&gt; &#125;) 更新操作分为整体更新和局部更新，整体更新是用一个新的文档完全替代匹配的文档。 危险： 使用替换更新时应当注意，如果查询条件匹配到多个文档，所有的文档都会被替换 下面主要说的是局部更新。 修改器现在有个文档products12345678910&#123; _id: 100, sku: "abc123", quantity: 250, instock: true, reorder: false, details: &#123; model: "14Q2", make: "xyz" &#125;, tags: [ "apparel", "clothing" ], ratings: [ &#123; by: "ijk", rating: 4 &#125; ]&#125; $set修改器 $set修改器用于指定一个字段的值，字段不存在时，则会创建字段。 修改错误或不在需要的字段，可以使用$unset方法将这个键删除 现在要把文档中 details.make字段的值更新为”zzz” 123456db.products.update( &#123; _id: 100&#125;, &#123; $set: &#123;"details.make": "zzz"&#125;, $currentDate: &#123;lastModified: true&#125; &#125;) 其中，$set操作符将details.make字段的值更新为zzz， $currentDate 操作符用来更新lastModified字段的值为当前时间，如果该字段不存在，$currentDate 操作符将会创建这个字段 $inc修改器 $inc修改器用于字段值的增加和减少 products文档如下：123456789&#123; _id: 1, sku: "abc123", quantity: 10, metrics: &#123; orders: 2, ratings: 3.5 &#125;&#125; 将quantity字段的值减2，并且将orders的值加1 1234db.products.update( &#123; sku: "abc123"&#125;, &#123; $inc: &#123;quiantity: -2, "metrics.orders": 1 &#125;&#125;) DELETE 操作危险：remove中如果不带参数将删除所有数据 下面附的是 MongoDB 的速查手册]]></content>
      <tags>
        <tag>MongoDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MongoDB 基础知识]]></title>
    <url>%2F2017%2F06%2F27%2FMongoDB%20%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[MongoDB 是一个基于 分布式文件存储 的数据库。由 C++ 语言编写。 MongoDB 相关概念mongodb中基本的概念是文档、集合、数据库，和传统关系型数据库相关概念的对应关系如下表： RDBMS MongoDB 描述 database database 数据库 table collection 数据库表/集合 row document 数据记录行/文档 column field 数据字段/域 index index 索引 table joins MongoDB可以使用DbRef或者$lookup实现 primary key primary key 主键,MongoDB自动将_id字段设置为主键 数据存储————文档文档是一个键值(key-value)对(即BSON)类似JSON对象。MongoDB 的文档不需要设置相同的字段，并且相同的字段不需要相同的数据类型，这与关系型数据库有很大的区别，也是 MongoDB 非常突出的特点。 MongoDB的主键 _idMongoDB默认会为每个document生成一个 _id 属性，作为默认主键，且默认值为ObjectId,可以更改 _id 的值(可为空字符串)，但每个document必须拥有 _id 属性。这个字段是为了保证文档的唯一性。 MongoDB系统保留数据库 admin local config 时间数据类型MongoDB中的时间类型默认是MongoDate，MongoDate默认是按照UTC（世界标准时间）来存储。例如下面的两种使用方式: 1234567891011121314db.col.insert(&#123;"date": new Date(), num: 1&#125;)db.col.insert(&#123;"date": new Date().toLocaleString(), num: 2&#125;)db.col.find()&#123; "_id" : ObjectId("539944b14a696442d95eaf08"), "date" : ISODate("2014-06-12T06:12:01.500Z"), "num" : 1&#125;&#123; "_id" : ObjectId("539944b14a696442d95eaf09"), "date" : "Thu Jun 12 14:12:01 2014", "num" : 2&#125; note: 第一条数据存储的是一个Date类型，第二条存储存储的是String类型。两条数据的时间相差大约8个小时（忽略操作时间），第一条数据MongoDB是按照UTC时间来进行存储。 MongoDB中的一对多、多对多关系MongoDB的基本单元是Document（文档），通过文档的嵌套（关联）来组织、描述数据之间的关系。例如我们要表示一对多关系，在关系型数据库中我们通常会设计两张表A（一）、B（多），然后在B表中存入A的主键，以此做关联关系。然后查询的时候需要从两张表分别取数据。MongoDB中的Document是通过嵌套来描述数据之间的关系，例如： 1234567891011121314151617&#123; _id:ObjectId("akdjfiou23o4iu23oi5jktlksdjfa") teacherName: "foo", students: [ &#123; stuName: "foo", totalScore：100， otherInfo :[] ... &#125;,&#123; stuName: "bar", totalScore：90， otherInfo :[] ... &#125; ]&#125; 一次查询便可得到所有老师和同学的对应关系。 内嵌文档查询在MongoDB中文档的查询是与顺序有关的。例如： 1234567&#123; "address" : &#123; "province" : "河北省", "city" : "石家庄" &#125;, "number" : 2640613&#125; 要搜索province为“河北省”、city为“石家庄”可以这样: 12345678db.col.find( &#123; "address":&#123; "city" : "石家庄", "province" : "河北省" &#125; &#125;) 然而这样什么都不会查询到。事实上，这样的查询MongoDB会当做全量匹配查询，即document中所有属性与查询条件全部一致时才会被返回。当然这里的“全部一致”也包括属性的顺序。那么，上面的查询如果想搜索到之前的应该先补充number属性，然后更改address属性下的顺序。 在实际应用中我们当然不会这么来查询文档，尤其是需要查询内嵌文档的时候。MongoDB中提供”.”（点）表示法来查询内嵌文档。因此，上面的查询可以这样写： 12345db.col.find( &#123; "address.privince":"河北省" &#125;)]]></content>
      <tags>
        <tag>MongoDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SQL Server Slide Window partition]]></title>
    <url>%2F2017%2F06%2F22%2FSQL-Server-Slide-Window-partition%2F</url>
    <content type="text"><![CDATA[上篇文章详细介绍了表分区的概念和如何创建分区表。 一个分区表创建好之后，随着数据量的逐渐增长，历史数据越来越不受待见：查询和更新频率越来越低，那么把这些历史数据进行归档就十分必要了。如果使用常规的SELECT、INSERT、DELETE进行历史数据归档的话，会出现以下的一系列问题： DELETE效率太低 由于DELETE单个语句是一个事务性的语句，要么全部成功，要么全部失败。那么可想如果删除的是亿级别的数据，那么日志增长,IO负荷非常的大。 迁移过程表会被锁住，这样可能会出现死锁，一旦迁移失败，又会造成更大的IO问题。 这时候，在SQL Server中分区表有一个非常实用的语句 ALTER TABLE …SWITCH，这个DDL可以快速的将同文件组的表的某个分区迅速的转移到另外的表。这个是利用数据的位置偏移量的指针的转移到新表的方法来实现的。这种方案转移数据非常快。 滑动窗口 Sliding Window滑动窗口是为了在一个分区表上维持固定分区的方法，当新数据来的时候，创建新的数据分区、老的分区被移出分区表被删除或者被归档。由于滑动窗口操作在SQL Server中是元数据操作，所以速度会非常快。 下面以AdventureWorks数据库中表FactResellerSales出发，从0开始将该表进行分区，并且以滑动窗口的方式进行动态分区的维护并归档历史数据。 Table Partition实践创建测试数据库这个测试数据库有两个文件组，PRIMARY文件组在E盘，SECONDARY文件组在D盘，用于归档数据用。 12345678910111213141516CREATE DATABASE [PartitionDB]CONTAINMENT = NONEON PRIMARY(NAME = N'PartitionDB', FILENAME = N'E:\SQLServerData\PartitionDB.mdf', SIZE = 10240KB, FILEGROWTH = 1024KB ),FILEGROUP [SECONDARY](NAME = N'PartitionDBArchive', FILENAME = N'D:\SQLServerData\PartitionDBArchive.ndf', SIZE = 4096KB, FILEGROWTH = 1024KB )LOG ON(NAME = N'PartitionDB_log', FILENAME = N'E:\SQLServerData\PartitionDB_log.ldf', SIZE = 1024KB, FILEGROWTH = 10%)GOUSE [PartitionDB]GOIF NOT EXISTS (SELECT name FROM sys.filegroups WHERE is_default=1 AND name = N'PRIMARY') ALTER DATABASE [PartitionDB] MODIFY FILEGROUP [PRIMARY] DEFAULTGO 创建测试数据表以AdventureWorks的FactResellerSales表为例 12Select * INTO FactResellerSalesFROM [AdventureWorksDW2014].[dbo].[FactResellerSales] 在测试表数据导入之后，查看一下当前的表分区情况 12345SELECT o.name objectname,i.name indexname, partition_id, partition_number, [rows]FROM sys.partitions pINNER JOIN sys.objects o ON o.object_id=p.object_idINNER JOIN sys.indexes i ON i.object_id=p.object_id and p.index_id=i.index_idWHERE o.name LIKE '%FactResellerSales%' 如下图： 可以看见，现在这个表有一个默认的分区，并且全部数据都在这个分区里面。 在现有表上创建分区现在要在这个表上创建两个分区，根据OrderDate字段分为两个分区，一个小于2016-01-01，另外分区的数据大于等于这个时间(2016-01-01的数据分配到右面的分区)。按照上面文章写的步骤创建分区 步骤1： 创建分区函数 123CREATE PARTITION FUNCTION [myPartitionRange] (DATETIME) AS RANGE RIGHT FOR VALUES ('2016-01-01')GO 步骤2：创建分区方案 在上面的两个文件组(PRIMARY, SECONDARY)上创建分区方案 123CREATE PARTITION SCHEME myPartitionScheme AS PARTITION [myPartitionRange] TO ([SECONDARY],[PRIMARY]) 步骤3：在表上创建聚集索引并将分区方案作用在该字段上 这里由于要根据字段 OrderDate 进行分区并归档，所以在该字段上创建聚集索引。 12345CREATE CLUSTERED INDEX IX_FactResellerSales_OrderDate ON FactResellerSales (OrderDate) WITH (STATISTICS_NORECOMPUTE = OFF, IGNORE_DUP_KEY = OFF, ALLOW_ROW_LOCKS = ON, ALLOW_PAGE_LOCKS = ON) ON myPartitionScheme(OrderDate) -- AssignPartitionScheme 在创建好索引并应用分区方案后，再查看一下现在的表分区状态 12345SELECT o.name objectname,i.name indexname, partition_id, partition_number, [rows]FROM sys.partitions pINNER JOIN sys.objects o ON o.object_id=p.object_idINNER JOIN sys.indexes i ON i.object_id=p.object_id and p.index_id=i.index_idWHERE o.name LIKE '%FactResellerSales%' 可以看见，现在有两个分区：1、2，其中全部数据都在第一个分区里面，一共有60855条，也就是说60855条销售数据的OrderDate &lt; ‘2016-01-01’ 测试新数据现在分区已经创建好，向这个表插入三条新的2016-01-01之后的数据 1234567891011INSERT INTO dbo.FactResellerSales(ProductKey, OrderDateKey, DueDateKey, ShipDateKey, ResellerKey, EmployeeKey, PromotionKey, CurrencyKey ,SalesTerritoryKey , SalesOrderNumber, SalesOrderLineNumber, RevisionNumber, OrderQuantity, UnitPrice, ExtendedAmount, UnitPriceDiscountPct, DiscountAmount , ProductStandardCost, TotalProductCost, SalesAmount, TaxAmt, Freight, CarrierTrackingNumber, CustomerPONumber, OrderDate, DueDate, ShipDate)VALUES(592, 20160101, 20160101, 20160101, 490, 281, 16, 100, 4, 'SO71952', 42, 1, 3, 20, 60, 0, 0, 50, 60, 2, 0, 0, '9490-4552-81', 'PO9715163911', '2016-01-01 00:00:00.000', '2016-01-01 00:00:00.000', '2016-01-01 00:00:00.000'), (592, 20160101, 20160101, 20160101, 490, 281, 16, 100, 4, 'SO71952', 42, 1, 3, 20, 60, 0, 0, 50, 60, 2, 0, 0, '9490-4552-81', 'PO9715163911', '2016-01-02 00:00:00.000', '2016-01-02 00:00:00.000', '2016-01-02 00:00:00.000'), (592, 20160101, 20160101, 20160101, 490, 281, 16, 100, 4, 'SO71952', 42, 1, 3, 20, 60, 0, 0, 50, 60, 2, 0, 0, '9490-4552-81', 'PO9715163911', '2016-01-03 00:00:00.000', '2016-01-03 00:00:00.000', '2016-01-03 00:00:00.000') 在观察一下分区的情况，发现新插入的3条数据都在第二个分区里面 滑动窗口假设销售数据到了2017年，现在的任务就是把现有的两个分区合并，2016年以及以前的数据放到老的分区里面(D盘上面的那个文件组中)，2017年的数据插入到新的分区里面。 在split partition之前，必须使用alter partition scheme 指定一个NEXT USED FileGroup。如果Partiton Scheme没有指定 next used filegroup，那么alter partition function split range command 执行失败 12345678910111213141516171819202122232425262728DECLARE @CurrentYear DATETIME='2017-01-01'DECLARE @PrevMax DATETIME=(SELECT CONVERT(DATETIME,Value) FROM sys.partition_functions f INNER JOIN sys.partition_range_values r ON f.function_id = r.function_id WHERE f.name = 'myPartitionRange')IF @PrevMax&lt;@CurrentYearBEGIN TRYBEGIN TRAN-- Merge Old PartitionsALTER PARTITION FUNCTION myPartitionRange()MERGE RANGE (@PrevMax)-- Assign NEXT USED filegroupALTER PARTITION SCHEME myPartitionSchemeNEXT USED [PRIMARY]-- Split partitionALTER PARTITION FUNCTION myPartitionRange()SPLIT RANGE (@CurrentYear)COMMIT TRANPRINT 'COMITIINGGGGGG'END TRYBEGIN CATCHIF @@TRANCOUNT&gt;0ROLLBACK TRANEND CATCH 执行完上面的步骤后，你会发现，2016年的那3条数据也被合并到第一个分区里面了，新的分区2数据为空，留给2017年 再插入3条2017年的数据试验一下 1234567891011INSERT INTO dbo.FactResellerSales(ProductKey, OrderDateKey, DueDateKey, ShipDateKey, ResellerKey, EmployeeKey, PromotionKey, CurrencyKey, SalesTerritoryKey,SalesOrderNumber, SalesOrderLineNumber, RevisionNumber, OrderQuantity, UnitPrice, ExtendedAmount, UnitPriceDiscountPct, DiscountAmount, ProductStandardCost, TotalProductCost, SalesAmount, TaxAmt, Freight, CarrierTrackingNumber, CustomerPONumber, OrderDate, DueDate, ShipDate)VALUES(592, 20170101, 20170101, 20170101, 490, 281, 16, 100, 4, 'SO71952', 42, 1, 3, 20, 60, 0, 0, 50, 60, 2, 0, 0, '9490-4552-81', 'PO9715163911', '2017-01-01 00:00:00.000', '2017-01-01 00:00:00.000', '2017-01-01 00:00:00.000'), (592, 20170102, 20170102, 20170102, 490, 281, 16, 100, 4, 'SO71952', 42, 1, 3, 20, 60, 0, 0, 50, 60, 2, 0, 0, '9490-4552-81', 'PO9715163911', '2017-01-02 00:00:00.000', '2017-01-02 00:00:00.000', '2017-01-02 00:00:00.000'), (592, 20170103, 20170103, 20170103, 490, 281, 16, 100, 4, 'SO71952', 42, 1, 3, 20, 60, 0, 0, 50, 60, 2, 0, 0, '9490-4552-81', 'PO9715163911', '2017-01-03 00:00:00.000', '2017-01-03 00:00:00.000', '2017-01-03 00:00:00.000') 现在，2016年以及更久之前的数据保存在分区1中，文件组存储在D盘上，而2017年最新的数据在分区2中，存储在E盘上，这样，就把新老数据在存储上分开，利用更好存储设备的性能查询和更新操作等。而对于查询或者操作这个表的用户来说，逻辑上这还是一张表。 当然，还有另外一种方式进行老数据的归档操作，那就是两张物理表，一张仅存储最新数据，另一张存储归档数据]]></content>
      <tags>
        <tag>SQL Server</tag>
        <tag>Database</tag>
        <tag>表分区</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SQL Server表分区]]></title>
    <url>%2F2017%2F06%2F21%2FSQL%20Server%E8%A1%A8%E5%88%86%E5%8C%BA%2F</url>
    <content type="text"><![CDATA[某种特定业务下，部分业务数据可能只保留比较短的时间，例如每日的流量日志数据，可能每天亿级别的数据增长，随着数据量的逐渐增长，你会发现数据库能性能越来越慢，查询速度会明显变慢，而这时要想提高数据库的查询速度，你肯定会想到索引这种方式，但是随着索引的引入，数据的插入和更新也会变慢，因为在数据插入的时候，索引也是需要重建的。那怎么办呢？ (其实我觉得这种应用场景更好的解决方法是用流式处理的方式) 一个最简单的解决方法就是把一个大表拆分成多个小表，这个就叫做表分区，表分区有两种： 水平分区 (行级) 垂直分区 (列级) 下面主要说的是水平分区。 表分区有以下优点： 改善查询性能：对分区对象的查询可以仅搜索自己关心的分区，提高检索速度。 增强可用性：如果表的某个分区出现故障，表在其他分区的数据仍然可用； 维护方便：如果表的某个分区出现故障，需要修复数据，只修复该分区即可； 均衡I/O：可以把不同的分区映射到磁盘以平衡I/O，改善整个系统性能。 分区表是把数据按某种标准划分成区域存储在不同的文件组中，使用分区可以快速而有效地管理和访问数据子集，从而使大型表或索引更易于管理。合理的使用分区会很大程度上提高数据库的性能。 创建分区需要如下个步骤： 创建文件组 创建分区函数 创建分区方案 创建或者修改使用分区方案的表 举一个按照时间分区的案例： 确定分区键列的类型(DATETIME)以及分区的边界值: 2011-01-01 2012-01-01 2013-01-01 N个边界值确定 N+1 个分区 创建文件组T-SQL语法 1alter database &lt;数据库名&gt; add filegroup &lt;文件组名&gt; 下面创建4个分区文件组 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960USE AdventureWorksDW2014; GO -- Adds four new filegroups to the AdventureWorksDW2014 database ALTER DATABASE AdventureWorksDW2014 ADD FILEGROUP test1fg; GO ALTER DATABASE AdventureWorksDW2014 ADD FILEGROUP test2fg; GO ALTER DATABASE AdventureWorksDW2014 ADD FILEGROUP test3fg; GO ALTER DATABASE AdventureWorksDW2014 ADD FILEGROUP test4fg; -- Adds one file for each filegroup. ALTER DATABASE AdventureWorksDW2014 ADD FILE ( NAME = test1dat1, FILENAME = 'E:\FileGroupsData\t1dat1.ndf', SIZE = 5MB, MAXSIZE = 100MB, FILEGROWTH = 5MB ) TO FILEGROUP test1fg; ALTER DATABASE AdventureWorksDW2014 ADD FILE ( NAME = test2dat2, FILENAME = 'E:\FileGroupsData\t2dat2.ndf', SIZE = 5MB, MAXSIZE = 100MB, FILEGROWTH = 5MB ) TO FILEGROUP test2fg; GO ALTER DATABASE AdventureWorksDW2014 ADD FILE ( NAME = test3dat3, FILENAME = 'E:\FileGroupsData\t3dat3.ndf', SIZE = 5MB, MAXSIZE = 100MB, FILEGROWTH = 5MB ) TO FILEGROUP test3fg; GO ALTER DATABASE AdventureWorksDW2014 ADD FILE ( NAME = test4dat4, FILENAME = 'E:\FileGroupsData\t4dat4.ndf', SIZE = 5MB, MAXSIZE = 100MB, FILEGROWTH = 5MB ) TO FILEGROUP test4fg; GO 执行上面的脚本，可以创建4个文件组 使用样例数据库的FactResellerSales表做一个分区的实验 123456IF OBJECT_ID('dbo.SalesOrders')IS NOT NULLDROP TABLE dbo.SalesOrdersGOSELECT * INTO SalesOrdersFROM [AdventureWorksDW2014].[dbo].[FactResellerSales] 创建分区函数指定分依据区列（依据列唯一），分区数据范围规则，分区数量，然后将数据映射到一组分区上。 创建语法： 12create partition function 分区函数名(&lt;分区列类型&gt;) as range [left/right]for values (每个分区的边界值,....) 1234567CREATE PARTITION FUNCTION PF_Orders_OrderDateRange(DATETIME) AS RANGE RIGHT FOR VALUES ( '2011-01-01', '2012-01-01', '2013-01-01' ) 左边界/右边界：就是把临界值划分给上一个分区还是下一个分区。这里2013-01-01就属于下一个分区 注意：只有没有应用到分区方案中的分区函数才能被删除。 创建分区方案指定分区对应的文件组。 创建语法： 12-- 创建分区方案语法create partition scheme &lt;分区方案名称&gt; as partition &lt;分区函数名称&gt; [all]to (文件组名称,....) 1234CREATE PARTITION SCHEME PS_Orders AS PARTITION PF_Orders_OrderDateRange TO (test1fg, test2fg, test3fg, test4fg) ;GO 注意：只有没有分区表，或索引使用该分区方案是，才能对其删除。 创建使用分区方案的表创建语法： 1234--创建分区表语法create table &lt;表名&gt; ( &lt;列定义&gt;)on&lt;分区方案名&gt;(分区列名) 下面在已有的表SalesOrders上面应用分区方案，并在OrderDate字段上创建聚集索引 123456CREATE CLUSTERED INDEX IX_FactResellerSales_OrderDate ON dbo.SalesOrders (OrderDate) WITH (STATISTICS_NORECOMPUTE = OFF, IGNORE_DUP_KEY = OFF, ALLOW_ROW_LOCKS = ON, ALLOW_PAGE_LOCKS = ON) ON PS_Orders(OrderDate) -- AssignPartitionScheme -- 这里使用[PS_Orders]分区方案，根据OrderDate列进行分区 在创建分区表后，需要创建聚集分区索引 根据订单表Orders 查询时经常使用OrderDate 范围条件来查询的特点，我们最好在Orders.OrderDate 列上建立聚集索引（clustered index）。为了便于进行分区切换（partition swtich)。大多数情况下，建议在分区表上建立分区索引。 查询分区表信息查看分区依据列的指定值所在的分区123-- 查询分区依据列为'2013-01-01'的数据在哪个分区上SELECT $partition.PF_Orders_OrderDateRange('2013-01-01') -- 返回值是3，表示此值存在第2个分区 查询每个非空分区存在的行数12345-- 查看分区表中，每个非空分区存在的行数SELECT $partition.PF_Orders_OrderDateRange(OrderDate) as partitionNum , count(*) as recordCountFROM dbo.SalesOrdersGROUP BY $partition.PF_Orders_OrderDateRange(OrderDate) 查询各个分区的数据信息1234567SELECT PARTITION = $PARTITION.PF_Orders_OrderDateRange(OrderDate), ROWS = COUNT(*), MinVal = MIN(OrderDate), MaxVal = MAX(OrderDate)FROM [dbo].[SalesOrders]GROUP BY $PARTITION.PF_Orders_OrderDateRange(OrderDate)ORDER BY PARTITION 分区的拆分合并拆分分区在分区函数中新增一个边界值，即可将一个分区变为2个。 123--分区拆分alter partition function PF_Orders_OrderDateRange()split range('2014-01-01') 注意：如果分区函数已经指定了分区方案，则分区数需要和分区方案中指定的文件组个数保持对应一致。 合并分区与拆分分区相反，去除一个边界值即可。 123--合并分区alter partition function PF_Orders_OrderDateRange()merge range('2011-01-01') 分区数据移动分区数据移动可以使用 ALTER TABLE ....... SWITCH 语句快速有效地移动数据子集： 将某个表中的数据移动到另一个表中； 将某个表作为分区添加到现存的已分区表中； 将分区从一个已分区表切换到另一个已分区表； 删除分区以形成单个表。 切换分区表的一个分区到普通数据表创建普通表SalesOrder_2012用于存放订单日期为2012年的所有数据。从分区到普通表的切换，最好满足以下条件： 普通表必须建立在分区表切换出的分区所在的文件组上 普通表的表结构和分区表一致 普通表上的索引要和分区表一致，包括聚集索引和非聚集索引 普通表必须是空表 切换分区为3的数据从分区表到归档表 12ALTER TABLE dbo.SalesOrders SWITCH PARTITION 3TO dbo.SalesOrders_2012 现在再查看一下分区表的分区状态，分区为3的数据已经不在了 切换普通表数据到分区表的一个分区中下面要把上面已经归档的2012年的数据切换回来 按照上面的那种写法先试一下： 12ALTER TABLE dbo.SalesOrders_2012 SWITCH TOdbo.SalesOrders PARTITION 3 这时候会遇到错误 1234Msg 4982, Level 16, State 1, Line 1ALTER TABLE SWITCH statement failed.Check constraints of source table &apos;AdventureWorksDW2014.dbo.SalesOrders_2012&apos;allow values that are not allowed by range defined by partition 3 on target table &apos;AdventureWorksDW2014.dbo.SalesOrders&apos;. 这是因为表dbo.SalesOrders 的数据经过分区函数的分区列定义, 各个分区的数据实际上已经经过了数据约束检查，符合分区边界范围(Range)的数据才会录入到各个分区中。但是在存档表dbo.SalesOrders_2012中的数据实际上是没有边界约束的，比如完全可以手动的插入一条其他年的数据，所以进行SWITCH时肯定是不会成功的，这时候需要增加一个数据约束检查 12ALTER TABLE dbo.SalesOrders_2012 ADD CONSTRAINT CK_SalesOrders_OrderDateCHECK(OrderDate&gt;='2012-01-01' AND OrderDate&lt;'2013-01-01') 这时候再SWITCH，2012年扥分区数据就会到了分区表中。 切换分区表数据到分区表新的存档分区表在结构上和源分区表是一致的，包括分区函数和分区方案，但是需要重新创建，不能简单地直接使用dbo.SalesOrders 表上的分区函和分区方案，因为他们之间有绑定关系 创建分区函数和分区方案 123456789101112131415161718192021IF EXISTS (SELECT * FROM sys.partition_schemes WHERE name = 'PS_SalesOrdersArchive')DROP PARTITION SCHEME PS_SalesOrdersArchiveGOIF EXISTS (SELECT * FROM sys.partition_functions WHERE name = 'PF_SalesOrdersArchive_OrderDateRange')DROP PARTITION FUNCTION PF_SalesOrdersArchive_OrderDateRangeGOCREATE PARTITION FUNCTION PF_SalesOrdersArchive_OrderDateRange(DATETIME)AS RANGE RIGHT FOR VALUES( '2011-01-01', '2012-01-01', '2013-01-01')GOCREATE PARTITION SCHEME PS_SalesOrdersArchiveAS PARTITION PF_SalesOrdersArchive_OrderDateRangeTO (test1fg, test2fg, test3fg, test4fg)GO 创建归档表 1234567891011CREATE TABLE [dbo].[SalesOrdersArchive]( [ProductKey] [int] NOT NULL, [OrderDateKey] [int] NOT NULL, ... [OrderDate] [datetime] NOT NULL, [DueDate] [datetime] NULL, [ShipDate] [datetime] NULL) ON PS_SalesOrdersArchive(OrderDate)CREATE CLUSTERED INDEX IXC_SalesOrdersArchive_OrderDate ON dbo.SalesOrdersArchive(OrderDate) 切换分区到归档表 123ALTER TABLE dbo.SalesOrders SWITCH PARTITION 1 TO dbo.SalesOrdersArchive PARTITION 1ALTER TABLE dbo.SalesOrders SWITCH PARTITION 2 TO dbo.SalesOrdersArchive PARTITION 2ALTER TABLE dbo.SalesOrders SWITCH PARTITION 3 TO dbo.SalesOrdersArchive PARTITION 3 切换完成后，观察一下原表和归档表的分区数据状况： 原表： 归档表： 总结分区表分区切换并没有真正去移动数据,而是SQL Server 在系统底层改变了表的元数据。因此分区表分区切换是高效、快速、灵活的。利用分区表的分区切换功能，我们可以快速加载数据到分区表、卸载分区数据到普通表，然后TRUNCATE普通表，以实现快速删除分区表数据，快速归档不活跃数据到历史表。 表分区的相关概念和实际操作就介绍到这儿，下一篇重点介绍一下如何实现表分区随着时间窗口的移动而自动维护。]]></content>
      <tags>
        <tag>SQL Server</tag>
        <tag>Database</tag>
        <tag>表分区</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[堆栈和队列]]></title>
    <url>%2F2017%2F06%2F13%2F%E5%A0%86%E6%A0%88%E5%92%8C%E9%98%9F%E5%88%97%2F</url>
    <content type="text"><![CDATA[栈 (Stack)是一种后进先出(last in first off，LIFO)的数据结构 队列(Queue)则是一种先进先出 (fisrt in first out，FIFO)的结构 栈12345678910111213141516171819202122232425262728class Stack: def __init__(self): self.items = [] def is_empty(self): return not self.items def push(self, item): """adds a new item to the top of the stack""" self.items.append(item) def pop(self): """ removes the top item from the stack, popping an empty stack (list) will result in an error """ if not self.is_empty(): return self.items.pop() return "Pop from empty stack" def peek(self): """returns the top item from the stack but does not remove it""" if not self.is_empty(): return self.items[len(self.items) - 1] return "Stack is Empty" def size(self): return len(self.items) 队列]]></content>
      <tags>
        <tag>面试</tag>
        <tag>Algorithm</tag>
        <tag>Data Structure</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数组和链表]]></title>
    <url>%2F2017%2F06%2F13%2F%E6%95%B0%E7%BB%84%E5%92%8C%E9%93%BE%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[需要在内存中存储多项数据时，有两种基本方式：数组和链表 由于数组是连续存储的，在操作数组中的数据时就可以根据离首地址的偏移量直接存取相应位置上的数据，但是如果要在数据组中任意位置上插入一个元素，就需要先把后面的元素集体向后移一位为其空出存储空间。与之相反，链表是离散存储的，所以在插入一个数据时只要申请一片新空间，然后将其中的连接关系做一个修改就可以，但是显然在链表上查找一个数据时就要逐个遍历了。 数组的优势在于能够随机访问，而链表只能顺序访问。 链表的优势在于能够以较高的效率在任意位置插入或删除一个节点。 数组 链表 读取 O(1) O(n) 插入 O(n) O(1) 删除 O(n) O(1)]]></content>
      <tags>
        <tag>面试</tag>
        <tag>Algorithm</tag>
        <tag>Data Structure</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[搜索和排序之搜索]]></title>
    <url>%2F2017%2F06%2F12%2F%E6%90%9C%E7%B4%A2%E5%92%8C%E6%8E%92%E5%BA%8F%E4%B9%8B%E6%90%9C%E7%B4%A2%2F</url>
    <content type="text"><![CDATA[未完成 基本概念查找（Searching）就是根据给定的某个值，在查找表中确定一个其关键字等于给定值的数据元素（或记录）。 查找表按照操作方式可分为： 静态查找表（Static Search Table）：只做查找操作的查找表。它的主要操作是： 查询某个“特定的”数据元素是否在表中 检索某个“特定的”数据元素和各种属性 动态查找表（Dynamic Search Table）：在查找中同时进行插入或删除等操作： 查找时插入数据 查找时删除数据 无序表查找在数据不排序的线性查找，遍历数据元素。 算法分析：最好情况是第一个位置就找到了，为O(1)；最坏情况在最后一个位置找到，为O(n)， 平均查找次数为 (n+1)/2, 时间复杂度为O(n) 123456789101112def sequential_search(lst, key): length = len(lst) for i in range(length): if lst[i] == key: return i else: return Falseif __name__ == '__main__': l = [1,5,8,124,22,54,7,99,300,222] result = sequential_search(l, 123) print result 快速选择算法 (quick selection algorithm)快速选择算法能够在平均O(n)时间内从一个无序数组中返回第k大的元素。算法实际上利用了快速排序的思想，将数组依照一个轴值分割成两个部分，左边元素都比轴值小，右边元素都比轴值大。由于轴值下标已知，则可以判断所求元素落在数组的哪一部分，并在那一部分继续进行上述操作，直至找到该元素。与快排不同，由于快速选择算法只在乎所求元素所在的那一部分，所以时间复杂度是O(n)。 12 有序表查找数据按某种方式进行过排序 二分查找 Binary Search算法内容：在查找表中不断取中间元素与查找值进行比较，以二分之一的倍率进行表范围的缩小。时间复杂度：O(logn) 12345678910111213141516def binary_search(lst, key): low, high = 0, len(lst)-1 while low &lt; high: mid = (low + high) / 2 if key &lt; lst[mid]: high = mid - 1 elif key &gt; lst[mid]: low = mid + 1 else: return mid return Falseif __name__ == '__main__': l = [1, 5, 7, 8, 22, 54, 99, 123, 200, 222, 444] result = binary_search(l, 123) print result 插值查找插值查找是二分查找演化而来，相比于二分查找(折半),该算法考虑的是每次折的时候折多少，即不一定是1/2。在二分查找中mid=(low+high)/2=low+1/2*(high-low)，插值查找就是对1/2(系数,或者说比例)进行改变，它将1/2变成 (key - array[low])/(array[high] - array[low]),其实就是计算线性比例。 时间复杂度：O(logn) _ note: 因为插值查找是依赖线性比例的，如果当前数组分布不是均匀的，那么该算法就不合适。_ 12345678910111213141516def interpolate_search(lst, key): low, high = 0, len(lst)-1 while low &lt; high: mid = low + (high - low) * (key - lst[low])/(lst[high] - lst[low]) if key &lt; lst[mid]: high = mid - 1 elif key &gt; lst[mid]: low = mid + 1 else: return mid return Falseif __name__ == '__main__': l = [1, 5, 7, 8, 22, 54, 99, 123, 200, 222, 444] result = binary_search(l, 123) print result 斐波那契查找查找算法：在斐波那契数列找一个等于略大于查找表中元素个数的数F(n)，将原查找表扩展为长度为F(n)(如果要补充元素，则补充重复最后一个元素，直到满足数组元素个数为F(n)个元素)，完成后进行斐波那契分割，即F(n)个元素分割为前半部分F(n-1)个元素，后半部分F(n-2)个元素，找出要查找的元素在那一部分并递归，直到找到。时间复杂度：O(logn)，平均性能优于二分查找。 利用斐波那契数列的性质，黄金分割的原理来确定mid的位置 线性索引查找对于海量的无序数据，为了提高查找速度，一般会为其构造索引表。索引就是把一个关键字与它相对应的记录进行关联的过程。一个索引由若干个索引项构成，每个索引项至少包含关键字和其对应的记录在存储器中的位置等信息。 索引按照结构可以分为：线性索引、树形索引和多级索引。线性索引：将索引项的集合通过线性结构来组织，也叫索引表。 线性索引可分为：稠密索引、分块索引和倒排索引 稠密索引分块索引倒排索引二叉排序树二叉排序树又称为二叉查找树。它或者是一颗空树，或者是具有下列性质的二叉树： 若它的左子树不为空，则左子树上所有节点的值均小于它的根结构的值； 若它的右子树不为空，则右子树上所有节点的值均大于它的根结构的值； 它的左、右子树也分别为二叉排序树。 二叉排序树的操作： 查找：对比节点的值和关键字，相等则表明找到了；小了则往节点的左子树去找，大了则往右子树去找，这么递归下去，最后返回布尔值或找到的节点。 插入：从根节点开始逐个与关键字进行对比，小了去左边，大了去右边，碰到子树为空的情况就将新的节点链接。 删除：如果要删除的节点是叶子，直接删；如果只有左子树或只有右子树，则删除节点后，将子树链接到父节点即可；如果同时有左右子树，则可以将二叉排序树进行中序遍历，取将要被删除的节点的前驱或者后继节点替代这个被删除的节点的位置。 二叉排序树总结： 二叉排序树以链式进行存储，保持了链接结构在插入和删除操作上的优点。 在极端情况下，查询次数为1，但最大操作次数不会超过树的深度。也就是说，二叉排序树的查找性能取决于二叉排序树的形状，也就引申出了后面的平衡二叉树。 给定一个元素集合，可以构造不同的二叉排序树，当它同时是一个完全二叉树的时候，查找的时间复杂度为O(log(n))，近似于二分查找。 当出现最极端的斜树时，其时间复杂度为O(n)，等同于顺序查找，效果最差。 平衡二叉树多路查找树 B树2-3树3-4树B树B+树散列表散列表：所有的元素之间没有任何关系。元素的存储位置，是利用元素的关键字通过某个函数直接计算出来的。这个一一对应的关系函数称为散列函数或Hash函数。 采用散列技术将记录存储在一块连续的存储空间中，称为散列表或哈希表（Hash Table）。关键字对应的存储位置，称为散列地址。 散列表是一种面向查找的存储结构。它最适合求解的问题是查找与给定值相等的记录。但是对于某个关键字能对应很多记录的情况就不适用，比如查找所有的“男”性。也不适合范围查找，比如查找年龄20~30之间的人。排序、最大、最小等也不合适。 因此，散列表通常用于关键字不重复的数据结构。比如python的字典数据类型。 设计出一个简单、均匀、存储利用率高的散列函数是散列技术中最关键的问题。但是，一般散列函数都面临着冲突的问题。冲突：两个不同的关键字，通过散列函数计算后结果却相同的现象。collision 散列函数构造好的散列函数：计算简单、散列地址分布均匀 直接定址法 例如取关键字的某个线性函数为散列函数：f(key) = a*key + b (a,b为常数） 数字分析法 抽取关键字里的数字，根据数字的特点进行地址分配 平方取中法 将关键字的数字求平方，再截取部分 折叠法 将关键字的数字分割后分别计算，再合并计算，一种玩弄数字的手段。 除留余数法 最为常见的方法之一。对于表长为m的数据集合，散列公式为：f(key) = key mod p (p&lt;=m)mod：取模（求余数）该方法最关键的是p的选择，而且数据量较大的时候，冲突是必然的。一般会选择接近m的质数。随机数法选择一个随机数，取关键字的随机函数值为它的散列地址。f(key) = random(key) 总结，实际情况下根据不同的数据特性采用不同的散列方法，考虑下面一些主要问题： 计算散列地址所需的时间 关键字的长度 散列表的大小 关键字的分布情况 记录查找的频率 处理散列冲突散列表查找实现散列表查找性能分析]]></content>
      <tags>
        <tag>面试</tag>
        <tag>Algorithm</tag>
        <tag>Data Structure</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[搜索和排序之排序]]></title>
    <url>%2F2017%2F06%2F12%2F%E6%90%9C%E7%B4%A2%E5%92%8C%E6%8E%92%E5%BA%8F%E4%B9%8B%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[未完成 Bubble Sort 冒泡排序冒泡排序的原理非常简单，它重复地走访过要排序的数列，一次比较两个元素，如果他们的顺序错误就把他们交换过来。 步骤： 比较相邻的元素。如果第一个比第二个大，就交换他们两个。 对第0个到第n-1个数据做同样的工作。这时，最大的数就“浮”到了数组最后的位置上。 针对所有的元素重复以上的步骤，除了最后一个。 持续每次对越来越少的元素重复上面的步骤，直到没有任何一对数字需要比较。 12 针对上述代码还有两种优化方案。 Selection Sort 选择排序选择排序无疑是最简单直观的排序。它的工作原理如下。 步骤： 在未排序序列中找到最小（大）元素，存放到排序序列的起始位置。 再从剩余未排序元素中继续寻找最小（大）元素，然后放到已排序序列的末尾。 以此类推，直到所有元素均排序完毕。 12 Insertion Sort 插入排序插入排序的工作原理是，对于每个未排序数据，在已排序序列中从后向前扫描，找到相应位置并插入。 步骤： 从第一个元素开始，该元素可以认为已经被排序 取出下一个元素，在已经排序的元素序列中从后向前扫描 如果被扫描的元素（已排序）大于新元素，将该元素后移一位 重复步骤3，直到找到已排序的元素小于或者等于新元素的位置 将新元素插入到该位置后 重复步骤2~5 12 Shell Sort 希尔排序希尔排序，也称递减增量排序算法，实质是分组插入排序。由 Donald Shell 于1959年提出。希尔排序是非稳定排序算法。 希尔排序的基本思想是：将数组列在一个表中并对列分别进行插入排序，重复这过程，不过每次用更长的列（步长更长了，列数更少了）来进行。最后整个表就只有一列了。将数组转换至表是为了更好地理解这算法，算法本身还是使用数组进行排序。 12 Merge Sort 归并排序归并排序是采用分治法的一个非常典型的应用。归并排序的思想就是先递归分解数组，再合并数组。 先考虑合并两个有序数组，基本思路是比较两个数组的最前面的数，谁小就先取谁，取了后相应的指针就往后移一位。然后再比较，直至一个数组为空，最后把另一个数组的剩余部分复制过来即可。 再考虑递归分解，基本思路是将数组分解成left和right，如果这两个数组内部数据是有序的，那么就可以用上面合并数组的方法将这两个数组合并排序。如何让这两个数组内部是有序的？可以再二分，直至分解出的小组只含有一个元素时为止，此时认为该小组内部已有序。然后合并排序相邻二个小组即可。 12 Quick Sort 快速排序快速排序通常明显比同为Ο(n log n)的其他算法更快，因此常被采用，而且快排采用了分治法的思想，所以在很多笔试面试中能经常看到快排的影子。可见掌握快排的重要性。 步骤： 从数列中挑出一个元素作为基准数。 分区过程，将比基准数大的放到右边，小于或等于它的数都放到左边。 再对左右区间递归执行第二步，直至各区间只有一个数。 12 Heap Sort 堆排序堆排序在 top K 问题中使用比较频繁。堆排序是采用二叉堆的数据结构来实现的，虽然实质上还是一维数组。二叉堆是一个近似完全二叉树 。 二叉堆二叉堆具有以下性质： 父节点的键值总是大于或等于（小于或等于）任何一个子节点的键值。 每个节点的左右子树都是一个二叉堆（都是最大堆或最小堆）。 步骤： 构造最大堆（Build_Max_Heap）：若数组下标范围为0~n，考虑到单独一个元素是大根堆，则从下标n/2开始的元素均为大根堆。于是只要从n/2-1开始，向前依次构造大根堆，这样就能保证，构造到某个节点时，它的左右子树都已经是大根堆。 堆排序（HeapSort）：由于堆是用数组模拟的。得到一个大根堆后，数组内部并不是有序的。因此需要将堆化数组有序化。思想是移除根节点，并做最大堆调整的递归运算。第一次将heap[0]与heap[n-1]交换，再对heap[0…n-2]做最大堆调整。第二次将heap[0]与heap[n-2]交换，再对heap[0…n-3]做最大堆调整。重复该操作直至heap[0]和heap[1]交换。由于每次都是将最大的数并入到后面的有序区间，故操作完后整个数组就是有序的了。 最大堆调整（Max_Heapify）：该方法是提供给上述两个过程调用的。目的是将堆的末端子节点作调整，使得子节点永远小于父节点。 12 Bucket Sort 桶排序Counting Sort 计数排序## 总结 排序方法 平均情况 最好情况 最坏情况 辅助空间 稳定性 冒泡排序 O(n2) O(n) O(n2) O(1) 稳定 选择排序 O(n2) O(n2) O(n2) O(1) 不稳定 插入排序 O(n2) O(n) O(n2) O(1) 稳定 希尔排序 O(nlogn)~O(n2) O(n1.3) O(n2) O(1) 不稳定 堆排序 O(nlogn) O(nlogn) O(nlogn) O(1) 不稳定 归并排序 O(nlogn) O(nlogn) O(nlogn) O(n) 稳定 快速排序 O(nlogn) O(nlogn) O(n2) O(logn)~O(n) 不稳定]]></content>
      <tags>
        <tag>面试</tag>
        <tag>Algorithm</tag>
        <tag>Data Structure</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(转)Python基础知识面试题]]></title>
    <url>%2F2017%2F06%2F12%2F(%E8%BD%AC)Python%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E9%9D%A2%E8%AF%95%E9%A2%98%2F</url>
    <content type="text"><![CDATA[下面总结了一下常见的、易错的Python面试题 Question 1如下代码的输出是什么 1234567891011def extendList(val, list=[]): list.append(val) return listlist1 = extendList(10)list2 = extendList(123,[])list3 = extendList('a')print "list1 = %s" % list1print "list2 = %s" % list2print "list3 = %s" % list3 答案：123list1 = [10, &apos;a&apos;]list2 = [123]list3 = [10, &apos;a&apos;] 解释The new default list is created only once when the function is defined, and that same list is then used subsequently whenever extendList is invoked without a list argument being specified. This is because expressions in default arguments are calculated when the function is defined, not when it’s called. list1 and list3 are therefore operating on the same default list, whereas list2 is operating on a separate list that it created (by passing its own empty list as the value for the list parameter). The definition of the extendList function could be modified as follows, though, to always begin a new list when no list argument is specified, which is more likely to have been the desired behavior: 123456789def extendList(val, list=None): if list is None: list = [] list.append(val) return list# list1 = [10]# list2 = [123]# list3 = ['a'] Python的函数参数传递1234567891011a = 1def fun(a): a = 2fun(a)print a # 1a = []def fun(a): a.append(1)fun(a)print a # [1] 类型是属于对象的，而不是变量。而对象有两种, 可更改 (mutable) 与 不可更改(immutable)对象。在python中，strings, tuples, 和numbers是不可更改的对象，而list,dict等则是可以修改的对象。当一个引用传递给函数的时候,函数自动复制一份引用,这个函数里的引用和外边的引用没有半毛关系了.所以第一个例子里函数把引用指向了一个不可变对象,当函数返回的时候,外面的引用没半毛感觉.而第二个例子就不一样了,函数内的引用指向的是可变对象,对它的操作就和定位了指针地址一样,在内存里进行修改。 Question 2下面代码输出 1234def multipliers(): return [lambda x : i * x for i in range(4)] print [m(2) for m in multipliers()] The output of the above code will be [6, 6, 6, 6] (not [0, 2, 4, 6]). The reason for this is that Python’s closures are late binding. This means that the values of variables used in closures are looked up at the time the inner function is called. So as a result, when any of the functions returned by multipliers() are called, the value of i is looked up in the surrounding scope at that time. By then, regardless of which of the returned functions is called, the for loop has completed and i is left with its final value of 3. Therefore, every returned function multiplies the value it is passed by 3, so since a value of 2 is passed in the above code, they all return a value of 6 (i.e., 3 x 2). (Incidentally, as pointed out in The Hitchhiker’s Guide to Python, there is a somewhat widespread misconception that this has something to do with lambdas, which is not the case. Functions created with a lambda expression are in no way special and the same behavior is exhibited by functions created using an ordinary def.) Below are a few examples of ways to circumvent this issue. One solution would be use a Python generator as follows: 123def multipliers(): for i in range(4): yield lambda x : i * x Another solution is to create a closure that binds immediately to its arguments by using a default argument. For example: 12def multipliers(): return [lambda x, i=i : i * x for i in range(4)] Or alternatively, you can use the functools.partial function: 12345from functools import partialfrom operator import muldef multipliers(): return [partial(mul, i) for i in range(4)] 迭代器和生成器Question 3123456789101112131415161718class Parent(object): x = 1class Child1(Parent): passclass Child2(Parent): passprint Parent.x, Child1.x, Child2.xChild1.x = 2print Parent.x, Child1.x, Child2.xParent.x = 3print Parent.x, Child1.x, Child2.x# 1, 1, 1# 2, 1, 2# 3, 2, 3 解释in Python, class variables are internally handled as dictionaries. If a variable name is not found in the dictionary of the current class, the class hierarchy (i.e., its parent classes) are searched until the referenced variable name is found (if the referenced variable name is not found in the class itself or anywhere in its hierarchy, an AttributeError occurs). Therefore, setting x = 1 in the Parent class makes the class variable x (with a value of 1) referenceable in that class and any of its children. That’s why the first print statement outputs 1 1 1. Subsequently, if any of its child classes overrides that value (for example, when we execute the statement Child1.x = 2), then the value is changed in that child only. That’s why the second print statement outputs 1 2 1. Finally, if the value is then changed in the Parent (for example, when we execute the statement Parent.x = 3), that change is reflected also by any children that have not yet overridden the value (which in this case would be Child2). That’s why the third print statement outputs 3 2 3. Question 4在 Python2 中下面代码输出是什么 123456789101112131415def div1(x,y): print "%s/%s = %s" % (x, y, x/y) def div2(x,y): print "%s//%s = %s" % (x, y, x//y)div1(5,2)div1(5.,2)div2(5,2)div2(5.,2.)# 5/2 = 2# 5.0/2 = 2.5# 5//2 = 2# 5.0//2.0 = 2.0 By default, Python 2 automatically performs integer arithmetic if both operands are integers. As a result, 5/2 yields 2, while 5./2 yields 2.5. Note that you can override this behavior in Python 2 by adding the following import: 1from __future__ import division Also note that the “double-slash” (//) operator will always perform integer division, regardless of the operand types. That’s why 5.0//2.0 yields 2.0 even in Python 2. Python 3, however, does not have this behavior; i.e., it does not perform integer arithmetic if both operands are integers. Therefore, in Python 3, the output will be as follows: 12345/2 = 2.55.0/2 = 2.55//2 = 25.0//2.0 = 2.0 Question 51234list = ['a', 'b', 'c', 'd', 'e']print list[10:]# [] 输出为空list, 不会报 IndexError错误 As one would expect, attempting to access a member of a list using an index that exceeds the number of members (e.g., attempting to access list[10] in the list above) results in an IndexError. However, attempting to access a slice of a list at a starting index that exceeds the number of members in the list will not result in an IndexError and will simply return an empty list. What makes this a particularly nasty gotcha is that it can lead to bugs that are really hard to track down since no error is raised at runtime. Question 612345678list = [ [] ] * 5list # output?list[0].append(10)list # output?list[1].append(20)list # output?list.append(30)list # output? 答案1234[[], [], [], [], []][[10], [10], [10], [10], [10]][[10, 20], [10, 20], [10, 20], [10, 20], [10, 20]][[10, 20], [10, 20], [10, 20], [10, 20], [10, 20], 30] list = [ [ ] ] * 5 simply creates a list of 5 lists. However, the key thing to understand here is that the statement list = [ [ ] ] * 5 does NOT create a list containing 5 distinct lists; rather, it creates a a list of 5 references to the same list 12print id(list[0]) == id(list[1])# True list[0].append(10) appends 10 to the first list. But since all 5 lists refer to the same list, the output is: [[10], [10], [10], [10], [10]]. Similarly, list[1].append(20) appends 20 to the second list. But again, since all 5 lists refer to the same list, the output is now: [[10, 20], [10, 20], [10, 20], [10, 20], [10, 20]]. In contrast, list.append(30) is appending an entirely new element to the “outer” list, which therefore yields the output: [[10, 20], [10, 20], [10, 20], [10, 20], [10, 20], 30]. Question 7Given a list of N numbers, use a single list comprehension to produce a new list that only contains those values that are: even numbers, and from elements in the original list that had even indices 答案12345# 0 1 2 3 4 5 6 7 8list = [ 1 , 3 , 5 , 8 , 10 , 13 , 18 , 36 , 78 ]print [x for x in list[::2] if x%2==0]# [10, 18, 78] *args and **kwargs当你不确定你的函数里将要传递多少参数时你可以用*args.例如,它可以传递任意数量的参数: 12345678def print_everything(*args): for count, thing in enumerate(args): print '&#123;0&#125;. &#123;1&#125;'.format(count, thing)print_everything('apple', 'banana', 'cabbage')# 0. apple# 1. banana# 2. cabbage 相似的, **kwargs允许你使用没有事先定义的参数名: 1234567def table_things(**kwargs): for name, value in kwargs.items(): print '&#123;0&#125; = &#123;1&#125;'.format(name, value)table_things(apple = 'fruit', cabbage = 'vegetable')# cabbage = vegetable# apple = fruit 你也可以混着用.命名参数首先获得参数值然后所有的其他参数都传递给*args和**kwargs.命名参数在列表的最前端.例如: def table_things(titlestring, kwargs)*args和kwargs可以同时在函数的定义中,但是*args必须在**kwargs前面. 当调用函数时你也可以用*和**语法.例如: 1234567def print_three_things(a, b, c): print 'a = &#123;0&#125;, b = &#123;1&#125;, c = &#123;2&#125;'.format(a,b,c)mylist = ['aardvark', 'baboon', 'cat']print_three_things(*mylist)a = aardvark, b = baboon, c = cat Python里的拷贝引用和copy(),deepcopy()的区别 1234567891011121314151617181920import copya = [1, 2, 3, 4, ['a', 'b']] #原始对象b = a #赋值，传对象的引用c = copy.copy(a) #对象拷贝，浅拷贝d = copy.deepcopy(a) #对象拷贝，深拷贝a.append(5) #修改对象aa[4].append('c') #修改对象a中的['a', 'b']数组对象print 'a = ', aprint 'b = ', bprint 'c = ', cprint 'd = ', d# 输出结果：# a = [1, 2, 3, 4, ['a', 'b', 'c'], 5]# b = [1, 2, 3, 4, ['a', 'b', 'c'], 5]# c = [1, 2, 3, 4, ['a', 'b', 'c']]# d = [1, 2, 3, 4, ['a', 'b']]]]></content>
      <tags>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SQL Server窗口函数使用]]></title>
    <url>%2F2017%2F06%2F12%2FSQL%20Server%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[什么是窗口函数 Windows Function窗口函数属于集合函数，作用在行集上，下面这段关于窗口函数的介绍来自 PostgreSQL intro windows function A window function performs a calculation across a set of table rows that are somehow related to the current row. This is comparable to the type of calculation that can be done with an aggregate function. But unlike regular aggregate functions, use of a window function does not cause rows to become grouped into a single output row — the rows retain their separate identities. Behind the scenes, the window function is able to access more than just the current row of the query result. 窗口函数在SQL:2003标准中被添加，并在SQL:2008标准中被细化。传统的关系型数据库：Oracle、Sybase和DB2都已经支持窗口函数，像开源的PostgreSQL里面也已经有了对窗口函数的完整的实现。SQL Server 2005开始对窗口函数有了最初的支持，从SQL Server 2012开始，窗口函数也被SQL Server完全支持。 SQL Server窗口函数窗口函数的应用非常广泛，像分页、去重、分组的基础上返回 Top N 的行、计算 Running Totals、Gaps and islands、百分率, Hierarchy 排序、Pivoting 等等。 窗口函数是整个SQL语句最后被执行的部分，这意味着窗口函数是在SQL查询的结果集上进行的，因此不会受到Group By， Having，Where子句的影响 SQL Server 窗口函数主要用来处理由 OVER 子句定义的行集, 主要用来分析和处理 Running totals Moving averages Gaps and islands 在标准的SQL中，Window Function 的OVER语句中有三个非常重要的元素: Partitioning Ordering Framing 这三种元素的作用可以限制窗口集中的行，如果没有指定任何元素，那么窗口中包含的就是查询结果集中所有的行。 窗口函数的语法： 1234567-- Syntax for SQL Server, Azure SQL Database, and Azure SQL Data Warehouse OVER ( [ &lt;PARTITION BY clause&gt; ] [ &lt;ORDER BY clause&gt; ] [ &lt;ROW or RANGE clause&gt; ] ) Partition Divides the query result set into partitions. The window function is applied to each partition separately and computation restarts for each partition. 通过PARTITION BY 得到的窗口集是基于当前查询结果的当前行的一个集，比如说 PARTITION BY CustomerID，当前行的 CustomerID = 1，那么对于当前行的这个 Window 集就是在当前查询结果之上再加上 CustomerID = 1 的一个查询结果。 Order Defines the logical order of the rows within each partition of the result set. That is, it specifies the logical order in which the window function calculation is performed. Order By子句对于诸如Row_Number()，Rank()，Lead()，LAG()等函数是必须的，因为如果数据无序，这些函数的结果就没有任何意义 ROW / RANGE Further limits the rows within the partition by specifying start and end points within the partition. This is done by specifying a range of rows with respect to the current row either by logical association or physical association. Physical association is achieved by using the ROWS clause. ROWS 子句通过指定当前行之前或之后的固定数目的行，限制分区中的行数。 RANGE 子句通过指定针对当前行中的值的某一范围的值，从逻辑上限制分区中的行数 12345678910ROWS BETWEEN UNBOUNDED PRECEDING | &lt;n&gt; PRECEDING | &lt;n&gt; FOLLOWING | CURRENT ROWorROWS BETWEEN UNBOUNDED FOLLOWING | &lt;n&gt; PRECEDING | &lt;n&gt; FOLLOWING | CURRENT ROW UNBOUNDED PRECEDING 指的是相对于当前行来说之前的所有的行 UNBOUNDED FOLLOWING 指的是相对于当前行来说之后的所有的行 CURRENT ROW 就是当前行 简单的例子下面用一个简单的例子表示传统的聚合函数和窗口函数的区别 有一个需求：将AdventureWorks示例数据库中的Employee表按照性别进行聚合，希望得到的结果是：”登录名，性别，该性别所有员工的总数” 那么传统的写法是用子查询获得按照性别进行聚合的值，然后再关联 1234SELECT [LoginID] , [Gender] , (SELECT COUNT(*) FROM [AdventureWorks2012].[HumanResources].[Employee] a WHERE a.Gender=b.Gender) AS GenderTotalFROM [AdventureWorks2012].[HumanResources].[Employee] b 如果使用窗口函数完成这个功能，代码如下： 1234SELECT [LoginID] , [Gender] , COUNT(*) OVER(PARTITION BY Gender) AS GenderTotalFROM [AdventureWorks2012].[HumanResources].[Employee] 窗口函数与 Group, 子查询语句的比较对于Group来说，SELECT语句中的列必须是Group子句中出现的列或者是聚合列，那么如果需要同时在 SELECT 语句中查询其它的非 Group 或者非聚合列, 那么就需要额外的子查询。 一个和上面例子很相似的情景，比如要查询每个客户的每个订单的值，以及这个订单于这个订单客户的所有订单总和比，以及这个订单与这个客户所有订单平均值的差。 一个SELECT语句肯定是搞不定的，如下面代码： 12345678910111213141516WITH Aggregates AS( SELECT custid , SUM(val) AS sumval , AVG(val) AS avgval FROM Sales.OrderValues GROUP BY custid)SELECT O.orderid , O.custid , O.val , CAST(100. * O.val / A.sumval AS NUMERIC(5, 2)) AS pctcust , O.val - A.avgval AS diffcustFROM Sales.OrderValues AS OJOIN Aggregates AS AON O.custid = A.custid; 因为没有办法在一个Group查询中同时显示 Detail和汇总的信息 如果这时再加一个比 - 单个订单与总订单额/平均额比，这时汇总的级别又不相同了， 需要单独再汇总一次 额~ 又要添加一层子查询聚合 如果提出更多的聚合和比较，查询语句会越来越复杂，并且查询优化器也不能确定每次是否都访问的是同一个数据集，因此需要分别访问数据集，造成性能下降。 通过使用窗口函数可以很容易解决这些问题，因为可以为每一种聚合定义一个窗口上下文。 12345678SELECT orderid , custid , val , CAST(100.* val/ SUM(val) OVER(PARTITION BY custid) AS NUMERIC(5,2)) AS pctcut , val - AVG(val) OVER(PARTITION BY custid) AS diffcust , CAST(100.* val/ SUM(val) OVER() AS NUMERIC(5,2)) AS pctall , val - AVG(val) OVER() AS diffallFROM Sales.OrderValues 使用窗口函数的例子将 OVER 子句与 ROW_NUMBER 函数结合使用下面的脚本将 OVER 子句与 ROW_NUMBER 函数一起使用来显示分区内各行的行号，分区由 PARTITION BY PostalCode确定 1234567891011SELECT ROW_NUMBER() OVER(PARTITION BY PostalCode ORDER BY SalesYTD DESC) AS "Row Number" , p.LastName , s.SalesYTD , a.PostalCode FROM Sales.SalesPerson AS s INNER JOIN Person.Person AS p ON s.BusinessEntityID = p.BusinessEntityID INNER JOIN Person.Address AS a ON a.AddressID = p.BusinessEntityID WHERE TerritoryID IS NOT NULL AND SalesYTD &lt;&gt; 0 ORDER BY PostalCode; 用这种分配行号的方法，可以完成例如分页、去除重复元素、返回每组前N条数据等实际需求 将 OVER 子句与聚合函数结合使用12345678SELECT SalesOrderID, ProductID, OrderQty , SUM(OrderQty) OVER(PARTITION BY SalesOrderID) AS Total , AVG(OrderQty) OVER(PARTITION BY SalesOrderID) AS "Avg" , COUNT(OrderQty) OVER(PARTITION BY SalesOrderID) AS "Count" , MIN(OrderQty) OVER(PARTITION BY SalesOrderID) AS "Min" , MAX(OrderQty) OVER(PARTITION BY SalesOrderID) AS "Max" FROM Sales.SalesOrderDetail WHERE SalesOrderID IN(43659,43664); 生成移动平均值和累计合计下面的示例将 AVG 和 SUM 函数与 OVER 子句结合使用，以便为 Sales.SalesPerson 表中的每个地区提供年度销售额的累计合计。 数据按 TerritoryID 分区并在逻辑上按 SalesYTD 排序 12345678910111213SELECT BusinessEntityID , TerritoryID , DATEPART(yy,ModifiedDate) AS SalesYear , CONVERT(varchar(20),SalesYTD,1) AS SalesYTD , CONVERT(varchar(20),AVG(SalesYTD) OVER (PARTITION BY TerritoryID ORDER BY DATEPART(yy,ModifiedDate) ),1) AS MovingAvg , CONVERT(varchar(20),SUM(SalesYTD) OVER (PARTITION BY TerritoryID ORDER BY DATEPART(yy,ModifiedDate) ),1) AS CumulativeTotal FROM Sales.SalesPerson WHERE TerritoryID IS NULL OR TerritoryID &lt; 5 ORDER BY TerritoryID, SalesYear; 在 OVER 子句中指定的 ORDER BY 子句将确定应用 AVG 函数的逻辑顺序。 再往下，ORDER BY之后也可以指定 ROWS 子句进一步限制窗口的大小 12345678SELECT BusinessEntityID, TerritoryID , DATEPART(yy,ModifiedDate) AS SalesYear , CONVERT(varchar(20),SalesYTD,1) AS SalesYTD , CONVERT(varchar(20),SUM(SalesYTD) OVER (PARTITION BY TerritoryID ORDER BY DATEPART(yy,ModifiedDate) ROWS BETWEEN CURRENT ROW AND 1 FOLLOWING ),1) AS CumulativeTotal FROM Sales.SalesPerson WHERE TerritoryID IS NULL OR TerritoryID &lt; 5; 在这个例子里面， ROWS子句 ROWS BETWEEN CURRENT ROW AND 1 FOLLOWING 限制窗口为： 当前行的行 对其 下面1行 所以查询结果为： 123456789101112131415SELECT t.OrderYear , t.OrderMonth , t.TotalDue , SUM(t.TotalDue) OVER(PARTITION BY OrderYear, OrderMonth ORDER BY t.OrderYear, t.OrderMonth ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS 'RunningTotal'FROM( SELECT YEAR(OrderDate) AS 'OrderYear' , MONTH(OrderDate) AS 'OrderMonth' , SalesPersonID , TotalDue FROM Sales.SalesOrderHeader ) AS tWHERE t.SalesPersonID = 274 AND t.OrderYear = 2005 在这个例子中，窗口被限制为：第一行 (UNBOUNDED PRECEDING) 到当前行 (CURRENT ROW) 查询结果为： 所以11月份的累计总和为4723 和 7140(4723.1073+2417.4793) 如果把ROWS限制改成RANGE会怎么样呢? 结果如下： RANGE选项包含窗口里的所有行，和当前行有相同ORDER BY值。上面的例子里面，对于2005年11月的2条记录你拿到同个汇总，因为这2行有同样的ORDER BY值（2005年11月） note: 使用ROWS选项你在物理级别定义在你窗口里有多少行。使用RANGE选项取决于ORDER BY值在窗口里有多少行被包含]]></content>
      <tags>
        <tag>SQL Server</tag>
        <tag>SQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Anaconda解决python包管理与环境管理]]></title>
    <url>%2F2017%2F06%2F08%2FAnaconda%E8%A7%A3%E5%86%B3python%E5%8C%85%E7%AE%A1%E7%90%86%E4%B8%8E%E7%8E%AF%E5%A2%83%E7%AE%A1%E7%90%86%2F</url>
    <content type="text"><![CDATA[Anaconda是一个用于科学计算的Python发行版，支持 Linux, Mac, Windows系统，提供了包管理与环境管理的功能，可以很方便地解决多版本python并存、切换以及各种第三方包安装问题。 conda可以理解为一个工具，也是一个可执行命令，其核心功能是包管理与环境管理 提供包管理，功能类似于 pip，Windows 平台安装第三方包经常失败的场景得以解决。 提供虚拟环境管理，功能类似于 virtualenv，解决了多版本Python并存问题 Anaconda具有跨平台、包管理、环境管理的特点，因此很适合快速在新的机器上部署Python环境。 Anaconda的下载页参见 官网下载 Anaconda安装成功之后，可以检查所安装的版本 1conda --version 由于不可名状的原因，需要修改其包管理镜像为国内源 12conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/conda config --set show_channel_urls yes Conda的环境管理Conda的环境管理功能允许我们同时安装若干不同版本的Python，并能自由切换。对于上述安装过程，假设我们采用的是Python 2.7对应的安装包，那么Python 2.7就是默认的环境 现在，创建一个自定义的Python环境 1conda create -n py27 python=2.7 其中py27是新添加环境的名字，可以自定义修改。 之后通过activate py27和deactivate py27命令激活、退出该环境 1activate py27 现在把创建的环境都列出来，其中当前使用的环境前面用*号标注 1conda info --envs 123456789101112131415161718# 创建一个名为py3的环境，指定Python版本是3.4（不用管是3.4.x，conda会为我们自动寻找3.4.x中的最新版本）conda create --name py3 python=3.4 # 安装好后，使用activate激活某个环境activate py3 # for Windowssource activate py3 # for Linux &amp; Mac# 激活后，会发现terminal输入的地方多了py3的字样，实际上，此时系统做的事情就是把默认2.7环境从PATH中去除，再把3.4对应的命令加入PATH # 此时，再次输入python --version# 可以得到`Python 3.4.5 :: Anaconda 4.1.1 (64-bit)`，即系统已经切换到了3.4的环境 # 如果想返回默认的python 2.7环境，运行deactivate py3 # for Windowssource deactivate py3 # for Linux &amp; Mac # 删除一个已有的环境conda remove --name py3 --all Conda的包管理现在要使用conda来管理包了，以前常用的是Python的pip包管理工具。 1234567# 安装scipyconda install scipy# conda会从从远程搜索scipy的相关信息和依赖项目，对于python 3.4，conda会同时安装numpy和mkl（运算加速的库） # 查看已经安装的packagesconda list# 最新版的conda是从site-packages文件夹中搜索已经安装的包，不依赖于pip，因此可以显示出通过各种方式安装的包 实例：让Python2和3在Jupyter Notebook中共存多版本的Python或者R等语言，在Jupyter中被称作kernel。 如果这个Python版本已经存在（比如我们刚才添加的py27环境），那么你可以直接为这个环境安装 ipykernel包 1conda install -n py27 ipykernel note: -n 后面的名字为所要安装到的环境名 然后激活这个环境 1python -m ipykernel install --user 如果所需版本并不是已有的环境，可以直接在创建环境时便为其预装 ipykernel。 1conda create -n py27 python=2.7 ipykernel 打开jupyter notebook 1jupyter notebook 最后两个字：省心 附： conda cheat sheet]]></content>
      <tags>
        <tag>Python</tag>
        <tag>Anaconda</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[T-SQL查询语句执行顺序]]></title>
    <url>%2F2017%2F06%2F06%2FT-SQL%E6%9F%A5%E8%AF%A2%E8%AF%AD%E5%8F%A5%E6%89%A7%E8%A1%8C%E9%A1%BA%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[数据库查询语句可以说是基础中的基础，查询语句是后面查询性能优化的基础，但是很多人并不能很准确的说出数据库查询的逻辑流程。 SQL 语句中元素逻辑查询处理，指的是标准SQL定义的如何处理查询和返回最终结果的概念性路径。和其他的语言代码执行顺序不同，SQL 查询不是按照代码顺序来进行逻辑查询。下面这段SQL 查询 12345678SELECT empid , YEAR(orderdate) AS OrderYear , COUNT(*) AS NumOrdersFROM Sales.OrdersWHERE custid = 71GROUP BY empid, YEAR(orderdate)HAVING COUNT(*) &gt; 1ORDER BY empid, OrderYear 这段SQL 语句其实是按照下面的顺序进行的逻辑处理： FROM WHERE GROUP BY HAVING SELECT ORDER BY FROM 子句指定要查询的表名称和进行多表运算的表运算符(JOIN)； WHERE 子句可以指定一个谓词或者逻辑表达式来筛选由 FROM阶段返回的行； GROUP BY阶段允许用户把前面阶段返回的行排列到组中； HAVING 子句可以指定一个谓词来筛选前面GROUP出的组，而不是筛选单个行； SELECT 子句用户指定要返回到查询结果表中属性(列)； ORDER BY 子句允许对输出行进行排序。 流程图这里引用 Itzik Ben-Gan 的流程图 分步分析FROM 子句FROM阶段标识出查询的来源表，并处理表运算符。在涉及到联接运算的查询中（各种join），主要有以下几个步骤： 求笛卡尔积。不论是什么类型的联接运算，首先都是执行交叉连接（cross join），求笛卡儿积，生成虚拟表VT1-J1。 ON筛选器。这个阶段对上个步骤生成的VT1-J1进行筛选，根据ON子句中出现的谓词进行筛选，让谓词取值为true的行通过了考验，插入到VT1-J2。 添加外部行。如果指定了outer join，还需要将VT1-J2中没有找到匹配的行，作为外部行添加到VT1-J2中，生成VT1-J3。 经过以上步骤，FROM阶段就完成了。概括地讲，FROM阶段就是进行预处理的，根据提供的运算符对语句中提到的各个表进行处理（除了join，还有apply，pivot，unpivot） WHERE 子句WHERE阶段是根据&lt;where_predicate&gt;中条件对VT1中的行进行筛选，让条件成立的行才会插入到VT2中。 GROUP BY阶段GROUP阶段按照指定的列名列表，将VT2中的行进行分组，生成VT3。最后每个分组只有一行。 HAVING阶段该阶段根据HAVING子句中出现的谓词对VT3的分组进行筛选，并将符合条件的组插入到VT4中。 SELECT阶段这个阶段是投影的过程，处理SELECT子句提到的元素，产生VT5。这个步骤一般按下列顺序进行 计算SELECT列表中的表达式，生成VT5-1。 若有DISTINCT，则删除VT5-1中的重复行，生成VT5-2 若有TOP，则根据ORDER BY子句定义的逻辑顺序，从VT5-2中选择签名指定数量或者百分比的行，生成VT5-3 ORDER BY阶段根据ORDER BY子句中指定的列明列表，对VT5-3中的行，进行排序，生成游标VC6. 当然SQL SERVER在实际的查询过程中，有查询优化器来生成实际的工作计划。以何种顺序来访问表，使用什么方法和索引，应用哪种联接方法，都是由查询优化器来决定的。优化器一般会生成多个工作计划，从中选择开销最小的那个去执行。逻辑查询处理都有非常特定的顺序，但是优化器常常会走捷径。]]></content>
      <tags>
        <tag>SQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo command Cheatsheet]]></title>
    <url>%2F2017%2F06%2F05%2FHexo-command-Cheatsheet%2F</url>
    <content type="text"><![CDATA[hexo123npm install hexo -g #安装 npm update hexo -g #升级 hexo init #初始化 简写12345hexo n "我的博客" # == hexo new "我的博客" #新建文章hexo p # == hexo publish #发布草稿hexo g # == hexo generate#生成hexo s # == hexo server #启动服务预览hexo d # == hexo deploy#部署 服务器12345678hexo server #Hexo 会监视文件变动并自动更新，您无须重启服务器。hexo server -s #静态模式hexo server -p 5000 #更改端口hexo server -i 192.168.1.1 #自定义 IPhexo clean #清除缓存 网页正常情况下可以忽略此条命令hexo g #生成静态网页hexo d #开始部署 监视文件变动12hexo generate #使用 Hexo 生成静态文件快速而且简单hexo generate --watch #监视文件变动 完成后部署两个命令的作用是相同的 12345hexo generate --deployhexo deploy --generatehexo deploy -ghexo server -g 草稿1hexo publish [layout] &lt;title&gt; 模版123456789hexo new "postName" #新建文章hexo new page "pageName" #新建页面hexo generate #生成静态页面至public目录hexo server #开启预览访问端口（默认端口4000，'ctrl + c'关闭server）hexo deploy #将.deploy目录部署到GitHubhexo new [layout] &lt;title&gt;hexo new photo "My Gallery"hexo new "Hello World" --lang tw 变量 描述 layout 布局 title 标题 date 文件建立日期]]></content>
      <tags>
        <tag>hexo</tag>
        <tag>Cheat Sheet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo设置主题和其他配置]]></title>
    <url>%2F2017%2F06%2F05%2FHexo%E8%AE%BE%E7%BD%AE%E4%B8%BB%E9%A2%98%E5%92%8C%E5%85%B6%E4%BB%96%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[博客的主题很多，我选择一个非常流行的主题 NexT，作为我博客的主题，这个主题简约，并且文档和维护都很好。 下面记录一下我的Hexo站点的配置，以及NexT主题的配置，以备日后查找 站点配置博客根目录下的_config.yml 文件是站点的配置文件。 为了能够使Hexo部署到GitHub上，需要安装一个插件： 1npm install hexo-deployer-git --save 设置站点配置文件指定部署的位置 1234deploy: type: git repo: https://github.com/lvraikkonen/lvraikkonen.github.io.git branch: master 在 站点配置文件 中找到theme字段，把值改为 next 主题NexT配置选择主题样式Scheme 是 NexT 提供的一种特性，借助于 Scheme，NexT 为你提供多种不同的外观。同时，几乎所有的配置都可以 在 Scheme 之间共用。目前 NexT 支持三种 Scheme，他们是： Muse - 默认 Scheme，这是 NexT 最初的版本，黑白主调，大量留白 Mist - Muse 的紧凑版本，整洁有序的单栏外观 Pisces - 双栏 Scheme，小家碧玉似的清新 (我的) 123#scheme: Muse#scheme: Mistscheme: Pisces 头像设置编辑 主题配置文件， 修改字段 avatar， 值设置成头像的链接地址。其中，头像的链接地址可以是： 地址 值 完整的互联网URI http://example.com/avatar.png 站内地址 - 将头像放置主题目录下的 source/uploads/ 或者 放置在 source/images/ 目录下 配置为：avatar: /images/avatar.png 添加标签页和分类页12hexo new page "tags"hexo new page "categories" 同时，在/source目录下会生成一个tags文件夹和categories文件夹，里面各包含一个index.md文件。 修改/source/tags目录下的 index.md文件 12345title: tagsdate: 2017-05-29 18:16:02type: "tags"--- 修改/source/categories目录下的 index.md文件 12345title: categoriesdate: 2015-09-29 18:17:14type: "categories"--- 修改 主题配置文件， 去掉相应的注释 1234567menu: home: / #主页 categories: /categories #分类页（需手动创建） #about: /about #关于页面（需手动创建） archives: /archives #归档页 tags: /tags #标签页（需手动创建） #commonweal: /404.html #公益 404 （需手动创建） 设置网站的图标Favicon从网上找一张 icon 图标文件，放在 source 目录下就可以了 添加友情链接在 站点配置文件 中添加参数： 1234links_title: 友情链接links: #百度: http://www.baidu.com/ #新浪: http://example.com/ 设置代码高亮NexT 使用 Tomorrow Theme 作为代码高亮，共有5款主题供你选择。 NexT 默认使用的是 白色的 normal 主题，可选的值有 normal，night， night blue， night bright， night eighties 更改 highlight_theme 字段，将其值设定成你所喜爱的高亮主题 配置Algolia 搜索官网 注册一个账号，可以用Github账户注册 登录进入Dashboard控制台页面，创建一个新Index 进入 API Keys 界面，拷贝 Application ID 、Search-Only API Key 和 Admin API Key 编辑 站点配置文件 ，新增以下配置： 123456algolia: applicationID: 'your applicationID' apiKey: 'your Search-Only API Key' adminApiKey: 'your Admin API Key' indexName: 'your newcreated indexName' chunkSize: 5000 安装Hexo Algolia在Hexo根目录执行如下指令，进行Hexo Algolia的安装： 1npm install --save hexo-algolia 到Hexo的根目录，在其中找到package.json文件，修改其中的hexo-algolia属性值为^0.2.0，如下图所示： 当配置完成，在站点根目录下执行hexo algolia 来更新Index 1hexo algolia 注意： 如果发现没有上传数据，这时候可以先 hexo clean 然后再 hexo algolia _ 在 主题配置文件中，找到Algolia Search 配置部分： 123456789# Algolia Searchalgolia_search: enable: true hits: per_page: 10 labels: input_placeholder: Search for Posts hits_empty: "We didn't find any results for the search: $&#123;query&#125;" hits_stats: "$&#123;hits&#125; results found in $&#123;time&#125; ms" 将 enable 改为true 即可，根据需要你可以调整labels 中的文本。 配置来必力评论评论插件，最出名的是 Disqus，但是对于国内用户来说，自带梯子好些。改用多说，路边社消息，多说好像要完蛋了。发现了个叫 LiveRe（来必力）的评论插件，韩国出的，用着感觉还不错。 来必力官网注册账号 LiveRe 有两个版本： City 版：是一款适合所有人使用的免费版本； Premium 版：是一款能够帮助企业实现自动化管理的多功能收费版本。 选择City版就可以了 获取 LiveRe UID。 编辑 主题配置文件， 编辑 livere_uid 字段，设置如下： 1livere_uid: #your livere_uid NexT 已经支持来必力，这样就能在页面显示评论了 添加阅读次数统计这里使用 LeanCloud 为文章添加统计功能 注册账号登陆以后获得 AppID以及 AppKey这两个参数即可正常使用文章阅读量统计的功能了。 创建应用 配置应用 在应用的数据配置界面，左侧下划线开头的都是系统预定义好的表。在弹出的选项中选择创建Class来新建Class用来专门保存我们博客的文章访问量等数据:为了保证我们前面对NexT主题的修改兼容，此处的新建Class名字为 Counter 复制 AppID以及 AppKey 修改 主题配置文件 12345# You can visit https://leancloud.cn get AppID and AppKey.leancloud_visitors: enable: true app_id: #&lt;app_id&gt; app_key: #&lt;app_key&gt; 重新生成并部署博客就可以显示阅读量了 note: 记录文章访问量的唯一标识符是 文章的发布日期以及文章的标题，因此请确保这两个数值组合的唯一性，如果你更改了这两个数值，会造成文章阅读数值的清零重计 后台可以看到，刚才创建的Counter类 添加字数统计功能首先在博客目录下使用 npm 安装插件 1npm install hexo-wordcount --save 在 主题配置文件中打开wordcount 统计功能 123456# Post wordcount display settings# Dependencies: https://github.com/willin/hexo-wordcountpost_wordcount: item_text: true wordcount: true min2read: false 找到..\themes\next\layout_macro\post.swig 文件，将”字”、”分钟” 字样添加到如下位置 1234567&lt;span title=&quot;&#123;&#123; __(&apos;post.wordcount&apos;) &#125;&#125;&quot;&gt; &#123;&#123; wordcount(post.content) &#125;&#125; 字&lt;/span&gt; ...&lt;span title=&quot;&#123;&#123; __(&apos;post.min2read&apos;) &#125;&#125;&quot;&gt; &#123;&#123; min2read(post.content) &#125;&#125; 分钟&lt;/span&gt;]]></content>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SQL Server索引]]></title>
    <url>%2F2017%2F06%2F02%2FSQL%20Server%E7%B4%A2%E5%BC%95%2F</url>
    <content type="text"><![CDATA[索引是与表或视图关联的磁盘上结构，可以加快从表或视图中检索行的速度。在SQL Server中，索引和表（这里指的是加了聚集索引的表）的存储结构是一样的,都是B树，B树是一种用于查找的平衡多叉树。 索引的利弊查询执行的大部分开销是I/O，使用索引提高性能的一个主要目标是避免全表扫描，因为全表扫描需要从磁盘上读取表的每一个数据页，如果有索引指向数据值，则查询只需要读少数次的磁盘就行啦。所以合理的使用索引能加速数据的查询。但是索引并不总是提高系统的性能，带索引的表需要在数据库中占用更多的存储空间，同样用来增删数据的命令运行时间以及维护索引所需的处理时间会更长。所以我们要合理使用索引，及时更新去除次优索引。 数据表的基本结构一个新表被创建之时，系统将在磁盘中分配一段以8K为单位的连续空间，当字段的值从内存写入磁盘时，就在这一既定空间随机保存，当一个 8K用完的时候，数据库指针会自动分配一个8K的空间。这里，每个8K空间被称为一个数据页（Page），又名页面或数据页面，并分配从0-7的页号, 每个文件的第0页记录引导信息，叫文件头（File header）；每8个数据页（64Ｋ）的组合形成扩展区（Extent），称为扩展。全部数据页的组合形成堆（Heap） 聚集索引和非聚集索引在SQL SERVER中，聚集索引的存储是以B树存储，B树的叶子直接存储聚集索引的数据 非聚集索引与聚集索引具有相同的 B 树结构，它们之间的显著差别在于以下两点： 基础表的数据行不按非聚集键的顺序排序和存储。 非聚集索引的叶层是由索引页而不是由数据页组成。 非聚集索引也是一个B树结构，与聚集索引不同的是，B树的叶子节点存的是指向堆或聚集索引的指针。 索引的设计原则]]></content>
      <tags>
        <tag>SQL Server</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[看懂SQL Server执行计划]]></title>
    <url>%2F2017%2F06%2F02%2F%E7%9C%8B%E6%87%82SQL%20Server%E6%89%A7%E8%A1%8C%E8%AE%A1%E5%88%92%2F</url>
    <content type="text"><![CDATA[当我们写的SQL语句传到SQL Server的时候，查询分析器会将语句依次进行解析（Parse）、绑定（Bind）、查询优化（Optimization，有时候也被称为简化）、执行（Execution）。除去执行步骤外，前三个步骤之后就生成了执行计划，也就是SQL Server按照该计划获取物理数据方式，最后执行步骤按照执行计划执行查询从而获得结果。 执行计划 查询优化器对输入的 T-SQL 查询语句通过”计算”而选择出效率最高的一种执行方案，这个执行方案就是执行计划。 执行计划可以告诉你这个查询将会被如何执行或者已经被如何执行过，可以通过执行计划看到 SQL 代码中那些效率比较低的地方。 查看执行计划的方式我们可以通过图形化的界面，或者文本，或者XML格式查看，这样会比较方便理解执行计划要表达出来的意思。 当一个查询被提交到 SQL Server 后，服务器端很多进程实际上要做很多事情来确保数据出入的完整性。 对于 T-SQL 来说, 处理的主要有两个阶段：关系引擎阶段( relational engine)和存储引擎阶段( storage engine)。 关系引擎主要要做的事情就是首先确保 Query 语句能够正确的解析，然后交给查询优化并产生执行计划，然后执行计划就以二进制格式发到存储引擎来更新或者读取数据。 存储引擎主要处理的比如像锁、索引的维护和事务等 所以对于执行计划，重点的是关注关系引擎。 估算执行计划和实际执行计划Estimated Execution Plans vs. Actual Execution Plans 它们之间的区别就是: 估算的执行计划 是从查询优化器来的，是输入关系引擎的，它的执行步骤包括一些运算符等等都是通过一系列的逻辑分析出来的，是一种通过逻辑推算出来的计划，只能代表查询优化器的观点；实际执行计划 是真实的执行了”估算执行计划”后的一种真实的结果，是实实在在真实的执行反馈, 是属于存储引擎。 以上描述了关于执行计划的概念，下面以实际案例去解读一些基本语句， 例如SELECT, UPDATE,INSERT, DELETE 等查询的执行计划。 ————————————我是分隔符———————————– 有大约78个执行计划中的操作符，可以去 MSDN Book Online 随时查 下表表示一下常见的执行计划元素 Select (Result) Sort Spool Clustered Index Scan Key Lookup Eager Spool NonClustered Index Scan Compute Scalar Stream Aggregate Clustered Index Seek Constant Scan Distribute Streams NonClustered Index Seek Table Scan Repartition Streams Hash Match RID Lookup Gather Streams Nested Loops Filter Bitmap Merge Join Lazy Spool Split 操作符分为阻断式 blocking 和非阻断式non-blocking 常见操作符的执行计划解释Table Scan 表扫描 当表中没有聚集索引，又没有合适索引的情况下，会出现这个操作。这个操作是很耗性能的，他的出现也意味着优化器要遍历整张表去查找你所需要的数据 Clustered Index Scan / Index Scan 聚集索引扫描/非聚集索引扫描 这个图标两个操作都可以使用，一个聚集索引扫描，一个是非聚集索引扫描。 聚集索引扫描：聚集索引的数据体积实际是就是表本身，也就是说表有多少行多少列，聚集所有就有多少行多少列，那么聚集索引扫描就跟表扫描差不多，也要进行全表扫描，遍历所有表数据，查找出你想要的数据。 非聚集索引扫描：非聚集索引的体积是根据你的索引创建情况而定的，可以只包含你要查询的列。那么进行非聚集索引扫描，便是你非聚集中包含的列的所有行进行遍历，查找出你想要的数据。 看下面这个查询 12SELECT ct.*FROM Person.ContactType AS ct; 这个表有一个聚簇索引PK_ContactType_ContactTypeID，聚簇索引的叶子结点是存储数据的，所以对于这个聚簇索引的扫描和全表扫面基本类似，基本也是一行一行地进行扫描来满足查询。 如果在执行计划中遇到索引扫描，说明查询有可能返回比需要更多的行，这时候建议使用 WHERE语句去优化查询，确保只是需要的那些行被返回。 Clustered Index Seek / Index Seek 聚集索引查找/非聚集索引查找 聚集索引查找和非聚集索引查找都是使用该图标。 聚集索引查找：聚集索引包含整个表的数据，也就是在聚集索引的数据上根据键值取数据。 非聚集索引查找：非聚集索引包含创建索引时所包含列的数据，在这些非聚集索引的数据上根据键值取数据。 123SELECT ct.*FROM Person.ContactType AS ctWHERE ct.ContactTypeID = 7 这个表有一个聚簇索引PK_ContactType_ContactTypeID，建在ContactTypeID字段上，查询使用了这个聚簇索引来查找指定的数据。 索引查找和索引扫描不同，使用查找可以让优化器准确地通过键值找到索引的位置。 以上几种查询的性能对比： [Table Scan] 表扫描（最慢）：对表记录逐行进行检查 [Clustered Index Scan] 聚集索引扫描（较慢）：按聚集索引对记录逐行进行检查 [Index Scan] 索引扫描（普通）：根据索引滤出部分数据在进行逐行检查 [Index Seek] 索引查找（较快）：根据索引定位记录所在位置再取出记录 [Clustered Index Seek] 聚集索引查找（最快）：直接根据聚集索引获取记录 Key Lookup 键值查找 首先需要说的是查找，查找与扫描在性能上完全不是一个级别的，扫描需要遍历整张表，而查找只需要通过键值直接提取数据，返回结果，性能要好。 当你查找的列没有完全被非聚集索引包含，就需要使用键值查找在聚集索引上查找非聚集索引不包含的列。 RID Lookup RID查找 跟键值查找类似，只不过RID查找，是需要查找的列没有完全被非聚集索引包含，而剩余的列所在的表又不存在聚集索引，不能键值查找，只能根据行表示Rid来查询数据。 123456SELECT p.BusinessEntityID , p.LastName , p.FirstName , p.NameStyleFROM Person.Person AS pWHERE p.LastName LIKE 'Jaf%'; Person.Person 表有非聚簇索引 IX_Person_LastName_FirstName_MiddleName作用在LastName、FirstName和MiddleName列上面，而列 NameStyle并没有被非聚集索引所包含，所以需要使用 KeyLookUp在聚集索引上查找不包含的列。如果这个列所在的表不存在聚集索引，那就只能通过RId，也就是行号在查询了。 Sort对数据集合进行排序，需要注意的是，有些数据集合在索引扫描后是自带排序的。 Filter根据出现在having之后的操作运算符，进行筛选 Computer Scalar在需要查询的列中需要自定义列，比如count(*) as cnt , select name+’’+age 等会出现此符号。 JOIN 连接查询当多表连接时，SQL Server会采用三类不同的连接方式：散列连接，循环嵌套连接，合并连接 Hash Join 这个图标有两种地方用到，一种是表关联，一种是数据聚合运算时 (GROUP BY) 下面有两个概念： Hashing：在数据库中根据每一行的数据内容，转换成唯一符号格式，存放到临时哈希表中，当需要原始数据时，可以给还原回来。类似加密解密技术，但是他能更有效的支持数据查询。 Hash Table：通过hashing处理，把数据以key/value的形式存储在表格中，在数据库中他被放在tempdb中。 Hash Join是做大数据集连接时的常用方式，优化器使用两个表中较小（相对较小）的表利用Join Key在内存中建立散列表 Hash Table，然后扫描较大的表并探测散列表，找出与Hash表匹配的行。这种方式适用于较小的表完全可以放于内存中的情况 如果在执行计划中见到Hash Match Join，也许应该检查一下是不是缺少或者没有使用索引、没有用到WHERE等等。 Nested Loops Join 这个操作符号，把两个不同列的数据集汇总到一张表中。提示信息中的Output List中有两个数据集，下面的数据集（inner set）会一一扫描与上面的数据集（out set），直到扫描完为止，这个操作才算是完成。 对于被连接的数据子集较小的情况，嵌套循环连接是个较好的选择。在嵌套循环中，内表被外表驱动，外表返回的每一行都要在内表中检索找到与它匹配的行，因此整个查询返回的结果集不能太大 Merge Join 这种关联算法是对两个已经排过序的集合进行合并。如果两个聚合是无序的则将先给集合排序再进行一一合并，由于是排过序的集合，左右两个集合自上而下合并效率是相当快的。 通常情况下散列连接的效果都比排序合并连接要好，然而如果行源已经被排过序，在执行排序合并连接时不需要再排序了，这时排序合并连接的性能会优于散列连接。Merge join 用在没有索引，并且数据已经排序的情况。 12345SELECT c.CustomerIDFROM Sales.SalesOrderDetail odJOIN Sales.SalesOrderHeader ohON od.SalesOrderID = oh.SalesOrderIDJOIN Sales.Customer c ON oh.CustomerID = c.CustomerID 由于没有使用WHERE语句，所以优化器对Customer表使用聚集索引扫描，对SalesOrderHeader表使用非聚集索引扫描 Customer表和SalesOrderHeader表使用Merge Join操作符进行关联，关联字段是CustomerID字段，这个字段在上面的索引扫描之后都是有序的。如果不是有序的，优化器会在前面进行排序或者是直接将两个表进行Hash Join连接。 根据执行计划细节要做的优化操作 如果select * 通常情况下聚集索引会比非聚集索引更优。 如果出现Nested Loops，需要查下是否需要聚集索引，非聚集索引是否可以包含所有需要的列。 Hash Match连接操作更适合于需要做Hashing算法集合很小的连接。 Merge Join时需要检查下原有的集合是否已经有排序，如果没有排序，使用索引能否解决。 出现表扫描，聚集索引扫描，非聚集索引扫描时，考虑语句是否可以加where限制，select * 是否可以去除不必要的列。 出现Rid查找时，是否可以加索引优化解决。 在计划中看到不是你想要的索引时，看能否在语句中强制使用你想用的索引解决问题，强制使用索引的办法Select CluName1,CluName2 from Table with(index=IndexName)。 看到不是你想要的连接算法时，尝试强制使用你想要的算法解决问题。强制使用连接算法的语句：select * from t1 left join t2 on t1.id=t2.id option(Hash/Loop/Merge Join) 看到不是你想要的聚合算法是，尝试强制使用你想要的聚合算法。强制使用聚合算法的语句示例：select age ,count(age) as cnt from t1 group by age option(order/hash group) 看到不是你想要的解析执行顺序是，或这解析顺序耗时过大时，尝试强制使用你定的执行顺序。option（force order） 看到有多个线程来合并执行你的sql语句而影响到性能时，尝试强制是不并行操作。option（maxdop 1） 在存储过程中，由于参数不同导致执行计划不同，也影响啦性能时尝试指定参数来优化。option（optiomize for（@name=’zlh’）） 不操作多余的列，多余的行，不做务必要的聚合，排序。]]></content>
      <tags>
        <tag>SQL Server</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Markdown Cheatsheet]]></title>
    <url>%2F2017%2F05%2F31%2FMarkdown-Cheatsheet%2F</url>
    <content type="text"><![CDATA[开始写博客后，Markdown成为一种常用的文档格式，Markdown 的语法全由一些符号所组成，这些符号经过精挑细选，其作用一目了然，里面的语法记下来，为了以后查询方便。 文本样式1234*斜体*或_斜体_**粗体*****加粗斜体***~~删除线~~ 斜体 或 _斜体_ 粗体 加粗斜体 删除线 This text will be italic This will also be italic This text will be bold This will also be bold You can combine them 引用使用大于号 &gt; 表示引用内容： As Grace Hopper said: I’ve always been more interestedin the future than in the past. 分级标题第一种写法 1234这是一个一级标题============================这是一个二级标题-------------------------------------------------- 第二种写法 123456# 一级标题## 二级标题### 三级标题#### 四级标题##### 五级标题###### 六级标题 超链接Markdown 支持两种形式的链接语法： 行内式和参考式两种形式，行内式一般使用较多。 行内式语法说明： []里写链接文字，()里写链接地址, ()中的”“中可以为链接指定title属性，title属性可加可不加。title属性的效果是鼠标悬停在链接上会出现指定的 title文字。链接文字’这样的形式。链接地址与链接标题前有一个空格。 12欢迎来到[我的博客](https://lvraikkonen.github.io/)欢迎来到[我的博客](https://lvraikkonen.github.io/ &quot;博客名&quot;) 欢迎来到我的博客 欢迎来到我的博客 参考式参考式超链接一般用在学术论文上面，或者另一种情况，如果某一个链接在文章中多处使用，那么使用引用 的方式创建链接将非常好，它可以让你对链接进行统一的管理。 语法说明：参考式链接分为两部分，文中的写法 [链接文字][链接标记]，在文本的任意位置添加[链接标记]:链接地址 “链接标题”，链接地址与链接标题前有一个空格。 如果链接文字本身可以做为链接标记，你也可以写成[链接文字][][链接文字]：链接地址的形式，见代码的最后一行。 123456我经常去的几个网站[Google][1]、[Leanote][2]以及[自己的博客][3][Leanote 笔记][2]是一个不错的[网站][]。[1]:http://www.google.com &quot;Google&quot;[2]:http://www.leanote.com &quot;Leanote&quot;[3]:https://lvraikkonen.github.io &quot;lvraikkonen&quot;[网站]:https://lvraikkonen.github.io 我经常去的几个网站Google、Leanote 以及自己的博客，Leanote 笔记是一个不错的网站。 自动链接语法说明：Markdown 支持以比较简短的自动链接形式来处理网址和电子邮件信箱，只要是用&lt;&gt;包起来， Markdown 就会自动把它转成链接。一般网址的链接文字就和链接地址一样，例如： 12&lt;http://example.com/&gt;&lt;address@example.com&gt; 显示效果： http://example.com/ &#x61;&#100;&#x64;&#114;&#101;&#x73;&#115;&#x40;&#101;&#120;&#x61;&#x6d;&#112;&#x6c;&#x65;&#x2e;&#x63;&#111;&#109; 列表无序列表使用星号 *、加号 +或是减号 - 作为列表标记 Red Green Blue 有序列表则使用数字接着一个英文句点： Bird McHale Parish 列表项目可以包含多个段落，每个项目下的段落都必须缩进 4 个空格或是 1 个制表符： This is a list item with two paragraphs. Lorem ipsum dolorsit amet, consectetuer adipiscing elit. Aliquam hendreritmi posuere lectus. Vestibulum enim wisi, viverra nec, fringilla in, laoreetvitae, risus. Donec sit amet nisl. Aliquam semper ipsumsit amet velit. Suspendisse id sem consectetuer libero luctus adipiscing. 如果要在列表项目内放进引用，那 &gt; 就需要缩进： A list item with a blockquote: This is a blockquoteinside a list item. 图像和链接很相似的语法来标记图片，同样也允许两种样式： 行内式和参考式 行内式 一个惊叹号 ! 接着一个方括号，里面放上图片的替代文字 接着一个普通括号，里面放上图片的网址，最后还可以用引号包住并加上 选择性的 ‘title’ 文字 123![Alt text](/path/to/img.jpg)![Alt text](/path/to/img.jpg &quot;Optional title&quot;) Inline-style: 参考式id是图片参考的名称，图片参考的定义方式则和链接参考一样：12![Alt text][id][id]: url/to/image &quot;Optional title attribute&quot; Reference-style: 代码如果要标记一小段行内代码，你可以用反引号把它包起来 code 也可以使用三个反引号包裹一段代码，并指定一种语言 12var s = "JavaScript syntax highlighting";alert(s); 12s = "Python syntax highlighting"print s 12No language indicated, so no syntax highlighting.But let&apos;s throw in a &lt;b&gt;tag&lt;/b&gt;. 表格 Tables Are Cool col 3 is right-aligned $1600 col 2 is centered $12 zebra stripes are neat $1 以下是Github支持的markdown语法： To-Do listTASK LISTS [x] this is a complete item [ ] this is an incomplete item [x] @mentions, #refs, links, formatting, and tags supported [x] list syntax required (any unordered or ordered list supported) EMOJIGitHub supports emoji!:+1: :sparkles: :camel: :tada::rocket: :metal: :octocat:]]></content>
      <tags>
        <tag>Cheat Sheet</tag>
        <tag>Markdown</tag>
        <tag>备忘</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计短链接TinyURL]]></title>
    <url>%2F2017%2F05%2F27%2F%E8%AE%BE%E8%AE%A1%E7%9F%AD%E9%93%BE%E6%8E%A5TinyURL%2F</url>
    <content type="text"></content>
      <tags>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python Collection集合模块]]></title>
    <url>%2F2017%2F05%2F27%2FPython%20Collection%E9%9B%86%E5%90%88%E6%A8%A1%E5%9D%97%2F</url>
    <content type="text"><![CDATA[1import collections collections是Python内建的一个集合模块，提供了许多有用的集合类。Python拥有一些内置的数据类型，比如str, int, list, tuple, dict等， collections模块在这些内置数据类型的基础上，提供了几个额外的数据类型： namedtuple(): 生成可以使用名字来访问元素内容的tuple子类 deque: 双端队列，可以快速的从另外一侧追加和推出对象 Counter: 计数器，主要用来计数 OrderedDict: 有序字典 defaultdict: 带有默认值的字典 namedtuplenamedtuple主要用来产生可以使用名称来访问元素的数据对象，通常用来增强代码的可读性。 namedtuple是一个函数，它用来创建一个自定义的tuple对象，并且规定了tuple元素的个数，并可以用属性而不是索引来引用tuple的某个元素。 1234567891011from collections import namedtuple# namedtuple('名称', [属性list])Point = namedtuple('Point', ['x', 'y'])p = Point(1, 2)print p.x, p.y# 1, 2isinstance(p, Point)# Trueisinstance(p, tuple)# True 用namedtuple可以很方便地定义一种数据类型，它具备tuple的不变性，又可以根据属性来引用 dequedeque其实是double-ended queue的缩写，翻译过来就是双端队列，它最大的好处就是实现了从队列头部快速增加和取出对象: .popleft(), .appendleft() list对象的这两种用法的时间复杂度是 O(n) ，也就是说随着元素数量的增加耗时呈 线性上升。而使用deque对象则是 O(1) 的复杂度 123456&gt;&gt;&gt; from collections import deque&gt;&gt;&gt; q = deque(['a', 'b', 'c'])&gt;&gt;&gt; q.append('x')&gt;&gt;&gt; q.appendleft('y')&gt;&gt;&gt; qdeque(['y', 'a', 'b', 'c', 'x']) deque提供了很多方法，例如 rotate 123456789101112131415161718192021# -*- coding: utf-8 -*-"""下面这个是一个有趣的例子，主要使用了deque的rotate方法来实现了一个无限循环的加载动画"""import sysimport timefrom collections import dequefancy_loading = deque('&gt;--------------------')while True: print '\r%s' % ''.join(fancy_loading), fancy_loading.rotate(1) sys.stdout.flush() time.sleep(0.08)# Result:# 一个无尽循环的跑马灯# -------------&gt;------- CounterCounter是一个简单的计数器，例如，统计字符出现的个数 12345678910&gt;&gt;&gt; from collections import Counter&gt;&gt;&gt; c = Counter()&gt;&gt;&gt; for ch in 'programming':... c[ch] = c[ch] + 1...&gt;&gt;&gt; cCounter(&#123;'g': 2, 'm': 2, 'r': 2, 'a': 1, 'i': 1, 'o': 1, 'n': 1, 'p': 1&#125;)&gt;&gt;&gt; # 获取出现频率最高的5个字符&gt;&gt;&gt; print c.most_common(5) Counter实际上也是dict的一个子类，上面的结果可以看出，字符’g’、’m’、’r’各出现了两次，其他字符各出现了一次。 defaultdict使用dict时，如果引用的Key不存在，就会抛出KeyError。如果希望key不存在时，返回一个默认值，就可以用defaultdict : 如果使用defaultdict，只要你传入一个默认的工厂方法，那么请求一个不存在的key时， 便会调用这个工厂方法使用其结果来作为这个key的默认值 1234567&gt;&gt;&gt; from collections import defaultdict&gt;&gt;&gt; dd = defaultdict(lambda: 'N/A')&gt;&gt;&gt; dd['key1'] = 'abc'&gt;&gt;&gt; dd['key1'] # key1存在'abc'&gt;&gt;&gt; dd['key2'] # key2不存在，返回默认值'N/A' OrderedDict使用dict时，Key是无序的。在对dict做迭代时，我们无法确定Key的顺序。 如果要保持Key的顺序，可以用OrderedDict 1234567&gt;&gt;&gt; from collections import OrderedDict&gt;&gt;&gt; d = dict([('a', 1), ('b', 2), ('c', 3)])&gt;&gt;&gt; d # dict的Key是无序的&#123;'a': 1, 'c': 3, 'b': 2&#125;&gt;&gt;&gt; od = OrderedDict([('a', 1), ('b', 2), ('c', 3)])&gt;&gt;&gt; od # OrderedDict的Key是有序的OrderedDict([('a', 1), ('b', 2), ('c', 3)]) Note: OrderedDict的Key会按照插入的顺序排列，不是Key本身排序]]></content>
      <tags>
        <tag>Python</tag>
        <tag>Data Structure</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SQL Server数据库分页]]></title>
    <url>%2F2017%2F05%2F27%2FSQL%20Server%E6%95%B0%E6%8D%AE%E5%BA%93%E5%88%86%E9%A1%B5%2F</url>
    <content type="text"><![CDATA[在编写Web应用程序等系统时，会涉及到与数据库的交互，如果数据库中数据量很大的话，一次检索所有的记录，会占用系统很大的资源，因此常常采用分页语句：需要多少数据就只从数据库中取多少条记录。 常见的对大数据量查询的解决方案有以下两种： 将全部数据先查询到内存中，然后在内存中进行分页，这种方式对内存占用较大，必须限制一次查询的数据量。 采用存储过程在数据库中进行分页，这种方式对数据库的依赖较大，不同的数据库实现机制不通，并且查询效率不够理想。以上两种方式对用户来说都不够友好。 使用ROW_NUMBER()函数分页SQL Server 2005之后引入了 ROW_NUMBER() 函数，通过该函数根据定好的排序字段规则，产生记录序号 123SELECT ROW_NUMBER() OVER ( ORDER BY dbo.Products.ProductID DESC ) AS rownum , *FROM dbo.Products 12345678SELECT *FROM ( SELECT TOP ( @pageSize * @pageIndex ) ROW_NUMBER() OVER ( ORDER BY dbo.Products.UnitPrice DESC ) AS rownum , * FROM dbo.Products ) AS tempWHERE temp.rownum &gt; ( @pageSize * ( @pageIndex - 1 ) )ORDER BY temp.UnitPrice 使用OFFSET FETCH子句分页SQL Server 2012中引入了OFFSET-FETCH语句，可以通过使用OFFSET-FETCH过滤器来实现分页 12345SELECT * FROM dbo.Products ORDER BY UnitPrice DESC OFFSET ( @pageSize * ( @pageIndex - 1 )) ROWS FETCH NEXT @pageSize ROWS ONLY;]]></content>
      <tags>
        <tag>Database</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[二叉树的遍历]]></title>
    <url>%2F2017%2F05%2F27%2F%E4%BA%8C%E5%8F%89%E6%A0%91%E7%9A%84%E9%81%8D%E5%8E%86%2F</url>
    <content type="text"><![CDATA[二叉树的遍历分为： 深度优先搜索(Depth First Search) 是沿着树的深度遍历树的节点，尽可能深的搜索树的分支。深度优先搜索二叉树是先访问根结点，然后遍历左子树接着是遍历右子树，因此我们可以利用堆栈的先进后出的特点，先将右子树压栈，再将左子树压栈，这样左子树就位于栈顶，可以保证结点的左子树先与右子树被遍历。 广度优先搜索(Breadth First Search) 是从根结点开始沿着树的宽度搜索遍历，可以利用队列实现广度优先搜索 二叉树的深度优先遍历的非递归的通用做法是采用栈，广度优先遍历的非递归的通用做法是采用队列 深度优先实现深度优先遍历又分为：前序、中序、后序遍历 前序遍历：根节点-&gt;左子树-&gt;右子树 中序遍历：左子树-&gt;根节点-&gt;右子树 后序遍历：左子树-&gt;右子树-&gt;根节点 note: 二叉搜索树BST的中序遍历，返回的结果是按顺序排列的 递归实现前序遍历伪代码： 123456preorder(node) if (node = null) return visit(node) preorder(node.left) preorder(node.right) 根节点-&gt;左子树-&gt;右子树 python实现 123456def preorder(self, node): """前序遍历""" if node: print node.data self.preorder(node.left) self.preorder(node.right) 中序遍历伪代码： 123456inorder(node) if (node = null) return inorder(node.left) visit(node) inorder(node.right) 左子树-&gt;根节点-&gt;右子树 123456def inorder(self, node): """中序遍历""" if node: self.inorder(node.left) print node.data self.inorder(node.right) 后序遍历伪代码： 123456postorder(node) if (node = null) return postorder(node.left) postorder(node.right) visit(node) 左子树-&gt;右子树-&gt;根节点 123456def postorder(self, node): """后序遍历""" if node: self.postorder(node.left) self.postorder(node.right) print node.data 非递归实现因为当遍历过根节点之后还要回来，所以必须将其存起来。考虑到后进先出的特点，选用栈存储。 前序遍历伪代码： 123456789iterativePreorder(node) parentStack = empty stack while (not parentStack.isEmpty() or node ≠ null) if (node ≠ null) visit(node) if (node.right ≠ null) parentStack.push(node.right) node = node.left else node = parentStack.pop() 123456789101112131415def preorderTraversal__iterative(root): """ :type root: TreeNode """ node = root stack = [] while node or stack: if node: print node.val if node.right: stack.append(node.right) node = node.left else: node = stack.pop() return 中序遍历伪代码： 12345678910iterativeInorder(node) s ← empty stack while (not s.isEmpty() or node ≠ null) while (node ≠ null) s.push(node) node ← node.left else node ← s.pop() visit(node) node ← node.right 1234567891011121314def inorderTraversal_iterative(root): """ :type root: TreeNode """ node = root stack = [] while node or stack: while node: stack.append(node) node = node.left node = stack.pop() print node.val node = node.right return result 后序遍历 后序遍历伪代码： 12345678910111213141516iterativePostorder(node) s ← empty stack lastNodeVisited ← null while (not s.isEmpty() or node ≠ null) if (node ≠ null) s.push(node) node ← node.left else peekNode ← s.peek() // if right child exists and traversing node // from left child, then move right if (peekNode.right ≠ null and lastNodeVisited ≠ peekNode.right) node ← peekNode.right else visit(peekNode) lastNodeVisited ← s.pop() 123456789101112131415161718def postorderTraversal(node): if node is None: return [] stack = [] result = [] lastNodeVisited = None while stack or node: if node: stack.append(node) node = node.left else: peekNode = stack[-1] if peekNode.right and lastNodeVisited != peekNode.right: node = peekNode.right else: result.append(peekNode) lastVisitedNode = stack.pop() return result 广度优先实现伪代码 12345678910levelorder(root) q ← empty queue q.enqueue(root) while (not q.isEmpty()) node ← q.dequeue() visit(node) if (node.left ≠ null) q.enqueue(node.left) if (node.right ≠ null) q.enqueue(node.right)]]></content>
      <tags>
        <tag>Data Structure</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python内置数据结构]]></title>
    <url>%2F2017%2F05%2F27%2FPython%E5%86%85%E7%BD%AE%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F</url>
    <content type="text"><![CDATA[Python 内置数据类型包括 list, tuple, dict, set ListPython内置的一种数据类型是列表：list。list是一种有序的集合，可以随时添加和删除其中的元素 列表常用操作及其复杂度 Operation big O description index [] O(1) 索引操作 index assignment O(1) 索引赋值操作 append O(1) 在列表末尾添加新的对象 pop() O(1) 移除列表中的一个元素（默认最后一个元素），并且返回该元素的值 pop(i) O(n) 移除列表中索引位置的值，并且返回该元素的值 insert(i,item) O(n) 将对象插入列表索引i位置 del operator O(n) 删除列表的的元素 iteration O(n) contains (in) O(n) get slice [x:y] O(k) del slice O(n) set slice O(n+k) reverse O(n) 反向列表中元素 remove O(n) 移除列表中某个值的第一个匹配项 concatenate O(k) sort O(n log n) 列表排序 multiply O(nk) DictionaryPython内置了字典：dict的支持，dict全称dictionary，在其他语言中也称为map，使用键-值（key-value）存储，具有极快的查找速度。dict内部存放的顺序和key放入的顺序是没有关系的。dict的key必须是不可变对象。这是因为dict根据key来计算value的存储位置，如果每次计算相同的key得出的结果不同，那dict内部就完全混乱了。这个通过key计算位置的算法称为哈希算法（Hash） dict: {&#39;A&#39;: 1, &#39;Z&#39;: -1} 创建方式 1234567a = dict(A=1, Z=-1)b = &#123;'A': 1, 'Z': -1&#125;c = dict(zip(['A', 'Z'], [1, -1]))d = dict([('A', 1), ('Z', -1)])e = dict(&#123;'Z': -1, 'A': 1&#125;)a == b == c == d == e# True 和list比较，dict有以下几个特点： 查找和插入的速度极快，不会随着key的增加而变慢； 需要占用大量的内存，内存浪费多。 而list相反： 查找和插入的时间随着元素的增加而增加； 占用空间小，浪费内存很少。 所以，dict是用空间来换取时间的一种方法 遍历一个dict，实际上是在遍历它的所有的Key的集合，然后用这个Key来获得对应的Value 12345678910111213d = &#123;'Adam': 95, 'Lisa': 85, 'Bart': 59, 'Paul': 75&#125;print d['Adam']# 95print d.get('Jason')# Nonefor key in d : print key, ':', d.get(key)# Lisa : 85# Paul : 75# Adam : 95# Bart : 59 Tupletuple和list非常类似，但是tuple一旦初始化就不能修改 Tuple 的不可变性元组一旦创建，它的元素就是不可变的， 例如如下： 1234t = ('a', 'b', ['A', 'B'])t[2][0] = 'X't[2][1] = 'Y'print t 当我们把list的元素’A’和’B’修改为’X’和’Y’后，tuple变为： Tuple的每个元素，指向永远不变，其中如果某个元素本身是可变的，那么元素内部也是可变的，但是元组的指向却是没有变化 Setset和dict类似，也是一组key的集合，但不存储value。由于key不能重复，所以，在set中，没有重复的key。 set可以看成数学意义上的无序和无重复元素的集合，因此，两个set可以做数学意义上的交集、并集等操作： 1234567s1 = set([1, 2, 3])s2 = set([2, 3, 4])s1 &amp; s2# set([2, 3])s1 | s2# set([1, 2, 3, 4]) set和dict的唯一区别仅在于没有存储对应的value，但是，set的原理和dict一样，所以，同样不可以放入可变对象，因为无法判断两个可变对象是否相等，也就无法保证set内部“不会有重复元素”。试试把list放入set，看看是否会报错。]]></content>
      <tags>
        <tag>Python</tag>
        <tag>Data Structure</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git 常用命令总结]]></title>
    <url>%2F2016%2F09%2F20%2Fgit%20%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[Git，目前主流的版本控制工具，git命令是一些命令行工具的集合，它可以用来跟踪，记录文件的变动。比如你可以进行保存，比对，分析，合并等等。 日常使用，一般记住一下6个命令就好了 Workspace：工作区 Index / Stage：暂存区 Repository：仓库区（或本地仓库） Remote：远程仓库 基本的git工作流 在工作目录中修改文件 暂存文件，将文件的快照放入暂存区域 提交更新，找到暂存区域的文件，将快照永久性存储到 Git 仓库目录 新建代码库12345678# 在当前目录新建一个Git代码库$ git init# 新建一个目录，将其初始化为Git代码库$ git init [project-name]# 下载一个项目和它的整个代码历史$ git clone [url] 配置Git的设置文件为 .gitconfig，它可以在用户主目录下（全局配置），也可以在项目目录下（项目配置）。 123456789# 显示当前的Git配置$ git config --list# 编辑Git配置文件$ git config -e [--global]# 设置提交代码时的用户信息$ git config [--global] user.name "[name]"$ git config [--global] user.email "[email address]" 增加/删除文件123456789101112131415161718192021# 添加指定文件到暂存区$ git add [file1] [file2] ...# 添加指定目录到暂存区，包括子目录$ git add [dir]# 添加当前目录的所有文件到暂存区$ git add .# 添加每个变化前，都会要求确认# 对于同一个文件的多处变化，可以实现分次提交$ git add -p# 删除工作区文件，并且将这次删除放入暂存区$ git rm [file1] [file2] ...# 停止追踪指定文件，但该文件会保留在工作区$ git rm --cached [file]# 改名文件，并且将这个改名放入暂存区$ git mv [file-original] [file-renamed] 提交版本现在我们已经添加了这些文件，然后我们将它们提交到仓库。 1$ git commit -m "Adding files" 如果您不使用-m，会出现编辑器来让你写自己的注释信息。当我们修改了很多文件，而不想每一个都add，想commit自动来提交本地修改，我们可以使用-a标识。 1$ git commit -a -m "Changed some files" git commit 命令的-a选项可将所有被修改或者已删除的且已经被git管理的文档提交到仓库中。 分支当你在做一个新功能的时候，最好是在一个独立的区域上开发，通常称之为分支。分支之间相互独立，并且拥有自己的历史记录。这样做的原因是： 稳定版本的代码不会被破坏 不同的功能可以由不同开发者同时开发 开发者可以专注于自己的分支，不用担心被其他人破坏了环境 在不确定之前，同一个特性可以拥有几个版本，便于比较 123456789101112131415161718192021222324252627282930313233343536373839404142# 列出所有本地分支$ git branch# 列出所有远程分支$ git branch -r# 列出所有本地分支和远程分支$ git branch -a# 新建一个分支，但依然停留在当前分支$ git branch [branch-name]# 新建一个分支，并切换到该分支$ git checkout -b [branch]# 新建一个分支，指向指定commit$ git branch [branch] [commit]# 新建一个分支，与指定的远程分支建立追踪关系$ git branch --track [branch] [remote-branch]# 切换到指定分支，并更新工作区$ git checkout [branch-name]# 切换到上一个分支$ git checkout -# 建立追踪关系，在现有分支与指定的远程分支之间$ git branch --set-upstream [branch] [remote-branch]# 合并指定分支到当前分支$ git merge [branch]# 选择一个commit，合并进当前分支$ git cherry-pick [commit]# 删除分支$ git branch -d [branch-name]# 删除远程分支$ git push origin --delete [branch-name]$ git branch -dr [remote/branch] 查看信息123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960# 显示有变更的文件$ git status# 显示当前分支的版本历史$ git log# 显示commit历史，以及每次commit发生变更的文件$ git log --stat# 搜索提交历史，根据关键词$ git log -S [keyword]# 显示某个commit之后的所有变动，每个commit占据一行$ git log [tag] HEAD --pretty=format:%s# 显示某个commit之后的所有变动，其"提交说明"必须符合搜索条件$ git log [tag] HEAD --grep feature# 显示某个文件的版本历史，包括文件改名$ git log --follow [file]$ git whatchanged [file]# 显示指定文件相关的每一次diff$ git log -p [file]# 显示过去5次提交$ git log -5 --pretty --oneline# 显示所有提交过的用户，按提交次数排序$ git shortlog -sn# 显示指定文件是什么人在什么时间修改过$ git blame [file]# 显示暂存区和工作区的差异$ git diff# 显示暂存区和上一个commit的差异$ git diff --cached [file]# 显示工作区与当前分支最新commit之间的差异$ git diff HEAD# 显示两次提交之间的差异$ git diff [first-branch]...[second-branch]# 显示今天你写了多少行代码$ git diff --shortstat "@&#123;0 day ago&#125;"# 显示某次提交的元数据和内容变化$ git show [commit]# 显示某次提交发生变化的文件$ git show --name-only [commit]# 显示某次提交时，某个文件的内容$ git show [commit]:[filename]# 显示当前分支的最近几次提交$ git reflog 远程同步1234567891011121314151617181920212223# 下载远程仓库的所有变动$ git fetch [remote]# 显示所有远程仓库$ git remote -v# 显示某个远程仓库的信息$ git remote show [remote]# 增加一个新的远程仓库，并命名$ git remote add [shortname] [url]# 取回远程仓库的变化，并与本地分支合并$ git pull [remote] [branch]# 上传本地指定分支到远程仓库$ git push [remote] [branch]# 强行推送当前分支到远程仓库，即使有冲突$ git push [remote] --force# 推送所有分支到远程仓库$ git push [remote] --all 撤销12345678910111213141516171819202122232425262728293031# 恢复暂存区的指定文件到工作区$ git checkout [file]# 恢复某个commit的指定文件到暂存区和工作区$ git checkout [commit] [file]# 恢复暂存区的所有文件到工作区$ git checkout .# 重置暂存区的指定文件，与上一次commit保持一致，但工作区不变$ git reset [file]# 重置暂存区与工作区，与上一次commit保持一致$ git reset --hard# 重置当前分支的指针为指定commit，同时重置暂存区，但工作区不变$ git reset [commit]# 重置当前分支的HEAD为指定commit，同时重置暂存区和工作区，与指定commit一致$ git reset --hard [commit]# 重置当前HEAD为指定commit，但保持暂存区和工作区不变$ git reset --keep [commit]# 新建一个commit，用来撤销指定commit# 后者的所有变化都将被前者抵消，并且应用到当前分支$ git revert [commit]# 暂时将未提交的变化移除，稍后再移入$ git stash$ git stash pop 转自阮一峰： http://www.ruanyifeng.com/blog/2015/12/git-cheat-sheet.html]]></content>
      <categories>
        <category>备忘</category>
      </categories>
      <tags>
        <tag>git</tag>
        <tag>github</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Scrapy编写爬虫]]></title>
    <url>%2F2016%2F09%2F18%2F%E4%BD%BF%E7%94%A8Scrapy%E7%BC%96%E5%86%99%E7%88%AC%E8%99%AB%2F</url>
    <content type="text"><![CDATA[Scrapy是Python开发的一个快速,高层次的屏幕抓取和web抓取框架，用于抓取web站点并从页面中提取结构化的数据。Scrapy用途广泛，可以用于数据挖掘、监测和自动化测试。 Scrapy Home Site 1pip install scrapy Scrapy 处理流程图借个图简单介绍下Scrapy处理的流程(这就是框架，帮我们完成了大部分工作) 引擎(Scrapy Engine)，用来处理整个系统的数据流处理，触发事务。 调度器(Scheduler)，用来接受引擎发过来的请求，压入队列中，并在引擎再次请求的时候返回。 下载器(Downloader)，用于下载网页内容，并将网页内容返回给蜘蛛。 蜘蛛(Spiders)，蜘蛛是主要干活的，用它来制订特定域名或网页的解析规则。编写用于分析response并提取item(即获取到的item)或额外跟进的URL的类。 每个spider负责处理一个特定(或一些)网站。 项目管道(Item Pipeline)，负责处理有蜘蛛从网页中抽取的项目，他的主要任务是清晰、验证和存储数据。当页面被蜘蛛解析后，将被发送到项目管道，并经过几个特定的次序处理数据。 下载器中间件(Downloader Middlewares)，位于Scrapy引擎和下载器之间的钩子框架，主要是处理Scrapy引擎与下载器之间的请求及响应。 蜘蛛中间件(Spider Middlewares)，介于Scrapy引擎和蜘蛛之间的钩子框架，主要工作是处理蜘蛛的响应输入和请求输出。 调度中间件(Scheduler Middlewares)，介于Scrapy引擎和调度之间的中间件，从Scrapy引擎发送到调度的请求和响应。 接下来简单介绍一下爬虫的写法，从开发爬虫到部署，把爬取的数据写入mongodb: 创建新的Scrapy工程 定义要抽取的Item对象 编写一个Spider来爬取某个网站并提取出Item对象 编写Item Pipeline来存储提取出来的Item对象 使用Scrapyd-client部署Spider项目 把Aqicn上的北京空气质量的数据爬取下来，为日后分析做准备 创建Scrapy工程在目录下执行命令 1scrapy startproject projectName 创建如下的项目文件夹，目录结构如下 定义Item对象创建一个scrapy.Item类，将所要爬取的字段定义好 12345678910111213141516171819202122import scrapyclass AirqualityItem(scrapy.Item): date = scrapy.Field() hour = scrapy.Field() city = scrapy.Field() # area = scrapy.Field() aqivalue = scrapy.Field() aqilevel = scrapy.Field() pm2_5 = scrapy.Field() pm10 = scrapy.Field() co = scrapy.Field() no2 = scrapy.Field() o3 = scrapy.Field() so2 = scrapy.Field() temp = scrapy.Field() dew = scrapy.Field() pressure = scrapy.Field() humidity = scrapy.Field() wind = scrapy.Field() # add field to log spider crawl time crawl_time = scrapy.Field() 编写SpiderSpider类里面定义如何从一个domain组中爬取数据，包括：初始化url列表、如何跟踪url和如何解析页面提取Item，定义一个Spider，需要继承scrapy.Spider类 name: 定义Spider的名称，以后调用爬虫应用时候使用; start_url: 初始化url; parse(): 解析下载后的Response对象，解析并返回页面数据并提取出相应的Item对象 抽取Item对象内容Scrapy Selector是Scrapy提供的一套选择器，通过特定的XPath或者CSS表达式来选择HTML文件中某个部分 (note: Chrome浏览器自带的copy XPath或者CSS功能非常好用)，在开发过程中，可以使用Scrapy内置的Scrapy-Shell来debug选择器。 爬虫的代码如下 123456789101112131415161718192021222324252627282930313233class AirQualitySpider(CrawlSpider): name = "AqiSpider" download_delay = 2 allowed_domains = ['aqicn.org'] start_urls = ['http://aqicn.org/city/beijing/en/'] def parse(self, response): sel = Selector(response) pm25 = int(sel.xpath('//*[@id="cur_pm25"]/text()').extract()[0]) pm10 = int(sel.xpath('//*[@id="cur_pm10"]/text()').extract()[0]) o3 = int(sel.xpath('//*[@id="cur_o3"]/text()').extract()[0]) no2 = int(sel.xpath('//*[@id="cur_no2"]/text()').extract()[0]) so2 = int(sel.xpath('//*[@id="cur_so2"]/text()').extract()[0]) co = int(sel.xpath('//*[@id="cur_co"]/text()').extract()[0]) item = AirqualityItem() item['date'] = updatetime.strftime("%Y%m%d") item['hour'] = updatetime.hour # strftime("%H%M%S") item['city'] = city item['aqivalue'] = aqivalue item['aqilevel'] = aqilevel item['pm2_5'] = pm25 item['pm10'] = pm10 item['co'] = co item['no2'] = no2 item['o3'] = o3 item['so2'] = so2 item['temp'] = temp item['dew'] = dew item['pressure'] = pressure item['humidity'] = humidity item['wind'] = wind item['crawl_time'] = cur_time yield item 数据存储到MongoDB在Item已经被爬虫抓取之后，Item被发送到Item Pipeline去做更复杂的处理，比如存储到文件中或者数据库中。Item Pipeline常见的用途如下 清洗抓取来的HTML数据 验证抓取来的数据 查询与去除重复数据 将Item存储到数据库中 12345678910111213141516class AirqualityPipeline(object): def __init__(self): connection = pymongo.MongoClient(settings['MONGODB_SERVER'], settings['MONGODB_PORT']) db = connection[settings['MONGODB_DB']] self.collection = db[settings['MONGODB_COLLECTION']] def process_item(self, item, spider): # save data into mongodb valid = True if not item: valid = False raise DropItem("Missing &#123;0&#125;".format(item)) if valid: self.collection.insert(dict(item)) log.msg("an aqi data added to MongoDB database!", level=log.DEBUG, spider=spider) return item 接下来需要在setting.py文件中配置Item Pipeline与数据库信息 12345678ITEM_PIPELINES = &#123; 'AirQuality.pipelines.AirqualityPipeline': 300,&#125;MONGODB_SERVER = "localhost"MONGODB_PORT = 27017MONGODB_DB = "aqihistoricaldata"MONGODB_COLLECTION = "aqidata" 到此，简单的爬虫就已经写好了，可以使用以下命令来抓取相关页面来测试一下这个爬虫 1scrapy crawl AqiSpider 其中，AqiSpider就是在Spider程序中设置的Spider的name属性 防止爬虫被禁的几种方法很多网站都有反爬虫的机制，对于这些网站，可以采用以下的一些办法来绕开反爬虫机制： 使用User Agent池，每次发送请求的时候从池中选取不一样的浏览器头信息 禁止Cookie，有些网站会根据Cookie识别用户身份 设置dowload_delay，频繁请求数据肯定会被禁 使用Scrapyd和Scrapyd-client部署爬虫scrapyd是一个用于部署和运行scrapy爬虫的程序，它允许你通过JSON API来部署爬虫项目和控制爬虫运行。crapyd是一个守护进程，监听爬虫的运行和请求，然后启动进程来执行它们。 安装12pip install scrapydpip install scrapyd-client 启动服务1scrapyd 配置服务器信息编辑scrapy.cfg文件，添加如下内容 123[deploy:MySpider]url = http://localhost:6800/project = AirQuality 其中，MySpider是服务器名称， url是服务器地址 检查配置，列出当前可用的服务器 1scrapyd-deploy -l 部署Spider项目 1scrapyd-deploy MySpider -p AirQuality 部署完成后，在http://localhost:6800 可以看到如下的爬虫部署的监控信息 可以使用curl命令去调用爬虫，也可以使用contab命令来定时的去调用爬虫，来实现定时爬取的任务。 版权声明： 除非注明，本博文章均为原创，转载请以链接形式标明本文地址。]]></content>
      <categories>
        <category>Python</category>
        <category>框架</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何拍摄星空]]></title>
    <url>%2F2016%2F09%2F07%2F%E5%A6%82%E4%BD%95%E6%8B%8D%E6%91%84%E6%98%9F%E7%A9%BA%2F</url>
    <content type="text"><![CDATA[好久没写博客了，重新拾起来吧。今天说点技术无关的话题——摄影 我是一个纯业余的风光狗，向往美丽的大自然，喜欢仰望天空，把深邃的星空拍摄下来，以免子孙后代在严重的光污染中忘记了这篇美丽星空的存在。下面简单介绍一下如何拍星空： 来自知乎我的回答我曾经在西藏拍摄过星空，对于拍摄有一定的体会和经验，我说说拍摄方法吧： 首先，光污染问题。国内很多的地方，尤其是大城市，肉眼几乎看不到几颗星星，更不用说银河了。所以要拍银河星空的话，必须要到完全没有光污染的旷野，当然半夜在荒郊野岭拍照对于心理是个很大的挑战。 关于拍摄器材。要有个大光圈的广角镜头，最好用单反或者SONY的高级微单来拍，要的是机身的优秀高感也就是ISO能力；光圈的话，当然越大越好，因为外界环境很黑暗，所以需要长时间曝光，进光量一定要够；角度越广角越好，广角更能拍出宽阔感。推荐佳能的14L镜头，这个绝对是星空专用镜头。（一定要用三脚架，越稳定越好） 关于拍摄参数。M档，我的常用参数是，光圈f2.8，快门15-20秒，感光度ISO1600-6400关于对焦。我来纠正一下很多人误传的一条知识：手动对焦值无穷远再往回拧一点点。这是绝对的错误，这个一点点到底是多少？没人能说清。所以正确的对焦方法是：打开相机的实时取景功能LV，放大到10倍，然后看星点是否对上焦（也就是实心点不发虚），然后调整曝光参数，试拍两张，再调整感光度，达到满意为止。掌握这几点之后，你肯定能拍出肯漂亮的星空银河了，不过没有前景的话，照片就会很枯燥。所以找个漂亮的前景也很重要，因为使用大光圈，所以前景在构图时候不要离得太近，否则会虚掉。至于补光，我只能说多试几次。关于补光的话，不要一直开着手电筒对着前景照射，否则会强烈过曝，正确的方式是：在按下快门到曝光结束这段时间段的最后一秒，打开手电筒对着前景闪一下，这就足够了。 祝大家都能拍出漂亮的星空照附上在西藏拍的银河和泸沽湖拍的星空 西藏定日县城外小河沟拍的银河 云南泸沽湖边的星空 版权声明： 除非注明，本博文章均为原创，转载请以链接形式标明本文地址。]]></content>
      <tags>
        <tag>摄影</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[利用Github分支备份Hexo源文件]]></title>
    <url>%2F2016%2F05%2F31%2F%E5%88%A9%E7%94%A8Github%E5%88%86%E6%94%AF%E5%A4%87%E4%BB%BDHexo%E6%BA%90%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[Hexo是一款基于Node.js的静态博客框架，前一阵俺的笔记本泡水直接退役，但是博客的原文件还在那台死去的机器上，所以备份啊。。。本质上，Hexo是将本地的md文件编译成静态文件上传到github上（或者其他），所以建议是将本地的整个Hexo项目（blog）原件同步提交到github或者其他代码托管的站点。 hexo deploy 生成的静态网址默认是保存在your_blog_name/your_blog_name.github.io的master分支，可以在原有的基础上增加一个hexo_source分支保存网址原始数据，并将这个分支设置为默认分支，这样每次恢复和迁移文件时候只需要git clone即可获取迁移的文件了。 下面记录一下备份、以及在另外的电脑上恢复博客的过程，为了以后备查。 备份过程 本地克隆github.io的远程仓库 1git clone https://github.com/your_blog_name/your_blog_name.github.io.git 创建新的远程分支，用来备份hexo源文件 12git checkout -b hexo_sourcegit push origin hexo_source:hexo_source git checkout -b hexo_source hexo_source,并切换到该分支,等同于git branch hexo_source; git checkout hexo_source; git push origin hexo_source:hexo_source 提交本地新建分支hexo_source到远程服务器hexo_source分支(origin是默认远程主机名); git push origin :hexo_source或者 git push origin --delete hexo_source 删除远程分支hexo_source; git branch 可以查看当前分支；git branch -a 可以查看所有分支(包括远程分支)； 创建忽略规则文件 .gitignore 1vi .gitignore 按需添加如下内容： 12345678.DS_StoreThumbs.dbdb.json *.log.deploy*/node_modules/.npmignorepublic/ 上面最后一行 public 目录，因其已被 hexo 插件同步到 master 分支里，因此不需要再同步，deploy 是 hexo 的 git 配置存放目录，也不需要同步。其他内容可选择忽略也可以选择同步。 建议把Hexo博客目录下_config.yml文件复制粘贴一份，并重命名为hexo_config.yml；把themes目录下你用到主题目录下的_config.yml文件也复制一份，并粘贴到博客根目录，并命名为theme_config.yml 添加内容到仓库并提交到远程仓库 1234git add .git commit -m "first commit"git remote add origin git@github.com/your_blog_name/your_blog_name.github.io.git # 后面仓库目录改成自己新建的。git push -u origin hexo_source 按照以上的步骤就进行了 hexo 源文件的初次备份。以后每次修改了内容之后，都可通过以下几条命令实现同步。 123git add .git commit -m "..." # 双引号内填写更新内容git push origin hexo_source # 或者 git push 通过 git submodule 来同步第三方主题我们一般会选择第三方主题的仓库直接git clone下来。这是一个非常不好的习惯，正确做法是：Fork该第三方主题仓库，这样就会在自己账号下生成一个同名的仓库，并对应一个url，我们应该git clone自己账号下的url。 这样做的原因是：我们很有可能在原来主题基础上做一些自定义的小改动，为了保持多终端的同步，我们需要将这些改动提交到远程仓库。而第三方仓库我们是无法直接push的。 这样就会出现git仓库的嵌套问题，我们通过git submodule来解决这个问题。 1git submodule add git@github.com:lvraikkonen/hexo-theme-next.git themes/next 我们修改主题后: 12git commit -am "refine themes"git push origin hexo_source 然后就完成了第三方主题的备份 在其他电脑同步源文件时，需要执行如下命令来同步主题 12git submodule init // 这句很重要git submodule update 新机器同步现在需要在一台新的电脑上写博客，首先先安装好node、git、ssh、hexo，创建好hexo文件夹，安装好插件，(选做：将hexo生成的文件及文件夹删除) 12345npm install hexo-cli -ghexo init blog_foldercd blog_foldernpm installhexo server 在该文件夹(blog_folder)下 1234git init # 初始化git仓库git remote add origin &lt;server&gt; # 为本地仓库添加远程仓库git fetch --allgit reset --hard origin/hexo_source # 获取hexo_source分支源文件 然后就是写博客，在新电脑发布完博客之后(hexo deploy)，记得将博客备份同步到远程仓库 123git add .git commit -m "写了一篇博客"git push origin hexo_source 至此，已经完成了博客的撰写并修改了远端仓库的博客源文件，然后使用hexo g和hexo d更新博客就OK啦！ 新机器安装npm失败解决方案由于众所周知的原因，好多东西无法安装，可以添加第三方源来解决 12345678# 添加淘宝源npm install -g cnpm --registry=https://registry.npm.taobao.org# nrm类似包管理器cnpm install nrm -gnrm ls# 使用淘宝nrm use taobaonpm install -g hexo-cli 参考 关于博客同步的解决办法 使用Git Submodule管理子模块]]></content>
      <tags>
        <tag>hexo</tag>
        <tag>备忘</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用d3.js绘图]]></title>
    <url>%2F2015%2F11%2F28%2F%E4%BD%BF%E7%94%A8d3.js%E7%BB%98%E5%9B%BE%2F</url>
    <content type="text"><![CDATA[D3的全称是（Data-Driven Documents），是一个Javascript的函数库，主要用途是用HTML和SVG展现数据。下面简单回顾一下我从0出发把csv文件画在HTML页面上的过程。 0. 引入d3.js库引入js库可以直接引用网站上host的js库，也可以下载到本地folder下引入 1&lt;script src="http://d3js.org/d3.v3.min.js" charset="utf-8"&gt;&lt;/script&gt; 1. 创建SCG画布在 SVG 画布的预定义元素里，有六种基本图形： 矩形 圆形 椭圆 线段 折线 多边形 另外，还有一种比较特殊，也是功能最强的元素： 路径 画布中的所有图形，都是由以上七种元素组成。在绘制数据图表的时候，都是操作这几种图形元素。 123456789var margin = &#123;top: 20, right: 20, bottom: 30, left: 50&#125;, width = 960 - margin.left - margin.right, height = 500 - margin.top - margin.bottom;var svg = d3.select("body").append("svg") .attr("width", width + margin.left + margin.right) .attr("height", height + margin.top + margin.bottom) .append("g") .attr("transform", "translate(" + margin.left + "," + margin.top + ")"); 上面代码的意思是，选取HTML代码中的body元素，再后面添加svg画布元素，然后设置宽度高度等属性，svg的g元素类似于div，在这里作为一组元素的容器，后面加入的元素都放在g里面，g可以设置统一的css，里面的子元素会继承可继承css属性。margin和position对g的定位不起作用，只能使用translate通过位移来定位。 2. 定义比例尺对于画布或者图形的长度，不可能全部写死，需要通过数据的大小关系来动态确定，参考地图的比例尺。d3.js中，比例尺需要定义定义域和值域两个属性 有线性比例尺 d3.scale.linear() 和序数比例尺 d3.scale.ordinal() ，线性比例尺针对连续的定义域和值域，序数比例尺针对离散的。 12var xScale = d3.time.scale().range([0, width]);var yScale = d3.scale.linear().range([height, 0]); 这里先定义比例尺的值域，由于定义域需要根据数据来确定，所以写到了后面读取数据的部分。 3. 定义坐标轴d3.js中的坐标轴由 d3.svg.axis() 来实现，svg的坐标原点是左上角，向右为正，向下为正。 123456var xAxis = d3.svg.axis() .scale(xScale) .orient("bottom");var yAxis = d3.svg.axis() .scale(yScale) .orient("left"); x轴是日期，这里使用d3.time在时间和字符串之间做转换。y轴使用普通的线性缩放坐标轴。 4. 读取数据与绑定数据d3.js 中自带了读取csv、json等文件的方法。 123d3.json("data.json", function(error, json)&#123; // process data&#125;; d3.js 中是通过以下两个函数来绑定数据的： datum()：绑定一个数据到选择集上 data()：绑定一个数组到选择集上，数组的各项值分别与选择集的各元素绑定 12345678var data = json;// format date field to datedata.forEach(function(d)&#123; d.date = new Date(d.date); d.close = d.close;&#125;);xScale.domain(d3.extent(data, function(d)&#123; return d.date;&#125;));yScale.domain(d3.extent(data, function(d)&#123; return d.close;&#125;)); 在这里，data数据是一个列表对象，需要对列表中每一条数据的字段数据类型进行定义，之后需要做的是上面提到的定义x轴y轴的比例尺的定义域的定义。 5. 画线图形的主题是一条线，需要添加 path 元素，path的属性决定了线的路径，下面方法定义线的路径属性。 1234567var line = d3.svg.line() .x(function(d) &#123; return xScale(d.date); &#125;) .y(function(d) &#123; return yScale(d.close); &#125;);svg.append("path") .datum(data) .attr("class", "line") .attr("d", function(d)&#123; return line(d);&#125;); 6. 添加坐标轴call() 函数，其参数是前面定义的坐标轴 axis 1234567891011121314// add axissvg.append("g") .attr("class", "x axis") .attr("transform", "translate(0," + height + ")") .call(xAxis);svg.append("g") .attr("class", "y axis") .call(yAxis) .append("text") .attr("transform", "rotate(-90)") .attr("y", 6) .attr("dy", ".71em") .style("text-anchor", "end") .text("Price ($)"); 7. 生成图表上面的js脚本写好了之后，理论上就可以生成折线图了。不过在本地调试中，发现报错：文件没有找到。这个是因为由于安全考虑，浏览器不允许js脚本访问本地文件，解决方法有两个： 在本地开启一个web service 修改浏览器属性，允许访问本地文件 由于以后是要在网站上展示数据，所以我是用Flask在后台开启一个web服务，把d3所需要的数据生成出来 123456789101112131415161718from flask import Flaskfrom flask import render_templateimport jsonimport pandas as pdapp = Flask(__name__)data_path = './sampleData'@app.route("/")def index(): return render_template("line_chart.html")@app.route('/data')def get_data(): with open(data_path + '/line_chart.tsv') as data_file: sample_data = pd.read_csv(data_file, sep='\t') return sample_data.to_json(orient='records') 这样，在本地运行Flask，就可以展现出折线图了。这大约就是d3.js数据可视化的基本过程。 版权声明： 除非注明，本博文章均为原创，转载请以链接形式标明本文地址。]]></content>
      <tags>
        <tag>d3.js</tag>
        <tag>visualization</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Understanding TF-IDF]]></title>
    <url>%2F2015%2F11%2F27%2F2015-11-27-understanding-tf-idf%2F</url>
    <content type="text"></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>Scikit-Learn</tag>
        <tag>Text Mining</tag>
        <tag>Feature Extraction</tag>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Bag of Words Model]]></title>
    <url>%2F2015%2F11%2F26%2F2015-11-26-bag-of-words-model%2F</url>
    <content type="text"></content>
      <categories>
        <category>算法</category>
        <category>Kaggle</category>
      </categories>
      <tags>
        <tag>Scikit-Learn</tag>
        <tag>Kaggle</tag>
        <tag>Text Mining</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Cross-Validation in Scikit-Learn]]></title>
    <url>%2F2015%2F09%2F21%2F2015-09-21-cross-validation-in-scikit-learn%2F</url>
    <content type="text"></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>Python</tag>
        <tag>Scikit-Learn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[About Regularization]]></title>
    <url>%2F2015%2F09%2F18%2F2015-09-18-about-regularization%2F</url>
    <content type="text"><![CDATA[正则化防止过拟合]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>Scikit-Learn</tag>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Logistic Regression原理以及应用]]></title>
    <url>%2F2015%2F08%2F19%2F2015-08-12-logistic-regression-classifier-on-hands%2F</url>
    <content type="text"><![CDATA[逻辑回归算法是一个很有用的分类算法，这篇文章总结一下逻辑回归算法的相关内容。数据使用scikit-learn自带的Iris数据集。 Iris datasetIris数据集，里面包含3种鸢尾花品种的4各属性，这个分类问题可以描述成使用鸢尾花的属性，来判断这个品种倒地属于哪个品种类别。为了简单，这里使用两个类别：Setosa和Versicolor，两个属性：Length和Width 12345678910111213from sklearn import datasetsimport matplotlib.pyplot as pltimport pandas as pdimport numpy as npdata = datasets.load_iris()X = data.data[:100, : 2]y = data.target[:100]setosa = plt.scatter(X[:50, 0], X[:50, 1], c='b')versicolor = plt.scatter(X[50:, 0], X[50:, 1], c='r')plt.xlabel("Sepal Length")plt.ylabel("Seqal Width")plt.legend((setosa, versicolor), ("Setosa", "Versicolor")) 可以看出来，两个品种可以被区分开，接下来要使用一种算法，让计算机把这两个类别区分开。可以想象，可以使用线性回归，也就是画一条线来把两个类别分开，但是这种分割很粗暴，准确性也不高，所以接下来要使用的算法要使用概率的方法区分两个类别，比如，算法返回0.9，那么代表属于类别A的概率是90% 逻辑函数 Logistic Function这里使用的逻辑函数正好符合概率的定义，即函数返回值在[0, 1]区间内，函数又被称作sigmod函数： $$y=\dfrac {1} {1+e^{-x}}$$ 12345x_values = np.linspace(-5, 5, 100)y_values = [1 / (1 + np.exp(-x)) for x in x_values]plt.plot(x_values, y_values)plt.xlabel("X")plt.ylabel("y") 将逻辑函数应用到数据上现在，数据集有两个属性Sepal Length和Sepal Width，这两个属性可以写到如下的等式中： $$x=\theta_{0}+\theta_{1}SW +\theta_{2}SL$$ SL代表Sepal Length这个特征，SW代表Sepal Width这个特征，假如神告诉我们 $\theta_{0} = 1$，$\theta_{1} = 2$，$\theta_{2} = 4$，那么，长度为5并且宽度为3.5的这个品种，$x=1+\left( 2\ast 3.5\right) +\left( 4\ast 5\right) = 28 $，代入逻辑函数： $$\dfrac{1} {1+e^{-28}}=0.99$$ 说明这个品种数据Setosa的概率为99%。那么，告诉我们 $\theta$的取值的神是谁呢？ 算法学习Cost Function在学习线性回归时候，当时使用的是Square Error作为损失函数，那么在逻辑回归中能不能也用这种损失函数呢？当然可以，不过在逻辑回归算法中，使用Square Error作为损失函数是非凸函数，也就是说有多个局部最小值，不能取到全局最小值，所以这里应该使用其他的损失函数。 想象一下，我们假设求出来一个属性的结果值是1，也就是预测为Setosa类别，那么预测为Versicolor类别的概率为0，在全部的数据集上，假设数据都是独立分布的，那么我们的目标就是：把每个单独类别的概率结果值累乘起来，并求最大值： $$\prod_{Setosa}\frac{1}{1 + e^{-(\theta_{0} + \theta{1}SW + \theta_{2}SL)}}\prod_{Versicolor}1 - \frac{1}{1 + e^{-(\theta_{0} + \theta{1}SW + \theta_{2}SL)}}$$ 参考上面定义的逻辑函数： $$h(x) = \frac{1}{1 + e^{-x}}$$ 那么我们的目标函数就是求下面函数的最大值： $$\prod_{Setosa}h(x)\prod_{Versicolor}1 - h(x)$$ 解释一下，加入类别分别为0和1，回归结果$h_\theta(x)$表示样本属于类别1的概率，那么样本属于类别0的概率为 $1-h_\theta(x)$，则有 $$p(y=1|x,\theta)=h_\theta(x)$$ $$p(y=0|x,\theta)=1-h_\theta(x)$$ 可以写为下面公式，含义为：某一个观测值的概率 $$p(y|x,\theta)=h_\theta(x)^y(1-h_\theta(x))^{1-y}$$ 由于各个观测值相互独立，那么联合分布可以表示成各个观测值概率的乘积： $$L(\theta)=\prod_{i=1}^m{h_\theta(x^{(i)})^{y^{(i)}}(1-h_\theta(x^{(i)}))^{1-y^{(i)}}}$$ 上式称为n个观测的似然函数。我们的目标是能够求出使这一似然函数的值最大的参数估计。对上面的似然函数取对数 $$\begin{aligned} l(\theta)=log(L(\theta))=log(\prod_{i=1}^m{h_\theta(x^{(i)})^{y^{(i)}}(1-h_\theta(x^{(i)}))^{1-y^{(i)}}})=\sum_{i=1}^m{(y^{(i)}log(h_\theta(x^{(i)}))+(1-y^{(i)})log(1-h_\theta(x^{(i)})))} \end{aligned}$$ 最大化似然函数，使用梯度下降法，求出$\theta$值，稍微变换一下，那就是求下面式子的最小值 $$J\left( \theta \right) = -\sum_{i=1}^{m}y^{(i)}log(h(x^{(i)})) + (1-y^{(i)})log(1-h(x^{(i)}))$$ 梯度下降算法梯度下降算法为： $$\begin{aligned} \theta_j:=\theta_j-\alpha\frac{\partial J(\theta)}{\partial\theta_j} \end{aligned}$$ 梯度下降算法的推导对$\theta$参数求导，可得 $$\begin{aligned} \frac{\partial logh_\theta(x^{(i)})}{\partial\theta_j}&amp;=\frac{\partial log(g(\theta^T x^{(i)}))}{\partial\theta_j}\&amp;=\frac{1}{g(\theta^T x^{(i)})}{ g(\theta^T x^{(i)})) (1-g(\theta^T x^{(i)}))x_j^{(i)}}\&amp;=(1- g(\theta^T x^{(i)}))) x_j^{(i)}\&amp;=(1-h_\theta(x^{(i)}))x_j^{(i)} \end{aligned}$$ 同理可得， $$\begin{aligned} \frac{\partial(1-logh_\theta(x^{(i)}))}{\partial\theta_j}=-h_\theta(x^{(i)})x_j^{(i)} \end{aligned}$$ 所以 $$\begin{aligned} \frac{\partial l(\theta)}{\partial\theta_j}&amp;=\sum_{i=1}^m{(y^{(i)}(1-h_\theta(x^{(i)}))x_j^{(i)}+(1-y^{(i)})(-h_\theta(x^{(i)})x_j^{(i)}))}\&amp;=\sum_{i=1}^m{(y^{(i)}-h_\theta(x^{(i)}))x_j^{(i)}} \end{aligned}$$ 那么，最终梯度下降算法为： $$\begin{aligned} \theta_j:=\theta_j-\alpha\sum_{i=1}^m{(y^{(i)}-h_\theta(x^{(i)}))x_j^{(i)}} \end{aligned}$$ 注：虽然得到的梯度下降算法表面上看去与线性回归一样，但是这里 的 $h_{\theta }\left( x\right) =\dfrac {1} {1+e^{-\theta ^{T}x}}$ 与线性回归中不同。 梯度下降算法的技巧 变量缩放 (Normalize Variable) $\alpha$选择 设定收敛条件 实现Logigtic Regrssion算法以上，介绍了Logistic Regression算法的详细推导过程，下面就用Python来实现这个算法 首先是逻辑回归函数，也就是sigmoid函数 12def sigmoid(theta, x): return 1.0 / (1 + np.exp(-x.dot(theta))) 然后使用梯度下降算法估算$\theta$值，首先是gradient值 $$(y^{(i)}-h_\theta(x^{(i)}))x_j^{(i)}$$ 123def gradient(theta, x, y): first_part = sigmoid(theta, x) - np.squeeze(y) return first_part.T.dot(x) 损失函数cost function $$-\sum_{i=1}^{m}y^{(i)}log(h(x^{(i)})) + (1-y^{(i)})log(1-h(x^{(i)}))$$ 123456def cost_function(theta, x, y): h_theta = sigmoid(theta, x) y = np.squeeze(y) first = y * np.log(h_theta) second = (1 - y) * np.log(1 - h_theta) return np.mean(-first - second) 梯度下降算法，这种梯度下降算法也叫批量梯度下降(Batch gradient descent) $$\begin{aligned} \theta_j:=\theta_j-\alpha\sum_{i=1}^m{(y^{(i)}-h_\theta(x^{(i)}))x_j^{(i)}} \end{aligned}$$ 123456789101112def gradient_descent(theta, X, y, alpha=0.001, converge): X = (X - np.mean(X, axis=0)) / np.std(X, axis=0) cost_iter = [] cost = cost_function(theta, X, y) cost_iter.append([0, cost]) i = 1 while(change_cost &gt; converge): theta = theta - (alpha * gradient(theta, X, y)) cost = cost_function(theta, X, y) cost_iter.append([i, cost]) i += 1 return theta, np.array(cost_iter) 预测方法 12345def predict_function(theta, x): x = (x - np.mean(x, axis=0)) / np.std(x, axis=0) pred_prob = sigmoid(theta, x) pred_value = np.where(pred_prob &gt;= 0.5, 1, 0) return pred_value 损失函数变化趋势画出cost function的变化趋势，看看是不是已经收敛了 看来梯度下降算法已经收敛了。 使用Scikit-Learn中的Logistic Regression算法Scikit-Learn库中，已经包含了逻辑回归算法，下面用这个工具集来体验一下这个算法。 1234from sklearn import linear_modelmodel = linear_model.LogisticRegression()model.fit(X, y)model.predict(X_test) 其他优化算法 BFGS 随机梯度下降 Stochastic gradient descent L-BFGS Conjugate Gradient]]></content>
      <categories>
        <category>算法</category>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>Python</tag>
        <tag>Scikit-Learn</tag>
        <tag>Algorithm</tag>
        <tag>Logistic Regression</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[单变量线性回归]]></title>
    <url>%2F2015%2F08%2F13%2F2015-08-19-linear-regression-with-one-variable%2F</url>
    <content type="text"><![CDATA[1月份的时候，参加了Coursera上面Andrew Ng的Machine Learning课程，课程断断续续的学，没有透彻的理解、推导，再加上作业使用Octave完成，并且还是有模版的，不是从头到尾做出来的，所以效果很差，虽然拿到了完成证书，但是过后即忘。我觉得是时候从头学习一遍，并且用Python实现所有的作业内容了。 这里写个系列，就当作为这门课程的课程笔记。 机器学习的本质是首先将训练集“喂给”学习算法，进而学习到一个假设(Hypothesis)，然后将特征值作为输入变量输入给Hypothesis，得出输出结果。 线性回归先说一元线性回归，这里只有一个特征值x，Hypothesis可以写为 $$h_{\theta }\left( x\right) =\theta_{0}+\theta_{1}x$$ 代价函数 Cost Function现在假设已经有了这个假设，那么如何评价这个假设的准确性呢，这里用模型预测的值减去训练集中实际值来衡量，这个叫建模误差。线性回归的目标就是使建模误差最小化，从而找出能使建模误差最小化的模型参数。代价函数为： $$J(\theta) = \frac{1}{2m}\sum_{i=1}^m (h_\theta(x^{(i)})-y^{(i)})^2$$ 则目标为求出使得$J(\theta)$最小的$\theta$参数 梯度下降算法现在使用梯度下降算法来求出$\theta$参数，梯度下降算法的推导如下： $$\begin{aligned} \theta_j:=\theta_j-\alpha\frac{\partial J(\theta)}{\partial\theta_j} \end{aligned}$$ 对$\theta_0$的偏导数为： $$\begin{aligned} \frac{\partial}{\partial \theta_0}J(\theta_0, \theta_1) = \frac{1}{m} \sum_{i=1}^m (h_\theta(x^{(i)})-y^{(i)})\end{aligned}$$ 对$\theta_1$的偏导数为： $$\begin{aligned} \frac{\partial}{\partial \theta_1}J(\theta_0, \theta_1) = \frac{1}{m} \sum_{i=1}^m (h_\theta(x^{(i)})-y^{(i)})x^{(i)}\end{aligned}$$ 使用Python实现一元线性回归为了提高性能，使用 numpy 包来实现向量化计算， 12345678import numpy as npdef hypothesis(theta, x): return np.dot(x, theta)def cost_function(theta, x, y): loss = hypothesis(theta, x) - y return np.sum(loss ** 2) / (2 * len(y)) 实现梯度下降算法，需要计算下面四个部分： 计算假设Hypothesis 计算损失 loss = hypothesis - y，然后求出square root 计算Gradient = X’ * loss / m 更新参数theta -= alpha * gradient 1234567891011121314def gradient_descent(alpha, x, y, iters): # number of training dataset m = x.shape[0] theta = np.zeros(2) cost_iter = [] for iter in range(iters): h_theta = hypothesis(theta, x) loss = h_theta - y J = np.sum(loss ** 2) / (2 * m) cost_iter.append([iter, J]) # print "iter %s | J: %.3f" % (iter, J) gradient = np.dot(x.T, loss) / m theta -= alpha * gradient return theta, cost_iter 接下来造一些假数据试验一下 1234567from sklearn.datasets.samples_generator import make_regressionx, y = make_regression(n_samples=100, n_features=1, n_informative=1, random_state=0, noise=35)m, n = np.shape(x)x = np.c_[np.ones(m), x] ## add column value 1 as x0alpha = 0.01theta, cost_iter = gradient_descent(alpha, x, y, 1000)print theta 求出的参数值为[-2.8484052 , 43.202331] 将训练集数据和Hypothesis函数画出来 1234for i in range(x.shape[1]): y_predict = theta[0] + theta[1] * xplt.plot(x[:, 1], y, 'o')plt.plot(x, y_predict, 'k-') 接下来画出代价函数在每次迭代过程中的变化趋势，可以看出算法是否收敛 123plt.plot(cost_iter[:500, 0], cost_iter[:500, 1])plt.xlabel("Iteration Number")plt.ylabel("Cost") 接下来有必要好好复习一下线性代数、numpy和向量化计算。]]></content>
      <categories>
        <category>算法</category>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>Python</tag>
        <tag>Algorithm</tag>
        <tag>Coursera</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[利用Spark进行单词计数]]></title>
    <url>%2F2015%2F08%2F12%2F2015-08-12-word-count-example-in-spark%2F</url>
    <content type="text"><![CDATA[这里就不再介绍Spark了，这篇文章主要记录一下关于Spark的核心RDD的相关操作以及以单词计数这个简单的例子，描述一下Spark的处理流程。 Spark RDDSpark是以RDD概念为中心运行的。RDD是一个容错的、可以被并行操作的元素集合。创建一个RDD有两个方法：在你的驱动程序中并行化一个已经存在的集合；从外部存储系统中引用一个数据集，这个存储系统可以是一个共享文件系统，比如HDFS、HBase或任意提供了Hadoop输入格式的数据来源。 RDD支持两类操作： 转换(Transform) 动作(Action) 还是不翻译的好，下面都用英文描述。Transform：用于从已有的数据集转换产生新的数据集，Transform的操作是Lazy Evaluation的，也就是说这条语句过后，转换并没有发生，而是在下一个Action调用的时候才会返回结果。Action：用于计算结果并向驱动程序返回结果。 演示一下上面两种基本操作： 123lines = sc.textFile("data.txt")lineLength = line.map(lambda x: len(x))totalLength = lineLength.reduce(lambda x, y: x + y) 第一行是有外部存储系统中创建一个RDD对象，第二行定义map操作，是一个Transform操作，由于Lazy Evaluation，对象lineLength并没有立即计算得到。第三行，reduce是一个Action操作，这时，Spark将整个计算过程划分成许多任务在多台机器上并行执行，每台机器运行自己部分的map操作和reduce操作，最终将自己部分的运算结果返回给驱动程序。 12lineLength.persist()# lineLength.cache() 这一行，Spark将lineLength对象保存在内存中，以便后面计算中使用。Spark的一个重要功能就是在将数据集持久化（或缓存）到内存中以便在多个操作中重复使用。 以上就是RDD的一些基本操作，API文档中写的都很清楚，我就不多说了。 统计一篇文档中单词的个数首先，写一个函数，用来计算单词个数 12345def wordCount(wordListRDD): wordCountsCollected = wordListRDD .map(lambda x: (x, 1)) .reduceByKey(lambda x, y: x + y) return wordCountsCollected 使用正则表达式清理原始文本 123456import reimport stringdef removePunctuation(text): regex = re.compile('[%s]' % re.escape(string.punctuation)) return regex.sub('', text).lower().strip()print removePunctuation(' No under_score!') 去读文件内容到RDD中 1234567import os.pathbaseDir = os.path.join('data')inputPath = os.path.join('cs100', 'lab1', 'shakespeare.txt')fileName = os.path.join(baseDir, inputPath)# shakespeareRDD = (sc.textFile(fileName, 8).map(removePunctuation))print '\n'.join(shakespeareRDD.zipWithIndex().map(lambda (l, num): '&#123;0&#125;: &#123;1&#125;'.format(num,l)).take(15)) 这时候，需要把单词通过空格隔开，然后过滤掉为空的内容 1234shakespeareWordsRDD = shakespeareRDD.flatMap(lambda x: x.split())shakespeareWordCount = shakespeareWordsRDD.count()print shakespeareWordsRDD.top(5)shakeWordsRDD = shakespeareWordsRDD 统计出出现次数前15多的单词以及个数： 12top15WordAndCounts = wordCount(shakeWordsRDD).takeOrdered(15, key=lambda (k, v): -v)print '\n'.join(map(lambda (w, c): '&#123;0&#125;: &#123;1&#125;'.format(w, c), top15WordsAndCounts)) 输出结果为： word count the: 27361 and: 26028 i: 20681 to: 19150 of: 17463 a: 14593 you: 13615 my: 12481 in: 10956 that: 10890 is: 9134 not: 8497 with: 7771 me: 7769 it: 7678 Spark是用Scala写出来的，所以可想而知如果用Scala写的效率会比Python高一些，在这儿顺便贴一个Scala版写的WordCount： 123val wordCounts = textFile.flatMap(line =&gt; line.split(" ")) .map(word =&gt; (word, 1)) .reduceByKey((a, b) =&gt; a + b) 真是简洁，Spark真好，嘿嘿~]]></content>
      <categories>
        <category>Big Data</category>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Big Data</tag>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[朴素贝叶斯算法的一些细节和小技巧]]></title>
    <url>%2F2015%2F08%2F11%2F2015-08-11-some-note-about-naive-bayes%2F</url>
    <content type="text"><![CDATA[某特征属性的条件概率为0当特征属性为离散值时，只要统计训练样本中各个划分在每个类别中出现的频率即可用来估计P(a|y)，若某一特征值的概率为0则会使整个概率乘积变为0，这会让分类器的准确性大幅下降。 这时候使用Laplace校准：即假定训练数据库很大，以至于对每个计数加1造成的估计概率的变化忽略不计。 连续分布假定值服从高斯分布(正态分布)当特征属性为连续值时，通常假定其值服从高斯分布，即： $$p\left( x_{i}|y\right) =\dfrac {1} {\sqrt {2\pi \sigma_{y}^{2}}}exp\left( -\dfrac {\left( x_{i}-\mu_{y}\right) ^{2}} {2\sigma_{y}^{2}}\right)$$ 所以，对于连续分布的样本特征的训练就是计算其均值和方差 小数连续相乘实际项目中，概率P往往是值很小的小数，连续的微小小数相乘容易造成下溢出使乘积为0或者得不到正确答案。一种解决办法就是对乘积取自然对数，将连乘变为连加，$\ln \left( AB\right) =\ln A+\ln B$。采用自然对数处理不会带来任何损失，可以避免下溢出或者浮点数舍入导致的错误。]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>Algorithm</tag>
        <tag>Bayes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[配置Octopress支持LaTex数学公式]]></title>
    <url>%2F2015%2F08%2F08%2F2015-08-08-adding-support-for-math-formula%2F</url>
    <content type="text"><![CDATA[Octopress 默认不支持 LaTex 写数学公式需要更改配置才可以。 设置需要使用kramdown来支持LaTex写数学公式 用kramdown替换rdiscount 安装kramdown 1$ sudo gem install kramdown 修改_config.yml配置文件，将所有rdiscount替换成kramdown 修改Gemfile，把gem &#39;rdiscount&#39;换成gem &#39;kramdown&#39; 添加MathJax配置在/source/_includes/custom/head.html文件中，添加如下代码： 123456789101112131415&lt;!-- mathjax config similar to math.stackexchange --&gt;&lt;script type="text/x-mathjax-config"&gt;MathJax.Hub.Config(&#123; jax: ["input/TeX", "output/HTML-CSS"], tex2jax: &#123; inlineMath: [ ['$', '$'] ], displayMath: [ ['$$', '$$']], processEscapes: true, skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'] &#125;, messageStyle: "none", "HTML-CSS": &#123; preferredFont: "TeX", availableFonts: ["STIX","TeX"] &#125;&#125;);&lt;/script&gt;&lt;script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"&gt;&lt;/script&gt; 修复 MathJax 右击页面空白 bug修改~/sass/base/theme.scss文件，如下代码变为： 1234&gt; div#main &#123; background: $sidebar-bg $noise-bg; border-bottom: 1px solid $page-border-bottom; &gt; div &#123; 随之出现的问题，以及解决方法将rdiscount替换成kramdown之后，以前写的博客里面，很多内容都不能正确显示了，并且在rake generate时候，会报错，内容大约是：Error: Pygments can&#39;t parse unknown language: &lt;/p&gt; 原生的语法高亮插件Pygments很强大，支持语言也很多，但是这时候报的错误让人一头雾水。 找出原因报错部分代码在/plugins/pygments_code.rb文件中， 12345678910111213def self.pygments(code, lang) if defined?(PYGMENTS_CACHE_DIR) path = File.join(PYGMENTS_CACHE_DIR, "#&#123;lang&#125;-#&#123;Digest::MD5.hexdigest(code)&#125;.html") if File.exist?(path) highlighted_code = File.read(path) else begin highlighted_code = Pygments.highlight(code, :lexer =&gt; lang, :formatter =&gt; 'html', :options =&gt; &#123;:encoding =&gt; 'utf-8', :startinline =&gt; true&#125;) rescue MentosError raise "Pygments can't parse unknown language: #&#123;lang&#125;." end File.open(path, 'w') &#123;|f| f.print(highlighted_code) &#125; end 修改一下代码，将出问题的代码高亮部分抛出来， 1raise &quot;Pygments can&apos;t parse unknown language: #&#123;lang&#125;#&#123;code&#125;.&quot; Google了一下原因， 原来是因为最新版的pygments这个插件对于Markdown的书写要求更严格了： Some of my older blog posts did not contain a space between the triple-backtick characters and the name of the language being highlighted. Earlier versions of pygments did not care, but the current version is a stickler. pygments appears to want a blank line between any triple-backtick line and any other text in the blog post. 好吧，以后写文章要更细心一点了。:) 参考： Octopress中使用Latex写数学公式 Pygments Unknown Language]]></content>
      <categories>
        <category>备忘</category>
      </categories>
      <tags>
        <tag>Octopress</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[朴素贝叶斯分类器实践]]></title>
    <url>%2F2015%2F08%2F07%2F2015-08-07-naive-bayes-classifier%2F</url>
    <content type="text"><![CDATA[实际案例举个运动员的例子： 如果我问你Brittney Griner的运动项目是什么，她有6尺8寸高，207磅重，你会说“篮球”；我再问你对此分类的准确度有多少信心，你会回答“非常有信心”。我再问你Heather Zurich，6尺1寸高，重176磅，你可能就不能确定地说她是打篮球的了，至少不会像之前判定Brittney那样肯定。因为从Heather的身高体重来看她也有可能是跑马拉松的。最后，我再问你Yumiko Hara的运动项目，她5尺4寸高，95磅重，你也许会说她是跳体操的，但也不太敢肯定，因为有些马拉松运动员也是类似的身高体重。 ——选自A Programmer’s Guide to Data Mining 这里所说的分类，就用到了所谓的概率模型。 朴素贝叶斯算法朴素贝叶斯算法使用每个属性(特征)属于某个类的概率做出预测，这是一个监督性学习算法，对一个预测性问题进行概率建模。训练模型的过程可以看做是对条件概率的计算，何以计算每个类别的相应条件概率来估计分类结果。 这个算法基于一个假设：所有特征相互独立，任意特征的值和其他特征的值没有关联关系，这种假设在实际生活中几乎不存在，但是朴素贝叶斯算法在很多领域，尤其是自然语言处理领域很成功。其他的典型应用还有垃圾邮件过滤等等。 贝叶斯分类器的基本原理 图片引用自Matt Buck 贝叶斯定理给出贝叶斯定理： $$p\left( h|D\right) =\dfrac {p\left( D|h\right) .p\left( h\right) } {p\left( D\right) }$$ 这个公式是贝叶斯方法论的基石。拿分类问题来说，h代表分类的类别，D代表已知的特征d1, d2, d3…，朴素贝叶斯算法的朴素之处在于，假设了特征d1, d2, d3…相互独立，所以贝叶斯定理又能写成： $$p\left( h|f_{1},f_{2}…f_{n}\right) =\dfrac {p\left( h\right) \prod_{i=1}^n p\left( f_{i}|h\right) } {p\left( f_{1},f_{2}…f_{n}\right) }$$ 由于$P\left( f_{1},\ldots f_{n}\right) $ 可视作常数，类变量$h$的条件概率分布就可以表达为： $$p\left( h|f_{1},…,f_{n}\right) =\dfrac {1} {Z}.p\left( h\right) \prod _{i=1}^{n}p\left( F_{i}|h\right)$$ 从概率模型中构造分类器以上，就导出了独立分布的特征模型，也就是朴素贝叶斯模型，使用最大后验概率MAP(Maximum A Posteriori estimation)选出条件概率最大的那个分类，这就是朴素贝叶斯分类器： $$\widehat {y}=\arg \max_{y}p\left( y\right) \prod_{i=1}^{n}p\left( x_{i}|y\right) $$ 参数估计所有的模型参数都可以通过训练集的相关频率来估计。常用方法是概率的最大似然估计，类的先验概率可以通过假设各类等概率来计算（先验概率 = 1 / (类的数量)），或者通过训练集的各类样本出现的次数来估计（A类先验概率=（A类样本的数量）/(样本总数)）。为了估计特征的分布参数，我们要先假设训练集数据满足某种分布或者非参数模型。 常见的分布模型：高斯分布(Gaussian naive Bayes)、多项分布(Multinomial naive Bayes)、伯努利分布(Bernoulli naive Bayes)等. 使用Scikit-learn进行文本分类目的：使用Scikit-learn库自带的新闻信息数据来进行试验，该数据集有19,000个新闻信息组成，通过新闻文本的内容，使用scikit-learn中的朴素贝叶斯算法，来判断新闻属于什么主题类别。参考：Scikit-learn Totorial 数据集123from sklearn.datesets import fetch_20newsgroupsnews = fetch_20newsgroups(subset='all')print news.keys() 查看一下第一条新闻的内容和分组 [‘description’, ‘DESCR’, ‘filenames’, ‘target_names’, ‘data’, ‘target’] From: Mamatha Devineni Ratnam &#x6d;&#x72;&#x34;&#55;&#x2b;&#64;&#97;&#110;&#x64;&#x72;&#x65;&#x77;&#46;&#x63;&#109;&#117;&#46;&#x65;&#x64;&#117;Subject: Pens fans reactionsOrganization: Post Office, Carnegie Mellon, Pittsburgh, PA… 10 rec.sport.hockey 划分训练集和测试集，分为80%训练集，20%测试集 123456split_rate = 0.8split_size = int(len(news.data) * split_rate)X_train = news.data[:split_size]y_train = news.target[:split_size]X_test = news.data[split_size:]y_test = news.target[split_size:] 特征提取为了使机器学习算法应用在文本内容上，首先应该把文本内容装换为数字特征。这里使用词袋模型(Bags of words) 词袋模型在信息检索中，Bag of words model假定对于一个文本，忽略其词序和语法，句法，将其仅仅看做是一个词集合，或者说是词的一个组合，文本中每个词的出现都是独立的，不依赖于其他词是否出现，或者说当这篇文章的作者在任意一个位置选择一个词汇都不受前面句子的影响而独立选择的。 Scikit-learn提供了一些实用工具(sklearn.feature_extraction.text)可以从文本内容中提取数值特征 1234567891011from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizerfrom sklearn.feature_extraction.text import TfidfTransformer# Tokenizing textcount_vect = CountVectorizer()X_train_counts = count_vect.fit_transform(X_train)# Tftf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)X_train_tf = tf_transformer.transform(X_train_counts)# Tf_idftfidf_transformer = TfidfTransformer()X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts) 稀疏性大多数文档通常只会使用语料库中所有词的一个子集，因而产生的矩阵将有许多特征值是0（通常99%以上都是0）。例如，一组10,000个短文本（比如email）会使用100,000的词汇总量，而每个文档会使用100到1,000个唯一的词。为了能够在内存中存储这个矩阵，同时也提供矩阵/向量代数运算的速度，通常会使用稀疏表征例如在scipy.sparse包中提供的表征。 训练模型上面使用文本中词的出现次数作为数值特征，可以使用多项分布估计这个特征，使用sklearn.naive_bayes模块的MultinomialNB类来训练模型。 12345678910from sklearn.naive_bayes import MultinomialNB# create classifierclf = MultinomialNB().fit(X_train_tfidf, y_train)docs_new = ['God is love', 'OpenGL on the GPU is fast']X_new_counts = count_vect.transform(docs_new)X_new_tfidf = tfidf_transformer.transform(X_new_counts)# using classifier to predictpredicted = clf.predict(X_new_tfidf)for doc, category in zip(docs_new, predicted): print('%r =&gt; %s' % (doc, news.target_names[category])) 使用Pipline这个类构建复合分类器Scikit-learn为了使向量化 =&gt; 转换 =&gt; 分类这个过程更容易，提供了Pipeline类来构建复合分类器，例如： 1234from sklearn.pipeline import Pipelinetext_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),]) 创建新的训练模型 1234567891011121314151617from sklearn.naive_bayes import MultinomialNBfrom sklearn.pipeline import Pipelinefrom sklearn.feature_extraction.text import TfidfVectorizer, HashingVectorizer, CountVectorizer #nbc means naive bayes classifiernbc_1 = Pipeline([ ('vect', CountVectorizer()), ('clf', MultinomialNB()),])nbc_2 = Pipeline([ ('vect', HashingVectorizer(non_negative=True)), ('clf', MultinomialNB()),])nbc_3 = Pipeline([ ('vect', TfidfVectorizer()), ('clf', MultinomialNB()),])# classifiernbcs = [nbc_1, nbc_2, nbc_3] 交叉验证下面是一个交叉验证函数： 1234567891011from sklearn.cross_validation import cross_val_score, KFoldfrom scipy.stats import semimport numpy as np# cross validation functiondef evaluate_cross_validation(clf, X, y, K): # create a k-fold croos validation iterator of k folds cv = KFold(len(y), K, shuffle=True, random_state=0) # by default the score used is the one returned by score method of the estimator (accuracy) scores = cross_val_score(clf, X, y, cv=cv) print scores print ("Mean score: &#123;0:.3f&#125; (+/-&#123;1:.3f&#125;)").format(np.mean(scores), sem(scores)) 将训练集分为10份，输出验证分数： 12for nbc in nbcs: evaluate_cross_validation(nbc, X_train, y_train, 10) 结果为： CountVectorizer Mean score: 0.849 (+/-0.002) HashingVectorizer Mean score: 0.765 (+/-0.006) TfidfVectorizer Mean score: 0.848 (+/-0.004) 可以看出：CountVectorizer和TfidfVectorizer特征提取的方法要比HashingVectorizer效果好。 优化模型优化单词提取在使用TfidfVectorizer特征提取时候，使用正则表达式，默认的正则表达式是u&#39;(?u)\b\w\w+\b&#39;，使用新的正则表达式ur&quot;\b[a-z0-9_\-\.]+[a-z][a-z0-9_\-\.]+\b&quot; 1234567nbc_4 = Pipeline([ ('vect', TfidfVectorizer( token_pattern=ur"\b[a-z0-9_\-\.]+[a-z][a-z0-9_\-\.]+\b",) ), ('clf', MultinomialNB()),])evaluate_cross_validation(nbc_4, X_train, y_train, 10) 分数是：Mean score: 0.861 (+/-0.004) ，结果好了一点 排除停止词TfidfVectorizer的一个参数stop_words，这个参数指定的词将被省略不计入到标记词的列表中，这里使用鼎鼎有名的NLTK语料库。 1234567891011import nltk# nltk.download()stopwords = nltk.corpus.stopwords.words('english')nbc_5 = Pipeline([ ('vect', TfidfVectorizer( stop_words=stop_words, token_pattern=ur"\b[a-z0-9_\-\.]+[a-z][a-z0-9_\-\.]+\b", )), ('clf', MultinomialNB()),])evaluate_cross_validation(nbc_5, X_train, Y_train, 10) 分数是：Mean score: 0.879 (+/-0.003)，结果又提高了 调整贝叶斯分类器的alpha参数MultinomialNB有一个alpha参数，该参数是一个平滑参数，默认是1.0，我们将其设为0.01 12345678nbc_6 = Pipeline([ ('vect', TfidfVectorizer( stop_words=stopwords, token_pattern=ur"\b[a-z0-9_\-\.]+[a-z][a-z0-9_\-\.]+\b", )), ('clf', MultinomialNB(alpha=0.01)),])evaluate_cross_validation(nbc_6, X_train, y_train, 10) 分数为：Mean score: 0.917 (+/-0.002)，哎呦，好像不错哦！不过问题来了，调整参数优化不能靠蒙，如何寻找最好的参数，使得交叉验证的分数最高呢？ 使用Grid Search优化参数使用GridSearch寻找vectorizer词频统计, tfidftransformer特征变换和MultinomialNB classifier的最优参数 Scikit-learn上关于GridSearch的介绍 1234567891011121314151617pipeline = Pipeline([('vect',CountVectorizer()),('tfidf',TfidfTransformer()),('clf',MultinomialNB()),]);parameters = &#123; 'vect__max_df': (0.5, 0.75), 'vect__max_features': (None, 5000, 10000), 'tfidf__use_idf': (True, False), 'clf__alpha': (1, 0.1, 0.01, 0.001, 0.0001),&#125;grid_search = GridSearchCV(pipeline, parameters, n_jobs=1)from time import timet0 = time()grid_search.fit(X_train, y_train)print "done in %0.3fs" % (time() - t0)print "Best score: %0.3f" % grid_search.best_score_ 输出最优参数1234567891011from sklearn import metricsbest_parameters = dict()best_parameters = grid_search.best_estimator_.get_params()for param_name in sorted(parameters.keys()): print "\t%s: %r" % (param_name, best_parameters[param_name])pipeline.set_params(clf__alpha = 1e-05, tfidf__use_idf = True, vect__max_df = 0.5, vect__max_features = None)pipeline.fit(X_train, y_train)pred = pipeline.predict(X_test) 经过漫长的等待，终于找出了最优参数： done in 1578.965sBest score: 0.902 clf__alpha: 0.01tfidf__use_idf: Truevect__max_df: 0.5vect__max_features: None 在测试集上的准确率为：0.915，分类效果还是不错的 1print np.mean(pred == y_test) 评价分类效果在测试集上测试朴素贝叶斯分类器的分类效果 12345678from sklearn import metricsimport numpy as np#print X_test[0], y_test[0]for i in range(20): print str(i) + ": " + news.target_names[i]predicted = pipeline.fit(X_train, y_train).predict(X_test)print np.mean(predicted == y_test)print metrics.classification_report(y_test, predicted) 结果是这样的： id groupname 0 alt.atheism 1 comp.graphics 2 comp.os.ms-windows.misc 3 comp.sys.ibm.pc.hardware 4 comp.sys.mac.hardware 5 comp.windows.x 6 misc.forsale 7 rec.autos 8 rec.motorcycles 9 rec.sport.baseball 10 rec.sport.hockey 11 sci.crypt 12 sci.electronics 13 sci.med 14 sci.space 15 soc.religion.christian 16 talk.politics.guns 17 talk.politics.mideast 18 talk.politics.misc 19 talk.religion.misc 准确率：0.922811671088 id precision recall f1-score support 0 0.94 0.87 0.91 175 1 0.85 0.87 0.86 199 2 0.91 0.84 0.88 221 3 0.81 0.87 0.84 179 4 0.87 0.92 0.89 177 5 0.91 0.92 0.91 179 6 0.88 0.79 0.83 205 7 0.94 0.95 0.94 228 8 0.96 0.98 0.97 183 9 0.96 0.95 0.96 197 10 0.98 1.00 0.99 204 11 0.96 0.98 0.97 218 12 0.93 0.92 0.92 172 13 0.93 0.95 0.94 200 14 0.96 0.96 0.96 198 15 0.93 0.97 0.95 191 16 0.92 0.97 0.94 173 17 0.98 0.99 0.98 184 18 0.95 0.92 0.94 172 19 0.83 0.78 0.81 115 avg / total 0.92 0.92 0.92 3770 参考 JasonDing的 【机器学习实验】使用朴素贝叶斯进行文本的分类； Scikit-Learn Totorial]]></content>
      <categories>
        <category>算法</category>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>Python</tag>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[开始使用Scikit-Learn]]></title>
    <url>%2F2015%2F07%2F23%2F2015-07-23-start-to-use-scikit-learn%2F</url>
    <content type="text"><![CDATA[Python和R是做数据分析、数据挖掘、机器学习非常好的两门语言，在这儿不去讨论谁更好这个问题，没有最好，只有合适上手。对于码农出身，非科班统计学的我来说，使用Python相当习惯和顺手。 Python数据科学栈Python有很多做数据的类库，先列出常用的几个： Numpy、Scipy 基础数据类型 Matplotlib 绘图库 Pandas Ipython notebook Scikit-learn、MLlib 机器学习库 使用Scikit-learn的过程数据加载首先，将数据加载到内存中 12345678import numpy as npimport urlliburl = "http://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data"# download the fileraw_data = urllib.urlopen(url)dataset = np.loadtxt(raw_data, delimiter=',')X = dataset[:, 0:7]y = dataset[:, 8] X为特征数组，y为目标变量 数据标准化 (Data Normalization)大多数的梯度算法对数据的缩放很敏感，比如列A是体重数据(50kg等等)，列B是身高数据(170cm等等)，简单地说就是两个属性尺度不一样，所以在运行算法前要进行标准化或者叫归一化。 123from sklearn import preprocessing# normalize the data attributesnormalized_X = preprocessing.normalize(X) 特征选取虽然特征工程是一个相当有创造性的过程，有时候更多的是靠直觉和专业的知识，但对于特征的选取，已经有很多的算法可供直接使用。Scikit-Learn中的Recursive Feature Elimination Algorithm算法： 12345678from sklearn.feature_selection import RFEfrom sklearn.linear_model import LogisticRegressionmodel = LogisticRegression()# create RFE model and select 3 attributesrfe = RFE(model, 3)rfe = rfe.fit(X, y)print rfe.support_print rfe.ranking_ 机器学习算法看一看Scikit-learn库中所带的算法： 逻辑回归算法 Logistic Regression 朴素贝叶斯算法 Naive Bayes k-最邻算法 KNN 决策树 Decision Tree 支持向量机 SVM 逻辑回归 大多数情况下被用来解决分类问题（二元分类），但多类的分类（所谓的一对多方法）也适用。这个算法的优点是对于每一个输出的对象都有一个对应类别的概率。 1234567891011from sklearn import metricsfrom sklearn.linear_model import LogisticRegressionmodel = LogisticRegression()model.fit(X, y)print(model)# make predictionsexpected = ypredicted = model.predict(X)# summarize the fit of the modelprint(metrics.classification_report(expected, predicted))print(metrics.confusion_matrix(expected, predicted)) 朴素贝叶斯 它也是最有名的机器学习的算法之一，它的主要任务是恢复训练样本的数据分布密度。这个方法通常在多类的分类问题上表现的很好。 1234567891011from sklearn import metricsfrom sklearn.naive_bayes import GaussianNBmodel = GaussianNB()model.fit(X, y)print(model)# make predictionsexpected = ypredicted = model.predict(X)# summarize the fit of the modelprint(metrics.classification_report(expected, predicted))print(metrics.confusion_matrix(expected, predicted)) k-最近邻 kNN（k-最近邻）方法通常用于一个更复杂分类算法的一部分。例如，我们可以用它的估计值做为一个对象的特征。有时候，一个简单的kNN算法在良好选择的特征上会有很出色的表现。当参数（主要是metrics）被设置得当，这个算法在回归问题中通常表现出最好的质量。 123456789101112from sklearn import metricsfrom sklearn.neighbors import KNeighborsClassifier# fit a k-nearest neighbor model to the datamodel = KNeighborsClassifier()model.fit(X, y)print(model)# make predictionsexpected = ypredicted = model.predict(X)# summarize the fit of the modelprint(metrics.classification_report(expected, predicted))print(metrics.confusion_matrix(expected, predicted)) 决策树 分类和回归树（CART）经常被用于这么一类问题，在这类问题中对象有可分类的特征且被用于回归和分类问题。决策树很适用于多类分类。 123456789101112from sklearn import metricsfrom sklearn.tree import DecisionTreeClassifier# fit a CART model to the datamodel = DecisionTreeClassifier()model.fit(X, y)print(model)# make predictionsexpected = ypredicted = model.predict(X)# summarize the fit of the modelprint(metrics.classification_report(expected, predicted))print(metrics.confusion_matrix(expected, predicted)) 支持向量机SVM SVM（支持向量机）是最流行的机器学习算法之一，它主要用于分类问题。同样也用于逻辑回归，SVM在一对多方法的帮助下可以实现多类分类。 123456789101112from sklearn import metricsfrom sklearn.svm import SVC# fit a SVM model to the datamodel = SVC()model.fit(X, y)print(model)# make predictionsexpected = ypredicted = model.predict(X)# summarize the fit of the modelprint(metrics.classification_report(expected, predicted))print(metrics.confusion_matrix(expected, predicted)) 评价算法TBD 评价算法的好坏大约有几个方面： precision recall F1 score 优化算法的参数TBD]]></content>
      <categories>
        <category>算法</category>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>Python</tag>
        <tag>Scikit-Learn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Install Spark on Mac OSX Yosemite]]></title>
    <url>%2F2015%2F07%2F17%2F2015-07-17-install-spark-on-mac-osx-yosemite%2F</url>
    <content type="text"><![CDATA[Spark是个好东西。 Spark有以下四种运行模式： local: 本地单进程模式，用于本地开发测试Spark代码 standalone：分布式集群模式，Master-Worker架构，Master负责调度，Worker负责具体Task的执行 on yarn/mesos: ‌运行在yarn/mesos等资源管理框架之上，yarn/mesos提供资源管理，spark提供计算调度，并可与其他计算框架(如MapReduce/MPI/Storm)共同运行在同一个集群之上 on cloud(EC2): 运行在AWS的EC2之上 在Spark上又有多个应用，尤其是MLlib，Spark SQL和DataFrame，提供给数据科学家们无缝接口去搞所谓Data Science 本文记录一下我在Mac上安装Spark单机为分布式的过程 1.安装环境Spark依赖JDK 6.0以及Scala 2.9.3以上版本，安装好Java和Scala，然后配置好Java、Scala环境，最后再用java -version和scala -version验证一下 在~/.bash_profile中加入： 123# Setting PATH for scalaexport SCALA_HOME=/usr/local/Cellar/scala/2.11.6export PATH=$SCALA_HOME/bin:$PATH 别忘了 1source ~/.bash_profile 生效 由于在后面学习中主要会用到Spark的Python接口pyspark，所以在这儿也需要配置Python的环境变量： 1234# Setting PATH for Python 2.7# The orginal version is saved in .bash_profile.pysavePATH="/usr/local/Cellar/python/2.7.9/bin:$&#123;PATH&#125;"export PATH 2.伪分布式安装Spark的安装和简单，只需要将Spark的安装包download下来，加入PATH即可。这里我用的是Spark 1.4.0 当然，这里也可以使用Homebrew安装，那就更轻松了，直接 1$ brew install apache-spark 就搞定了，不过Homebrew安装没办法自己控制箱要安装的版本 这里我使用下载对Hadoop2.6的预编译版本安装 123cd /usr/local/Cellar/wget http://www.apache.org/dyn/closer.cgi/spark/spark-1.4.0/spark-1.4.0-bin-hadoop2.6.tgztar zxvf spark-1.4.0-bin-hadoop2.6.tgz 设置Spark环境变量，~/.bash_profile： 123456export SPARK_MASTER=localhostexport SPARK_LOCAL_IP=localhostexport SPARK_HOME=/usr/local/Cellar/spark-1.4.0-bin-hadoop2.6export PATH=$PATH:$SCALA_HOME/bin:$SPARK_HOME/binexport PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.8.2.1-src.zip:$PYTHONPATHexport PATH="/usr/local/sbin:$PATH" 安装完成，貌似也没什么安装哈~ 跑起来执行Spark根目录下的pyspark就可以以交互的模式使用Spark了，这也是他的一个优点 出现Spark的标志，那就说明安装成功了。下面再小配置下，让画面的log简单一点。在$SPARK_HOME/conf/下配置一下log4j的设置。 把log4j.properties.template文件复制一份，并删掉.template的扩展名 把这个文件中的INFO内容全部替换成WARN 在IPython中运行Spark说Spark好，那么IPython更是一大杀器，这个以后再介绍。先说设置 首先，创建IPython的Spark配置 1$ ipython profile create pyspark 然后创建文件$HOME/.ipython/profile_spark/startup/00-pyspark-setup.py并添加： 12345678import osimport sysspark_home = os.environ.get('SPARK_HOME', None)if not spark_home: raise ValueError('SPARK_HOME environment variable is not set')sys.path.insert(0, os.path.join(spark_home, 'python'))sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.8.2.1-src.zip'))execfile(os.path.join(spark_home, 'python/pyspark/shell.py')) 在IPython notebook中跑Spark 1$ ipython notebook --profile=pyspark 开始学习Spark吧！ 参考： Getting Started with Spark (in Python)]]></content>
      <categories>
        <category>Big Data</category>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Big Data</tag>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Install Hadoop on Yosemite]]></title>
    <url>%2F2015%2F07%2F17%2F2015-07-17-install-hadoop-on-mac-osx-yosemite%2F</url>
    <content type="text"><![CDATA[终于进入正题，开始写一写我在大数据方面走过的路，自认为被其他人甩下了，所以一定要紧追而上。 首先现在我的Mac上装上单节点的Hadoop玩玩，个人感觉Apache系列的项目，只要download下来，再配置以下参数就能玩了。 在这里感谢如下教程： INSTALLING HADOOP ON MAC Writing an Hadoop MapReduce Program in Python 下面开始吧 准备这个阶段主要就是准备一下JAVA的环境，Mac默认是安装了Java的，不过版本就不知道了，这个还是自己安装一下并且写到环境变量里来得踏实 Java Download 安装完之后，Java被装到了这个位置1/Library/Java/JavaVirtualMachines/jdk1.7.0_79.jdk/Contents/Home ，把这个地址写到系统的环境变量文件.bash_profile里 123# Setting PATH for javaexport JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.7.0_79.jdk/Contents/Homeexport PATH=$JAVA_HOME/bin:$PATH 配置SSH Nothing needs to be done here if you have already generated ssh keys. To verify just check for the existance of ~/.ssh/id_rsa and the ~/.ssh/id_rsa.pub files. If not the keys can be generated using 1$ ssh-keygen -t rsa Enable Remote Login “System Preferences” -&gt; “Sharing”. Check “Remote Login”Authorize SSH KeysTo allow your system to accept login, we have to make it aware of the keys that will be used 1$ cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys Let’s try to login. 123$ ssh localhostLast login: Fri Mar 6 20:30:53 2015$ exit 安装Homebrew在Mac上，最好的包安装工具就是Homebrew，执行下面代码安装： 1$ ruby -e "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)" 安装Hadoop我去，这么简单，直接 1$ brew install hadoop 就搞定了。。。这样，Hadoop会被安装在/usr/local/Cellar/hadoop目录下 下面才是重点，配置Hadoop 配置Hadoophadoop-env.sh该文件在/usr/local/Cellar/hadoop/2.6.0/libexec/etc/hadoop/hadoop-env.sh 找到如下这行： 1export HADOOP_OPTS="$HADOOP_OPTS -Djava.net.preferIPv4Stack=true" 改为： 1export HADOOP_OPTS="$HADOOP_OPTS -Djava.net.preferIPv4Stack=true -Djava.security.krb5.realm= -Djava.security.krb5.kdc=" Core-site.xml该文件在/usr/local/Cellar/hadoop/2.6.0/libexec/etc/hadoop/core-site.xml 123456789&lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/usr/local/Cellar/hadoop/hdfs/tmp&lt;/value&gt; &lt;description&gt;A base for other temporary directories.&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;fs.default.name&lt;/name&gt; &lt;value&gt;hdfs://localhost:9000&lt;/value&gt; &lt;/property&gt; mapred-site.xml文件在/usr/local/Cellar/hadoop/2.6.0/libexec/etc/hadoop/mapred-site.xml 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapred.job.tracker&lt;/name&gt; &lt;value&gt;localhost:9010&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; hdfs-site.xml文件在/usr/local/Cellar/hadoop/2.6.0/libexec/etc/hadoop/hdfs-site.xml 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 这就配置好了 添加启动关闭Hadoop快捷命令为了以后方便使用Hadoop，在.bash_profile中添加 12alias hstart="/usr/local/Cellar/hadoop/2.6.0/sbin/start-dfs.sh;/usr/local/Cellar/hadoop/2.6.0/sbin/start-yarn.sh"alias hstop="/usr/local/Cellar/hadoop/2.6.0/sbin/stop-yarn.sh;/usr/local/Cellar/hadoop/2.6.0/sbin/stop-dfs.sh" 执行下面命令将配置生效： 1source ~/.bash_profile 以后就能使用命令hstart启动Hadoop服务，hstop关闭Hadoop 格式化HDFS在使用Hadoop之前，还需要将HDFS格式化 1$ hdfs namenode -format Running Hadoop奔跑吧Hadoop 1$ hstart 使用jps命令查看Hadoop运行状态 1234567$ jps18065 SecondaryNameNode18283 Jps17965 DataNode18258 NodeManager18171 ResourceManager17885 NameNode 下面是几个很有用的监控Hadoop地址： Resource Manager: http://localhost:50070 JobTracker: http://localhost:8088 Specific Node Information: http://localhost:8042 停止Hadoop： 1$ hstop 添加Hadoop环境变量为了以后安装Spark等方便，在~/.bash_profile配置中添加Hadoop环境变量 12345# Setting PATH for hadoopexport HADOOP_HOME=/usr/local/Cellar/hadoop/2.6.0/libexecexport PATH=$HADOOP_HOME/bin:$PATHexport HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop 可能遇到的问题跑起来之后，或者在跑起来的过程中，可能会遇到各种问题，由于控制台命令太多，很难知道到底是哪儿出的问题，所以我总结出几个我遇到的问题和解决方法，分享给大家。TBD NameNode启动失败 大功告成！可以在Hadoop上跑几个MapReduce任务了。]]></content>
      <categories>
        <category>Big Data</category>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Big Data</tag>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在不同的机器上写博客]]></title>
    <url>%2F2015%2F07%2F17%2F2015-07-17-write-blog-from-different-machine%2F</url>
    <content type="text"><![CDATA[在家的Mac上配置好了Octopress，上班到公司还是要面对大Windows，这时候，想写一篇博客记录一下遇到的问题，怎么办？ Octopress原理Octopress的git仓库(repository)有两个分支，分别是master和source。master存储的是博客网站本身（html静态页面），而source存储的是生成博客的源文件（包括配置等）。master的内容放在根目录的_deploy文件夹内，当你push源文件时会忽略，它使用的是rake deploy命令来更新的。 下面开始在一台新机器上搞 创建一个本地Octopress仓库将博客的源文件，也就是source分支clone到本地的Octopress文件夹内 1$ git clone -b source git@github.com:username/username.github.com.git octopress 然后将博客文件也就是master分支clone到Octopress文件夹的_deploy文件夹内 12$ cd octopress$ git clone git@github.com:username/username.github.com.git _deploy 然后安装博客 123$ gem install bundler$ bundle install$ rake setup_github_pages OK了 继续写博客当你要在一台电脑写博客或做更改时，首先更新source仓库 1234$ cd octopress$ git pull origin source # update the local source branch$ cd ./_deploy$ git pull origin master # update the local master branch 写完博客之后不要忘了push，下面的步骤在每次更改之后都必须做一遍。 12345$ rake generate$ git add .$ git commit -am "Some comment here." $ git push origin source # update the remote source branch $ rake deploy # update the remote master branch]]></content>
      <categories>
        <category>备忘</category>
      </categories>
      <tags>
        <tag>Octopress</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Github与Octopress写博客]]></title>
    <url>%2F2015%2F07%2F16%2F2015-07-16-Writing%20Blogs%20with%20Github%20Pages%20and%20Octopress%2F</url>
    <content type="text"><![CDATA[Octopress和Github Pages是什么？ Octopress是一个基于Ruby语言的开源静态网站框架 Github Pages是Github上的一项服务， 注册用户可以申请一个和自己账号关联的二级域名， 在上面可以托管一个静态网站，网站内容本身就是Github的一个repository也就是项目， 维护这个项目的代码就是在维护自己的网站。简单来说就是 yourname.github.io/ 使用Octopress搭建博客，然后使用Github托管，有以下几个原因： 免费 版本控制，可以使用git实现写文章、建网站时候修改的版本控制 Octopress容易上手，并且这个风格正是我喜欢的，尤其对于一个理工男 使用Markdown，markdown是世界上最流行的轻量级标记语言 很酷，能装逼，当然这不是重点 搭建Octopress博客系统Note: 在这儿写的是关于在Mac上安装Octopress博客系统，和windows有细微的差别，不过个人觉得还是用Mac，无论写代码还是做黑客都更专业。 安装基本工具git 对于Mac来说，安装XCode之后，自带了git，可以使用下面命令检查本机的git版本 1$ git version Ruby Mac本身自带Ruby，但是也许版本过低，在这儿多说一句：有时候在低版本ruby下搭建好的Octopress，莫名其妙不好用了，原因也许就是Mac升级之后，ruby也升级了，要注意一下 至于Mac下如何使用Homebrew安装，请查看其他文章。 12$ brew install ruby$ ruby --version Ruby版本在1.9.3以上就可以了，就可以使用gem来安装Ruby的包了 PS. gem在Ruby中，相当于Python中的pip 由于我们生活在一个伟大的国家，so在下一步安装前，先更改一下gem的更新源，改为淘宝的源 123gem sources -a http://ruby.taobao.org/gem sources -r http://rubygems.org/gem sources -l 三行命令的作用分别是：添加淘宝源；删除默认源；显示当前源列表。显示淘宝地址就表示成功。 安装bundle和bundler， 12gem install bundlegem install bundler Note: 安装配置完新版本的Ruby后，一定要重新安装bundle和bundler，否则bundle仍会bundler指向旧版本的Ruby，PS. 由于手贱，把MAC升级到最新系统了，结果各种奇妙的事情就发生了，不过处理方法一般都是：安装最新版本的Ruby，然后再安装bundler和bundler Octopress 这个就是我们要使用的框架，它是基于Jekyll的一个静态博客生成框架，Jekyll是一个静态网站生成框架，它有很多功能，也可以直接使用，但是就麻烦得多，很多东西要配置和从头写。 1234git clone git://github.com/imathis/octopress.git octopresscd octopressbundle installrake install 创建Github账号和Github Pages 大多数人都已经有了Github帐号了，访问Github来注册帐号，然后访问Github Pages来创建博客空间，唯一需要注意的是Repo必须是Github帐号.github.io，否则不会起作用。 然后运行： 1rake setup_github_pages 输入Github Page的Repo的地址，例如：git@github.com:username/username.github.io.git，就可以了 测试一下输入命令生成页面 1rake generate 生成完毕后，使用以下命令启动网站进程，默认占用4000端口， preview一下 1rake preview 可以使用 http://localhost:4000 访问你的博客页面了 配置博客配置文件是根目录下的 _config.yml文件，使用vim或者其他文本编辑器编辑它吧 12345678910# ----------------------- ## Main Configs ## ----------------------- #url: http://lvraikkonen.github.iotitle: My Data Science Pathsubtitle: Shut up, just codingauthor: Claus Lvsimple_search: https://www.google.com/searchdescription: 语法高亮例子： 1alert("欢迎") 1234import mathprint "Hello World"lst = range(100)print lst.map(lambda x: x**2) 123456789101112131415&lt;!-- mathjax config similar to math.stackexchange --&gt;&lt;script type="text/x-mathjax-config"&gt; MathJax.Hub.Config(&#123; jax: ["input/TeX", "output/HTML-CSS"], tex2jax: &#123; inlineMath: [ ['$', '$'] ], displayMath: [ ['$$', '$$']], processEscapes: true, skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'] &#125;, messageStyle: "none", "HTML-CSS": &#123; preferredFont: "TeX", availableFonts: ["STIX","TeX"] &#125; &#125;);&lt;/script&gt;&lt;script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"&gt;&lt;/script&gt; 添加社交分享Octopress默认是带有社交分享功能的，比如Twitter, Facebook, Google Plus等，但这些全世界都通用的东西在我大天朝就是不好使。 网站页的分享有很多第三方的库，这里用jiathis 在_config.yml中加入social_share: true 修改/source/_includes/post/sharing.html 访问jiathis获取分享的代码，放入新建的/source/_includes/post/social_media.html 添加博客评论Octopress也默认集成有评论系统Disqus，这个是国外最大的第三方评论平台，世界都在用，除了我大天朝。这里使用多说 到多说注册，获取用户名，也就是在多说上添的youname.duoshuo.com中的yourname 在_config.yml中添加 12duoshuo_comments: trueduoshuo_short_name: yourname 在/source/_layouts/post.html中把评论模版添加到网页中 创建/source/_includes/post/duoshuo.html，将上步获取的HTML代码放进去 国内访问加速jQuery源 修改jQuery的源 打开source/_includes/head.html，找到如下 1&lt;script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"&gt;&lt;/script&gt; 改为： 1&lt;script src="//ajax.aspnetcdn.com/ajax/jQuery/jquery-1.9.1.min.js"&gt;&lt;/script&gt; 字体源 Octopress的英文字体是加载的Google Fonts，我们将其改成国内的CDN源， 打开source/_includes/custom/head.html, 将其中的https://fonts.googleapis.com改为http://fonts.useso.com Twitter Facebook Google+关闭 在前面提到的_config.yml中相关的例如twitter_tweet_butto改为false 写博客要发布一篇新文章，在命令行中输入以下命令： 1rake new_post["postName"] 之后在/source/_post/里面就有该博文的markdown文件了，使用Markdown文本编辑器写博客吧 rake generate 生成静态的博客文件，生成的文件在_deploy中 rake preview 在本地预览博客，这与发布到Github Pages后的效果是一样的 rake deploy 这是最后一步，就是把Octopress生成的文件（在_deploy）发布到Github上面去。这里的实际是Octopress根据你的配置用sources中的模板，生成网页（HTML，JavaScript, CSS和资源），再把这些资源推送到yourname.github.io这个Repo中去，然后访问https://yourname.github.io 就能看到你的博客了 发布执行命令 12$ rake generate$ rake deploy 第一行命令用来生成页面，第二行命令用来部署页面，上述内容完成，就可以访问 http://[your_username].github.io/看博客了 Note: octopress 根目录为source分支， _deploy目录下为master分支，rake deploy时候会把_deploy下的内容发布到github上的master分支。别忘了把源文件（包括配置等）发布到source分支下 push时候可用 1$ git status 查看状态 执行以下命令，将源文件发布到Github的source分支 123git add .git commit -m "备注内容"git push origin source 如果遇到类似error: failed to push some refs to的错误，参考 stackoverflow解决]]></content>
      <categories>
        <category>备忘</category>
      </categories>
      <tags>
        <tag>Octopress</tag>
      </tags>
  </entry>
</search>
