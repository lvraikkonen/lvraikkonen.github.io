<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Flask 从入门到放弃3: 渲染模版]]></title>
    <url>%2F2017%2F08%2F14%2FFlask-%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E6%94%BE%E5%BC%833-%E6%B8%B2%E6%9F%93%E6%A8%A1%E7%89%88%2F</url>
    <content type="text"><![CDATA[MVCMVC：Model-View-Controller，中文名“模型-视图-控制器” Model（模型）是应用程序中用于处理应用程序数据逻辑的部分。通常模型对象负责在数据库中存取数据。 View（视图）是应用程序中处理数据显示的部分。通常视图是依据模型数据创建的。 Controller（控制器）是应用程序中处理用户交互的部分。通常控制器负责从视图读取数据，控制用户输入，并向模型发送数据。 Flask支持MVC模型，Flask默认使用Jinjia2模板引擎，对模版进行渲染，最终生成HTML文件。视图方法有两个作用：处理业务逻辑（比如操作数据库）和 返回响应内容。模板起到了将两者分开管理的作用。 下面介绍走动生成HTML的方法：模版渲染 模版默认情况下,Flask 在程序文件夹中的 templates 子文件夹中寻找模板。 一个模版文件 user.html： 12345678&lt;html&gt; &lt;head&gt; &lt;title&gt;&#123;&#123; title &#125;&#125; - microblog&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt;Hello, &#123;&#123; user_name &#125;&#125;!&lt;/h1&gt; &lt;/body&gt;&lt;/html&gt; 调用模版 123456789from flask import Flask, render_template@app.route('/')def index(): return render_template('index.html')@app.route('/user/&lt;name&gt;')def user(name): return render_template('user.html', user_name=name) render_template()第一个参数是模板的名称，然后是 键/值 对，user_name=name左边表示模板中的占位符，右边是当前视图中的变量。 变量类型模板中不仅能使用字符串数字等简单的数据类型，还能接收复杂的数据结构，比如dict、list、obj等。 1234&lt;p&gt;A value from a dictionary: &#123;&#123; mydict['key'] &#125;&#125;.&lt;/p&gt;&lt;p&gt;A value from a list: &#123;&#123; mylist[3] &#125;&#125;.&lt;/p&gt;&lt;p&gt;A value from a list, with a variable index: &#123;&#123; mylist[myintvar] &#125;&#125;.&lt;/p&gt;&lt;p&gt;A value from an object's method: &#123;&#123; myobj.somemethod() &#125;&#125;.&lt;/p&gt; 控制结构Jinjia2能够使用常见的控制流，如下是常用的几种控制流： if else for 12345&#123;% if user %&#125; Hello, &#123;&#123;user&#125;&#125;&#123;% else %&#125; Hello, stranger&#123;% endif %&#125; 12345&lt;ul&gt; &#123;% for comment in comments%&#125; &lt;li&gt;&#123;&#123; comment &#125;&#125;&lt;/li&gt; &#123;% endfor %&#125;&lt;/ul&gt; 模版继承和类继承的方式类似，如果多个页面的大部分内容相同，可以定义一个父模板，包含相同的内容，然后子模板继承内容，并根据需要进行部分修改。block标签定义的元素可以在衍生模板中修改。 123456789101112131415&lt;!DOCTYPE html&gt;&lt;html lang="en"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &#123;% block head%&#125; &lt;title&gt; &#123;% block title%&#125;&#123;% endblock%&#125;- My Application &lt;/title&gt; &#123;% endblock %&#125;&lt;/head&gt;&lt;body&gt; &#123;% block body%&#125; &#123;% endblock%&#125;&lt;/body&gt;&lt;/html&gt; 父模版定义了head, title和body三个块，可以在子模板中进行修改。 123456789101112&#123;% extends 'base.html'%&#125;&#123;% block title%&#125; Index&#123;% endblock %&#125;&#123;% block head%&#125; &#123;&#123; super() &#125;&#125; &lt;style&gt; &lt;/style&gt;&#123;% endblock%&#125;&#123;% block body%&#125; &lt;h1&gt;Helll, World!&lt;/h1&gt;&#123;% endblock%&#125; 继承父模版，extends命令声明这个模版继承自base.html。其中，super()这个命令调用父模版的内容。 例子：Jinjia2集成BootstrapBootstrap 是腿特开源的一个开源框架，可以使用这个框架创建一个比较漂亮的网页。这里使用叫做Flask-Bootstrap的Flask扩展。 1(venv) $ pip install flask-bootstrap 安装好之后，就可以在命名空间中导入了。 123from flask_bootstrap import Bootstrap# ...bootstrap = Bootstrap(app) 创建UI的父模版页面整体可以分为两部分：导航条和页面主体。 extends &quot;bootstrap/base.html&quot; 表明这个模版继承自Bootstrap中的bootstrap/base.html。在这个模板中定义了title, navbar, content和page_content这几个块。 自定义404页面UI的框架已经定义好了，下面就可以自定义一个模版，用来显示404页面。 404.html 123456789&#123;% extends "base.html" %&#125;&#123;% block title %&#125;Flasky - Page not found&#123;% endblock %&#125;&#123;% block page_content %&#125;&lt;div class="page-header"&gt; &lt;h1&gt;Not Found&lt;/h1&gt;&lt;/div&gt;&#123;% endblock %&#125; 这个页面继承自上面定义好的父模版，在404.html模版页里面，对title和page_content这两个块的内容进行修改。 现在页面的UI就从丑陋变得稍微美观一点了。]]></content>
      <categories>
        <category>Flask从入门到放弃</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Flask</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 浅复制与深复制]]></title>
    <url>%2F2017%2F07%2F31%2FPython-%E6%B5%85%E5%A4%8D%E5%88%B6%E4%B8%8E%E6%B7%B1%E5%A4%8D%E5%88%B6%2F</url>
    <content type="text"><![CDATA[Python中，万物皆对象。 在介绍Python的浅复制和深复制之前，先来歪个楼，说明一下Python的可变对象和不可变对象。提到这里，有两个坑不得不拿出来说一下。 坑1：可变对象作为函数默认值先介绍一个Python里面常见的坑： 1234567def append_to_list(value, def_list=[]): def_list.append(value) return def_listmy_list = append_to_list(1)my_other_list = append_to_list(2)print my_other_list 这时候的输出是什么呢？ 1[1, 2] 意不意外？惊不惊喜？ :) 为什么呢？这是因为这个默认值是在函数建立的时候就生成了, 每次调用都是用了这个对象的”缓存”。下面图表示第二次调用函数 append_to_list时候的引用状况： 这就是一条避坑指南：不要使用可变对象作为函数默认值 12345def append_to_list(value, def_list=None): if def_list is None: def_list = [] def_list.append(value) return def_list 坑2：list += 的不同行为123456789a1 = range(3)a2 = a1a2 += [3]print a1, a2a1 = range(3)a3 = a1a3 = a3 + [3]print a1, a3 你会发现第一段代码的结果a1和a2都为[0,1,2,3]，而第二段的代码a1为[0,1,2] a3为[0,1,2,3]。为什么两次会不同呢？上学时候老师不是说 a+=b 等价于 a=a+b 的吗？ 可变和不可变数据类型Python中，对象分为可变(mutable)和不可变(immutable)两种类型 字典型(dictionary)和列表型(list)的对象是可变对象 元组（tuple)、数值型（number)、字符串(string)均为不可变对象 下面验证一下这两种类型，可变对象一旦创建之后还可改变但是地址不会发生改变，即该变量指向的还是原来的对象。而不可变对象则相反，创建之后不能更改，如果更改则变量会指向一个新的对象。 12345678910111213s = 'abc' # 不可变对象print(id(s))# 45068120s += 'd'print(id(s))# 75648536l = ['a','b','c']print(id(l)) # 可变对象# 74842504l += 'd'print(id(l))# 74842504 这个案列也就解释了上面坑2的原因，原因：对于可变对象，例子中的list， += 操作调用 __iadd__ 方法，相当于 a1 = a1.__iadd__([3])，是直接在 a2(a1的引用)上面直接更新。而对于 +操作来说，会调用 __add__ 方法，返回一个新的对象。所以对于可变对象来说 a+=b 是不等价于 a=a+b 的。 好了，下面进入正题，如何复制一个对象。 浅复制Python中对象之间的赋值是按引用传递的 (敲黑板！) 标识一个对象唯一身份的是：对象的id(内存地址)，对象类型，对象值，而浅拷贝就是创建一个具有相同类型，相同值但不同id的新对象 如果需要复制对象，可以使用标准库中的copy模块，copy.copy是浅复制，只会复制父对象，而不会复制对象内部的子对象； 对于list对象，使用 list() 构造方法或者切片的方式，做的是浅复制，复制了外层容器，副本中的元素其实是源容器中元素的 引用 1234567l1 = [3, [55, 44], (7, 8, 9)]l2 = list(l1)print(id(l1), id(l2))print(id(l1[1]), id(l2[1]))# 2148206816520 2148206718344# 2148206717768 2148206717768 深复制copy.deepcopy是深复制，会复制对象及其子对象。 12345678910import copyorigin_list = [0, 1, 2, [3, 4]]copy_list = copy.copy(origin_list)deepcopy_list = copy.deepcopy(origin_list)origin_list.append('hhh')origin_list[3].append('aaa')print(origin_list, copy_list, deepcopy_list) 看了下面的引用关系，结果就猜不错了 在原列表后面添加一个元素，不会对复制的两个列表有影响；浅复制列表中最后一个元素是原列表最后一个元素的引用，所以添加一个元素也影响浅复制的列表。结果为 123origin_list: [0, 1, 2, [3, 4, &apos;aaa&apos;], &apos;hhh&apos;]copy_list: [0, 1, 2, [3, 4, &apos;aaa&apos;]]deepcopy_list: [0, 1, 2, [3, 4]]]]></content>
      <tags>
        <tag>Python</tag>
        <tag>Reference</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flask 从入门到放弃2: 深入理解@app.route()]]></title>
    <url>%2F2017%2F07%2F26%2FFlask-%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E6%94%BE%E5%BC%832-%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3-app-route%2F</url>
    <content type="text"><![CDATA[下面这段代码是Flask的主页上给出的，这是一段Hello World级别的代码段，但是里面包含的概念可一点都不简单。 123456from flask import Flaskapp = Flask(__name__)@app.route("/")def hello(): return "Hello World!" 这里面的 @app.route(&#39;/&#39;) 到底是什么意思呢，具体又是如何实现的呢？很多初学者都是很迷茫。我在集中精力理解了装饰器之后，慢慢的就对app.route 这个装饰器的原理以及目的有了了解了。 以前写过一篇文章，详细说明了装饰器的概念：搞懂Python装饰器 要是忘了可以随时复习一下。 给装饰器传参数需要外嵌一个工厂函数，调用这个函数，然后返回函数的装饰器 1234567891011121314151617def decorator_factory(enter_message, exit_message): # return this decorator print "In decorator_factory" def simple_deco(func): def wrapper(): print enter_message func() print exit_message return wrapper return simple_deco@decorator_factory("Start", "End")def hello(): print "Hello World"hello() 注意，这里使用 @decorator_factory(&quot;Start&quot;, &quot;End&quot;) 的时候，实际调用的是 decorator_factory 函数。相当于如下调用： 1decorator_factory(&quot;Start&quot;, &quot;End&quot;)(hello) 输出结果为： 创建自己的Flask类现在我们已经有了足够的装饰器的背景知识，可以模拟一下Flask对象里面的内容。 创建route装饰器我们知道，Flask是一个类，而类方法也可以被用作装饰器。 1234567891011121314class MyFlask(): # decorator_factory def route(self, route_str): def decorator(func): return func return decoratorapp = MyFlask()@app.route("/")def hello(): return "Hello World" 这里不想修改被装饰函数的行为，只是想获取被装饰函数的引用，以便后面注册这个函数用。 添加一个存储路由的字典现在，需要一个变量去存储路由和其关联的函数 123456789101112131415161718192021222324class MyFlask(): def __init__(self): self.routes = &#123;&#125; # decorator_factory def route(self, route_str): def decorator(func): self.routes[route_str] = func return func return decorator # access register variable def serve(self, path): view_function = self.routes.get(path) if view_function: return view_function() else: raise ValueError('Route "&#123;&#125;" has not been registered'.format(path))app = MyFlask()@app.route("/")def hello(): return "Hello World" 当给定的路径被注册过则返回函数运行结果，当路径尚未注册时则抛出一个异常。 解释动态URL形如 @app.route(&quot;/hello/&lt;username&gt;&quot;) 这样的路径又是如何解析出参数的呢？Flask使用正则的形式表达路径。这样就可以将路径作为一种模式进行匹配。 使用正则表达式123456import reroute_regex = re.compile(r'^/hello/(?P&lt;username&gt;.+)$')match = route_regex.match("/hello/ains")print match.groupdict() 输出结果为：{&#39;username&#39;: &#39;ains&#39;} 现在需要使用 (pattern, view_function) 这个元组来保存路径编译变成一个正则表达式和注册函数的关系。然后在装饰器中，把编译好的正则表达式和注册函数的元组保存在列表中。 1234567891011121314routes = []def build_route_pattern(route): route_regex = re.sub(r'(&lt;\w+&gt;)', r'(?P\1.+)', route) return re.compile("^&#123;&#125;$".format(route_regex))def route(self, route_str): def decorator(func): route_pattern = build_route_pattern(route_str) routes.append((route_pattern, func)) return func return decorator 接下来，再创建一个访问routes变量的函数，如果匹配上，则返回正则表达式匹配组和注册函数组成的元组。 1234567def get_route_match(path): for route_pattern, view_function in routes: m = route_pattern.match(path) if m: return m.groupdict(), view_function return None 再接下来要找出调用view_function的方法，使用来自正则表达式匹配组字典的正确参数 1234567def serve(path): route_match = get_route_match(path) if route_match: kwargs, view_function = route_match return view_function(**kwargs) else: raise ValueError('Route "&#123;&#125;"" has not been registered'.format(path)) 改好的MyFlask类如下： 123456789101112131415161718192021222324252627282930313233class MyFlask(): def __init__(self): self.routes = [] @staticmethod def build_route_pattern(route): route_regex = re.sub(r'(&lt;\w+&gt;)', r'(?P\1.+)', route) return re.compile("^&#123;&#125;$".format(route_regex)) def route(self, route_str): def decorator(func): route_pattern = self.build_route_pattern(route_str) self.routes.append((route_pattern, func)) return func return decorator def get_route_match(self, path): for route_pattern, view_function in self.routes: m = route_pattern.match(path) if m: return m.groupdict(), view_function return None def serve(self, path): route_match = self.get_route_match(path) if route_match: kwargs, view_function = route_match return view_function(**kwargs) else: raise ValueError('Route "&#123;&#125;"" has not been registered'.format(path)) 运行一段带参数的试试 1234567app = MyFlask()@app.route("/hello/&lt;username&gt;")def hello_user(username): return "Hello &#123;&#125;!".format(username)print app.serve("/hello/ains") 下面是程序运行的引用关系图]]></content>
      <categories>
        <category>Flask从入门到放弃</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Flask</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flask 从入门到放弃1: Hello World]]></title>
    <url>%2F2017%2F07%2F25%2FFlask-%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E6%94%BE%E5%BC%831-Hello-World%2F</url>
    <content type="text"><![CDATA[除了Flask，常见的Python Web框架还有： Django：全能型Web框架； web.py：一个小巧的Web框架； Bottle：和Flask类似的Web框架； Tornado：Facebook的开源异步Web框架。 Flask简介Flask 是一个用于 Python 的微型网络开发框架，依赖两个外部库： Jinja2 模板引擎和 Werkzeug WSGI 套件。Flask也被称为microframework，因为它使用简单的核心，用加载扩展的方式增加其他功能。 Flask 没有默认使用的数据库、窗体验证工具。但是，Flask 保留了扩增的弹性，可以用Flask扩展加入这些功能：ORM、窗体验证工具、文件上传、开放式身份验证技术。 准备环境对于Python来说，有相当数量的外部包，如果管理不当，会让人崩溃，创建一个Flask的Web项目更是这样，所以推荐一个项目一套环境。这里使用 virtualenv 在项目的目录中创建这个项目的虚拟环境。 123456$ sudo pip install virtualenv$ mkdir myproject$ cd myproject$ virtualenv venv$ . venv/bin/activate$ pip install Flask Hello World 应用官方文档上给出的hello world例子很小，但是也基本说明了一个Flask应用都包含了什么 123456789from flask import Flaskapp = Flask(__name__)@app.route('/')def hello_world(): return 'Hello World!'if __name__ == '__main__': app.run() 保存代码为 hello.py，在命令行运行 1python hello.py 这时候访问 http://127.0.0.1:5000/ 可以看见页面输出 Hello World! 程序都干了啥 首先导入了 Flask 类，这个类的实例会是WSGI应用程序 接下来创建了这个类的实例 app 然后，使用 route() 装饰器进行路由绑定，通过路由来绑定URL和Python函数的映射关系。 最后使用 run() 函数来让应用运行在本地服务器上。 其中 if __name__ == &#39;__main__&#39;: 确保服务器只会在该脚本被 Python 解释器直接执行的时候才会运行，而不是作为模块导入的时候。 好了，Ctrl+C关闭服务器。到这里一个麻雀虽小五脏俱全的小Flask应用就创建好了。]]></content>
      <categories>
        <category>Flask从入门到放弃</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Flask</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[搞懂Python装饰器]]></title>
    <url>%2F2017%2F07%2F20%2F%E6%90%9E%E6%87%82Python%E8%A3%85%E9%A5%B0%E5%99%A8%2F</url>
    <content type="text"><![CDATA[装饰器是Python中的一个高阶概念，装饰器是可调用的对象，其参数是另外一个函数。装饰器可能会处理被装饰的函数然后把它返回，或者将其替换成另外一个函数或者可调用对象。 这么介绍装饰器确实很难懂，还是以例子逐步理解更容易些。 装饰器的强大在于它能够在不修改原有业务逻辑的情况下对代码进行扩展，常见的应用场景有：权限校验、用户认证、日志记录、性能测试、事务处理、缓存等。 下面记录一下我逐步理解装饰器的过程。 一等函数在Python中，函数是一等对象，也就是说函数是满足以下条件的程序实体： 在运行时创建 能赋值给变量或者数据结构中的元素 能作为参数传给函数 能作为函数的返回结果 下面先看一个简单函数的定义 12def hello(): print("Hello world!") python解释器遇到这段代码的时候，发生了两件事： 编译代码生成一个函数对象 将名为”hello”的名字绑定到这个函数对象上 Python中函数是一等对象，也就是说函数可以像int、string、float对象一样作为参数、或者作为返回值等进行传递。 函数作为参数123456789101112def foo(bar): return bar + 1print(foo)print(foo(2))print(type(foo))def call_foo_with_arg(foo, arg): return foo(arg)print(call_foo_with_arg(foo, 3)) 函数 call_foo_with_arg 接收两个参数，其中一个是可被调用的函数对象 foo 嵌套函数函数也可以定义在另外一个函数中，作为嵌套函数 1234567891011def parent(): print("Printing from the parent() function.") def first_child(): return "Printing from the first_child() function." def second_child(): return "Printing from the second_child() function." print(first_child()) print(second_child()) first_child 和 second_child 函数是嵌套在 parent 函数中的函数。 当调用 parent 函数时，内嵌的first_child 和 second_child 函数也被调用，但是如果在 parent 函数中并不是调用first_child 和 second_child 函数， 而是返回这两个函数对象呢？ 下面歪个楼，先介绍一下Python的变量作用域的规则。 变量作用域Python是动态语言，Python的变量名解析机制有时称为LEGB法则，当在函数中使用未认证的变量名时，Python搜索4个作用域： local 函数内部作用域 enclosing 函数内部与内嵌函数之间 global 全局作用域 build-in 内置作用域 12345def f1(a): print(a) print(b)f1(3) 这段程序会抛出错误：”NameError: name ‘b’ is not defined”，这是因为在函数体内，Python编译器搜索上面 LEGB 的变量，没有找到。 再下面一个例子： 1234567b = 6def f2(a): print(a) print(b) b = 9f2(3) 在Python中，Python不要求声明变量，但是假定在函数体中被赋值的变量是局部变量，所以在这个函数体中，变量b被判断成局部变量，所以在print(b)调用时会抛出 “UnboundLocalError: local variable ‘b’ referenced before assignment” 的错误。 要想上面的代码运行，就必须手动在函数体内声明变量b为全局变量 12345678b = 6def f2(a): global b print(a) print(b) b = 9f2(3) 闭包有了上面的背景知识，下面就可以介绍闭包了。闭包是指延伸了作用域的函数。 要想理解这个概念还是挺难的，下面还是用例子来说明。 现在有个avg函数，用于计算不断增长的序列的平均值。 12345678910111213def make_averager(): series = [] def averager(new_value): series.append(new_value) total = sum(series) return total / len(series) return averageravg = make_averager()avg(10)avg(11) 调用函数 make_averager 时候，返回一个 averager 函数对象，每次调用 averager 函数，会把参数添加到 series 中，然后计算当前平均值。 series 是 make_averager 函数的局部变量，调用 avg(10) 时， make_averager 函数已经返回，所以本地作用域也就不存在了。但是在 averager 函数中，series 是自由变量 这里的 avg 就是一个闭包，本质上它还是函数，闭包是引用了自由变量(series)的函数(averager) nonlocal声明刚才的例子稍稍改动一下，使用total和count来计算移动平均值 12345678910111213def make_averager(): count = 0 total = 0 def averager(new_value): count += 1 total += new_value return total / count return averageravg = make_averager()avg(10) 这时候会抛出错误 “UnboundLocalError: local variable ‘count’ referenced before assignment”。这是因为：当count为数字或者任何不可变类型时，在函数体定义中 count = count + 1 实际上是为count赋值，所以count就变成了局部变量。为了避免这个问题，python3引入了 nonlocal 声明，作用是把变量标记成 自由变量 12345678910111213141516def make_averager(): count = 0 total = 0 def averager(new_value): nonlocal count, total count += 1 total += new_value return total / count return averageravg = make_averager()print(avg(10))print(avg(11))print(avg(12)) 装饰器了解了闭包之后，下面就可以用嵌套函数实现装饰器了。事实上，装饰器就是一种闭包的应用，只不过传递的是函数。 无参数装饰器下面写一个简单的装饰器的例子 123456789101112131415161718def makebold(fn): def wrapped(): return '&lt;b&gt;' + fn() + '&lt;/b&gt;' return wrappeddef makeitalic(fn): def wrapped(): return '&lt;i&gt;' + fn() + '&lt;/i&gt;' return wrapped@makebold@makeitalicdef hello(): return "Hello World"print(hello()) makeitalic 装饰器将函数 hello 传递给函数 makeitalic，函数 makeitalic 执行完毕后返回被包装后的 hello 函数，而这个过程其实就是通过闭包实现的 装饰器有一个语法糖@,直接@my_new_decorator就把上面一坨代码轻松化解了，这就是Pythonic的代码，简洁高效，使用语法糖其实等价于下面显式使用闭包 12hello_bold = makebold(hello)hello_italic = makeitalic(hello) 装饰器是可以叠加使用的，对于Python中的”@”语法糖，装饰器的调用顺序与使用 @ 语法糖声明的顺序相反，上面案例中叠加装饰器相当于如下包装顺序： 1hello = makebold(makeitalic(hello)) 被装饰的函数带参数再来一个例子 12345678910111213141516171819202122232425262728293031323334353637import timeimport functoolsdef clock(func): @functools.wraps(func) def clocked(*args, **kwargs): """ in wrapper """ t0 = time.time() # execute result = func(*args, **kwargs) elapsed = time.time() - t0 name = func.__name__ arg_lst = [] if args: arg_lst.append(', '.join(repr(arg) for arg in args)) if kwargs: pairs = ['%s=%r' % (k, w) for k, w in sorted(kwargs.items())] arg_lst.append(', '.join(pairs)) arg_str = ', '.join(arg_lst) print('[%0.8fs] %s(%s) -&gt; %r ' % (elapsed, name, arg_str, result)) return result return clocked@clockdef snooze(seconds): """ sleep for seconds """ time.sleep(seconds)@clockdef factorial(n): """ calculate n! """ return 1 if n&lt;2 else n*factorial(n-1) snooze和factorial函数会作为func参数传给clock函数，然后clock函数会返回clocked函数。所以现在factorial保留的是clocked函数的引用。但是这也是装饰器的一个副作用：会把被装饰函数的一些元数据，例如函数名、文档字符串、函数签名等信息覆盖掉。下面会使用functools库中的 @wraps 装饰器来避免这个。 内嵌包装函数 clocked 的参数跟被装饰函数的参数对应，这里使用了 (*args, **kwargs)，是为了适应可变参数。 clocked函数做了以下几件事： 记录初始时间 调用原来的factorial函数，保存结果 计算经过的时间 格式化收集的数据，然后打印出来 返回第2步保存的结果 12print('*'*40, 'Calling factorial(6)')print('6! = ', factorial(6)) 装饰器的典型行为就是：把被装饰的函数体换成新函数，二者接受相同的参数，返回被装饰的函数本该返回的值，同时有额外操作 另外，内嵌包装函数 clocked 添加了functools库中的 @wraps 装饰器，这个装饰器可以把被包装函数的元数据，例如函数名、文档字符串、函数签名等信息保存下来。 123print(snooze(5))print(snooze.__doc__)print('origin func name is:', snooze.__name__) 1234[5.01506114s] snooze(5) -&gt; NoneNone sleep for secondsorigin func name is: snooze 参数化装饰器如果装饰器本身需要传入参数，那就需要编写一个返回decorator的高阶函数，也就是针对装饰器进行装饰。 下面代码来自 Python Cookbook： 123456789101112131415161718192021222324252627282930from functools import wrapsimport loggingdef logged(level, name=None, message=None): """ Add logging to a function. level is the logging level, name is the logger name, and message is the log message. If name and message aren't specified, they default to the function's module and name. """ def decorate(func): logname = name if name else func.__module__ log = logging.getLogger(logname) logmsg = message if message else func.__name__ @wraps(func) def wrapper(*args, **kwargs): log.log(level, logmsg) return func(*args, **kwargs) return wrapper return decorate# Example use@logged(logging.DEBUG)def add(x, y): return x + y@logged(logging.CRITICAL, 'example')def spam(): print('Spam!') 最外层的函数 logged() 接受参数并将它们作用在内部的装饰器函数上面。 内层的函数 decorate() 接受一个函数作为参数，然后在函数上面放置一个包装器。这个装饰器的处理过程相当于： 1spam = logged(x, y)(spam) 首先执行logged(&#39;x&#39;, &#39;y&#39;)，返回的是 decorate 函数，再调用返回的函数，参数是 spam 函数。 装饰器在真实世界的应用更多的装饰器的案例： PythonDecoratorLibrary 1. 给函数调用做缓存像求第n个斐波那契数来说，是个递归算法，对于这种慢速递归，可以把耗时函数的结果先缓存起来，在调用函数之前先查询一下缓存，如果没有才调用函数 12345678910111213141516171819202122from functools import wrapsdef memo(func): cache = &#123;&#125; miss = object() @wraps(func) def wrapper(*args): result = cache.get(args, miss) if result is miss: result = func(*args) cache[args] = result return result return wrapper@memo@clockdef fib(n): if n &lt; 2: return n return fib(n-2) + fib(n-1) 也可以使用下面的functools库里面的 lru_cache 装饰器来实现缓存。 2. LRUCacheLRU就是Least Recently Used，即最近最少使用，是一种内存管理算法。 12345678910import functools@functools.lru_cache()@clockdef fibonacci(n): if n &lt; 2: return n return fibonacci(n-2) + fibonacci(n-1)print(fibonacci(6)) 3. 给函数输出记日志1234567891011121314151617181920212223import timefrom functools import wrapsdef log(func): @wraps(func) def wrapper(*args, **kwargs): print("Function running") ts = time.time() result = func(*args, **kwargs) te = time.time() print("Function = &#123;0&#125;".format(func.__name__)) print("Arguments = &#123;0&#125; &#123;1&#125;".format(args, kwargs)) print("Return = &#123;0&#125;".format(result)) print("time = %.6f seconds" % (te - ts)) return wrapper@logdef sum(x, y): return x + yprint(sum(1, 2)) 4. 数据库连接1234567891011121314151617181920def open_and_close_db(func): def wrapper(*a, **k): conn = connect_db() result = func(conn=conn, *a, **k) conn.commit() conn.close() return result return wrapper@open_and_close_dbdef query_for_dict(sql, conn): cur = conn.cursor() try: cur.execute(sql) conn.commit() entries = [dict(zip([i[0] for i in cur.description], row)) for row in cur.fetchall()] print entries except Exception,e: print e return entries 5. Flask路由拿Flask的 hello world来说： 123456789from flask import Flaskapp = Flask(__name__)@app.route("/")def hello(): return "Hello World!"if __name__ == '__main__': app.run() 到这儿，装饰器的一些基本概念就都清楚了。 参考Python 的闭包和装饰器 Fluent Python]]></content>
      <tags>
        <tag>Python</tag>
        <tag>装饰器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python dict类型的实现]]></title>
    <url>%2F2017%2F07%2F18%2FPython-dict%E7%B1%BB%E5%9E%8B%E7%9A%84%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[程序员们的经验里面，通常都会认为字典和集合的速度是非常快的，字典的搜索的时间复杂读为O(1)， 为什么能有这么快呢？在于字典和集合的后台实现。 散列表 Hash table散列表是一个稀疏数组，散列表里面的单元叫做表元 bucket， 在dict的散列表中，每个键值对都占用一个表元，每个表元有两个部分：一个对键值的引用，一个对值的引用。因为所有表元大小一致，可以通过偏移量来读取某个表元。由于是稀疏数组，python会设法保证还有大约三分之一的表元是空的，快要到达这个阈值的时候，会把原有的散列表复制到一个更大的空间里面。 如果要把一个对象放入散列表，那么需要先计算这个元素的散列值 12345map(hash, (0, 1, 2, 3))# [0, 1, 2, 3]map(hash, ("namea", "nameb", "namec", "named"))# [6674681622036885098, -1135453951840843879, 3071659021342785694, 5386947181042036450] 常用构造hash函数的方法构造散列函数有多种方式，比如直接寻址法、数字分析法、平方取中法、折叠法、随机数法、除留余数法等。著名的hash算法: MD5 和 SHA-1 是应用最广泛的Hash算法。 直接寻址法取keyword或keyword的某个线性函数值为散列地址。即H(key)=key或H(key) = a*key + b，当中a和b为常数（这样的散列函数叫做自身函数） 数字分析法分析一组数据，比方一组员工的出生年月日，这时我们发现出生年月日的前几位数字大体同样，这种话，出现冲突的几率就会非常大，可是我们发现年月日的后几位表示月份和详细日期的数字区别非常大，假设用后面的数字来构成散列地址，则冲突的几率会明显减少。因此数字分析法就是找出数字的规律，尽可能利用这些数据来构造冲突几率较低的散列地址。 平方取中法取keyword平方后的中间几位作为散列地址。 折叠法将keyword切割成位数同样的几部分，最后一部分位数能够不同，然后取这几部分的叠加和（去除进位）作为散列地址。 随机数法选择一随机函数，取keyword的随机值作为散列地址，通经常使用于keyword长度不同的场合。 除留余数法取keyword被某个不大于散列表表长m的数p除后所得的余数为散列地址。即 H(key) = key MOD p, p&lt;=m。不仅能够对keyword直接取模，也可在折叠、平方取中等运算之后取模。对p的选择非常重要，一般取素数或m，若p选的不好，easy产生同义词。 散列表算法为了获取my_dict[search_key] 的值， Python会调用hash(search_key) 来计算散列值，把这个值的最低几位数字当做偏移量，在散列表里面查找表元，如果摘到的表元是空的，则抛出KeyError异常，若不是空的， 表元里会有一对found_key: found_value，然后Python会检查search_key是否等于found_key，如果相等，就返回found_value。 如果search_key和found_key不匹配的话，就叫做散列冲突。 散列冲突解决方法开放寻址法 Open addressingPython是使用开放寻址法中的二次探查来解决冲突的。如果使用的容量超过数组大小的2/3，就申请更大的容量。数组大小较小的时候resize为4，较大的时候resize2。实际上是用左移的形式。 字典的C数据结构下面的C结构体来存储一个字典项，包括散列值、键和值。 12345typedef struct &#123; Py_ssize_t me_hash; PyObject *me_key; PyObject *me_value;&#125; PyDictEntry; 下面的结构代表了一个字典 12345678910typedef struct _dictobject PyDictObject;struct _dictobject &#123; PyObject_HEAD Py_ssize_t ma_fill; Py_ssize_t ma_used; Py_ssize_t ma_mask; PyDictEntry *ma_table; PyDictEntry *(*ma_lookup)(PyDictObject *mp, PyObject *key, long hash); PyDictEntry ma_smalltable[PyDict_MINSIZE];&#125;; ma_fill 是使用了的slots加 dummy slots的数量和。当一个键值对被移除了时，它占据的那个slot会被标记为dummy。如果添加一个新的 key 并且新 key 不属于dummy，则 ma_fill 增加 1 ma_used 是被占用了（即活跃的）的slots数量 ma_mask 等于数组长度减一，它被用来计算slot的索引。在查找元素的一个 key 时，使用 slot = key_hash &amp; mask 就能直接获得哈希槽序号 ma_table 一个 PyDictEntry 结构体的数组， PyDictEntry 包含 key 对象、value 对象，以及 key 的散列值 ma_lookup 一个用于查找 key 的函数指针 ma_smalltable 是一个初始大小为8的数组。 Cpython中，使用如下算法来进行二次探查序列查找空闲slot 123i = (5 * i + perturb + 1)slot_index = i &amp; ma_maskperturb &gt;&gt;= 5 字典的使用字典初始化第一次创建一个字典，PyDict_New()函数会被调用 123456789returns new dictionary objectfunction PyDict_New: allocate new dictionary object clear dictionary&apos;s table set dictionary&apos;s number of used slots + dummy slots (ma_fill) to 0 set dictionary&apos;s number of active slots (ma_used) to 0 set dictionary&apos;s mask (ma_value) to dictionary size - 1 = 7 set dictionary&apos;s lookup function to lookdict_string return allocated dictionary object 添加项当添加一个新键值对时PyDict_SetItem()被调用，该函数带一个指向字典对象的指针和一个键值对作为参数。它检查该键是否为字符串并计算它的hash值（如果这个键的哈希值已经被缓存了则用缓存值）。然后insertdict()函数被调用来添加新的键/值对，如果使用了的slots和dummy slots的总量超过了数组大小的2/3则重新调整字典的大小。 12345678910arguments: dictionary, key, valuereturn: 0 if OK or -1function PyDict_SetItem: if key&apos;s hash cached: use hash else: calculate hash call insertdict with dictionary object, key, hash and value if key/value pair added successfully and capacity orver 2/3: call dictresize to resize dictionary&apos;s table insertdict() 使用查找函数lookdict_string来寻找空闲的slot，这和寻找key的函数是一样的。lookdict_string()``函数利用hash和mask值计算slot的索引，如果它不能在slot索引（=hash &amp; mask）中找到这个key，它便开始如上述伪码描述循环来探测直到找到一个可用的空闲slot。第一次探测时，如果key为空(null)，那么如果找到了dummy slot则返回之 下面一个列子，如何将{‘a’: 1, ‘b’: 2, ‘z’: 26, ‘y’: 25, ‘c’: 5, ‘x’: 24} 键值对添加到字典里面。(字典结构的表大小为8) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748PyDict_SetItem: key=&apos;a&apos;, value = 1 hash = hash(&apos;a&apos;) = 12416037344 insertdict lookdict_string slot index = hash &amp; mask = 12416037344 &amp; 7 = 0 slot 0 is not used so return it init entry at index 0 with key, value and hash ma_used = 1, ma_fill = 1PyDict_SetItem: key=&apos;b&apos;, value = 2 hash = hash(&apos;b&apos;) = 12544037731 insertdict lookdict_string slot index = hash &amp; mask = 12544037731 &amp; 7 = 3 slot 3 is not used so return it init entry at index 3 with key, value and hash ma_used = 2, ma_fill = 2PyDict_SetItem: key=&apos;z&apos;, value = 26 hash = hash(&apos;z&apos;) = 15616046971 insertdict lookdict_string slot index = hash &amp; mask = 15616046971 &amp; 7 = 3 slot 3 is used so probe for a different slot: 5 is free init entry at index 5 with key, value and hash ma_used = 3, ma_fill = 3PyDict_SetItem: key=&apos;y&apos;, value = 25 hash = hash(&apos;y&apos;) = 15488046584 insertdict lookdict_string slot index = hash &amp; mask = 15488046584 &amp; 7 = 0 slot 0 is used so probe for a different slot: 1 is free init entry at index 1 with key, value and hash ma_used = 4, ma_fill = 4PyDict_SetItem: key=&apos;c&apos;, value = 3 hash = hash(&apos;c&apos;) = 12672038114 insertdict lookdict_string slot index = hash &amp; mask = 12672038114 &amp; 7 = 2 slot 2 is not used so return it init entry at index 2 with key, value and hash ma_used = 5, ma_fill = 5PyDict_SetItem: key=&apos;x&apos;, value = 24 hash = hash(&apos;x&apos;) = 15360046201 insertdict lookdict_string slot index = hash &amp; mask = 15360046201 &amp; 7 = 1 slot 1 is used so probe for a different slot: 7 is free init entry at index 7 with key, value and hash ma_used = 6, ma_fill = 6 移除项PyDict_DelItem()被用来删除一个字典项。key的散列值被计算出来作为查找函数的参数，删除后这个slot就成为了dummy slot。 参考Python dictionary implementation Fluent Python]]></content>
      <tags>
        <tag>Python</tag>
        <tag>dict</tag>
        <tag>hash</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MongoDB索引操作]]></title>
    <url>%2F2017%2F06%2F29%2FMongoDB%E7%B4%A2%E5%BC%95%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[上面那篇MongoDB Documentation关于查询优化的案例，数据只有10条，看不出来性能有多少提升，这篇再尝试一个例子。 准备数据首先先插入1千万条测试数据 123456use testfor(var i = 0; i &lt; 10000000; i++)&#123; var rand = parseInt(i * Math.random()); db.person.insert(&#123;name: "hxc"+i, age: i&#125;)&#125; 数据插入需要好久，这个在以后也是个需要优化的地方。 使用 explain函数分析查询性能现在在person这个collection上面没有创建任何索引，这里使用MongoDB提供的 explain工具分析一下一个查询的性能 1db.person.find(&#123; name: "hxc"+10000&#125;).explain() 上面查询使用的是COLLSCAN，就是表扫描，totalDocsExamined为全部1千万，nReturn为1，最终返回一个文档， executionTimeMillisEstimate为4637，预计耗时4637毫秒。 创建索引试一试在name字段上创建一个索引呢？ 12db.person.ensureIndex(&#123; name: 1&#125;)db.person.find(&#123; name: "hxc"+10000&#125;).explain() 再来看看查询的性能，这回使用的是IDXSCAN，mongodb在后台使用B树结构来存放索引，这里使用的索引名字是name_1，只浏览了一个文档就返回这一个文档，executionTimeMillisEstimate预计耗时0毫秒。 note: 在创建索引的时候，会占用一个写锁，如果数据量很大的话，会产生很多问题，所以建议用background方式为大表创建索引。 1db.person.ensureIndex(&#123; name: 1&#125;, &#123;background: 1&#125;) 创建组和索引有时候查询不是单条件的，可能是多条件，这时候可以创建组合索引来加速查询 1234db.person.ensureIndex(&#123; name: 1, birthday: 1&#125;)db.person.ensureIndex(&#123; birthday: 1, name: 1&#125;)db.person.getIndexes() 下面就是创建好的所有索引 其中第一个索引是在创建collection的时候系统自动创建的一个唯一性索引，key值为 _id。 最后两个是刚才所创建的组合索引，这两个组和索引使用的字段虽然是一样的，但是这是两个完全不同的索引 下面分析一下下面的查询使用的到底是哪个索引 1db.person.find(&#123; birthday: "1989-05-01", name: "mary"&#125;).explain() 可以看出，最终优化器选择了使用name_1_birthday_1这个索引。查询优化器会使用我们建立的这些索引来创建查询方案，优化器会从中选择最优的执行查询，同时也可以看到被优化器拒绝的查询计划。当然如果非要用自己指定的查询方案，这也是可以的，在mongodb中给我们提供了hint方法让我们可以暴力执行。]]></content>
      <tags>
        <tag>MongoDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MongoDB 分析查询计划]]></title>
    <url>%2F2017%2F06%2F29%2FMongoDB-%E5%88%86%E6%9E%90%E6%9F%A5%E8%AF%A2%E8%AE%A1%E5%88%92%2F</url>
    <content type="text"><![CDATA[查询计划和传统的关系型数据库的执行计划类似，MongoDB也提供了查询计划。MongoDB的查询优化器处理查询语句，并且从生成的执行计划中选择最优的来执行查询过程。下图显示了MongoDB查询计划的逻辑步骤： 分析查询的性能MongoDB 提供了一个 explain 命令让我们获知系统如何处理查询请求。利用 explain 命令，我们可以很好地观察系统如何使用索引来加快检索，同时可以针对性优化索引。 分析实例准备数据123456789101112db.inventory.insertMany([&#123; "_id" : 1, "item" : "f1", type: "food", quantity: 500 &#125;,&#123; "_id" : 2, "item" : "f2", type: "food", quantity: 100 &#125;,&#123; "_id" : 3, "item" : "p1", type: "paper", quantity: 200 &#125;,&#123; "_id" : 4, "item" : "p2", type: "paper", quantity: 150 &#125;,&#123; "_id" : 5, "item" : "f3", type: "food", quantity: 300 &#125;,&#123; "_id" : 6, "item" : "t1", type: "toys", quantity: 500 &#125;,&#123; "_id" : 7, "item" : "a1", type: "apparel", quantity: 250 &#125;,&#123; "_id" : 8, "item" : "a2", type: "apparel", quantity: 400 &#125;,&#123; "_id" : 9, "item" : "t2", type: "toys", quantity: 50 &#125;,&#123; "_id" : 10, "item" : "f4", type: "food", quantity: 75 &#125;]) 无索引的查询在这个collection没有索引的情况下，写一个查询，查询quantity字段的值在100和200之间的文档 1db.inventory.find(&#123; quantity: &#123; $gte: 100, $lte: 200&#125;&#125;) 这时候，我们想知道这个查询语句所选择的查询计划是怎么样的： 123db.inventory.find( &#123; quantity: &#123;$gte: 100, $lte: 200&#125;&#125;).explain("executionStats") 下图为返回的结果： queryPlanner.winningPlan.stage 显示的是 COLLSCAN集合扫描，也就是关系型数据库的全表扫描，看到这个说明性能肯定不好 nReturned为3，符合的条件的返回为3条 totalKeysExamined为0，没有使用index。 totalDocsExamined为10，扫描了所有记录。 优化的方向也很明显， 就是如何减少检查的文档数量。 创建索引在quantity字段上创建索引： 1db.inventory.createIndex( &#123;quantity: 1&#125;) 再次执行上面的命令查看查询计划 queryPlanner.winningPlan.inputStage.stage 显示IXSCAN说明使用了索引 executionStats.nReturned 有3条文档符合条件返回 executionStats.totalKeysExamined 扫描了3个索引 executionStats.totalDocsExamined 一共扫描了3个文档 参考资料： MongoDB Documentation: Analyze Query Performance]]></content>
      <tags>
        <tag>MongoDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MongoDB CRUD 快速复习]]></title>
    <url>%2F2017%2F06%2F28%2FMongoDB-CRUD-%E5%BF%AB%E9%80%9F%E5%A4%8D%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[提到数据库的基本操作，无论关系型还是非关系型，首先想到的肯定是数据的增删改查 (Create, Read, Update, Delete)，下面记录一下MongoDB里面的CRUD操作。 INSERT 操作 MongoDB里面数据叫做文档Document，其实就是JSON对象，常见的插入操作分为单条插入 insertOne() 和多条插入 insertMany() ，单条插入传入一个JSON对象，多条插入传入一个多个JSON对象的数组 FIND 操作 Query操作是这4中操作最重要的部分， 查询的语法如下： query criteria，可选。表示集合的查询条件。可指定条件查询参数，或一个空对象({}) projection，可选。表示查询时所要返回的字段，省略此参数将返回全部字段。其格式如下：{ field1: , field2: … } 返回查询文档的游标，即：执行find()方法时其返回的文档，实际是对文档引用的一个游标。当指定projection参数时，返回值仅包含指定的字段和_id字段，也可以指定不返回_id字段 查询参数下面这些在SQL的WHERE子句中的操作符，在MongoDB中都有实现。 SQL : 12&gt;, &gt;=, &lt;, &lt;=, !=And，OR，In，NotIn MongoDB: 12&quot;$gt&quot;, &quot;$gte&quot;, &quot;$lt&quot;, &quot;$lte&quot;, &quot;$ne&quot;&quot;$and&quot;, &quot;$or&quot;, &quot;$in&quot;，&quot;$nin&quot; 举几个查询的例子： 1234567891011121314db.inventory.find( &#123; status: &#123; $in: [ "A", "D" ] &#125; &#125; )# SELECT * FROM inventory WHERE status in ("A", "D")db.inventory.find( $and: [ &#123; status: "A", qty: &#123; $lt: 30 &#125; &#125; ] )# SELECT * FROM inventory WHERE status = "A" AND qty &lt; 30db.inventory.find( &#123; $or: [ &#123; status: "A" &#125;, &#123; qty: &#123; $lt: 30 &#125; &#125; ] &#125; )# SELECT * FROM inventory WHERE status = "A" OR qty &lt; 30db.inventory.find( &#123; status: "A", $or: [ &#123; qty: &#123; $lt: 30 &#125; &#125;, &#123; item: /^p/ &#125; ]&#125; )# SELECT * FROM inventory WHERE status = "A" AND ( qty &lt; 30 OR item LIKE "p%") 如果查询使用的是嵌套的文档的属性，那就使用 &quot;field.nestedField&quot; UPDATE 操作UPDATE 的语法如下： 12345678910db.collection.update( &lt;query&gt;, &lt;update&gt;, &#123; upsert: &lt;boolean&gt;, multi: &lt;boolean&gt;, writeConcern: &lt;document&gt;, collation: &lt;document&gt; &#125;) 更新操作分为整体更新和局部更新，整体更新是用一个新的文档完全替代匹配的文档。 危险： 使用替换更新时应当注意，如果查询条件匹配到多个文档，所有的文档都会被替换 下面主要说的是局部更新。 修改器现在有个文档products12345678910&#123; _id: 100, sku: "abc123", quantity: 250, instock: true, reorder: false, details: &#123; model: "14Q2", make: "xyz" &#125;, tags: [ "apparel", "clothing" ], ratings: [ &#123; by: "ijk", rating: 4 &#125; ]&#125; $set修改器 $set修改器用于指定一个字段的值，字段不存在时，则会创建字段。 修改错误或不在需要的字段，可以使用$unset方法将这个键删除 现在要把文档中 details.make字段的值更新为”zzz” 123456db.products.update( &#123; _id: 100&#125;, &#123; $set: &#123;"details.make": "zzz"&#125;, $currentDate: &#123;lastModified: true&#125; &#125;) 其中，$set操作符将details.make字段的值更新为zzz， $currentDate 操作符用来更新lastModified字段的值为当前时间，如果该字段不存在，$currentDate 操作符将会创建这个字段 $inc修改器 $inc修改器用于字段值的增加和减少 products文档如下：123456789&#123; _id: 1, sku: "abc123", quantity: 10, metrics: &#123; orders: 2, ratings: 3.5 &#125;&#125; 将quantity字段的值减2，并且将orders的值加1 1234db.products.update( &#123; sku: "abc123"&#125;, &#123; $inc: &#123;quiantity: -2, "metrics.orders": 1 &#125;&#125;) DELETE 操作危险：remove中如果不带参数将删除所有数据 下面附的是 MongoDB 的速查手册]]></content>
      <tags>
        <tag>MongoDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MongoDB 基础知识]]></title>
    <url>%2F2017%2F06%2F27%2FMongoDB-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[MongoDB 是一个基于 分布式文件存储 的数据库。由 C++ 语言编写。 MongoDB 相关概念mongodb中基本的概念是文档、集合、数据库，和传统关系型数据库相关概念的对应关系如下表： RDBMS MongoDB 描述 database database 数据库 table collection 数据库表/集合 row document 数据记录行/文档 column field 数据字段/域 index index 索引 table joins MongoDB可以使用DbRef或者$lookup实现 primary key primary key 主键,MongoDB自动将_id字段设置为主键 数据存储————文档文档是一个键值(key-value)对(即BSON)类似JSON对象。MongoDB 的文档不需要设置相同的字段，并且相同的字段不需要相同的数据类型，这与关系型数据库有很大的区别，也是 MongoDB 非常突出的特点。 MongoDB的主键 _idMongoDB默认会为每个document生成一个 _id 属性，作为默认主键，且默认值为ObjectId,可以更改 _id 的值(可为空字符串)，但每个document必须拥有 _id 属性。这个字段是为了保证文档的唯一性。 MongoDB系统保留数据库 admin local config 时间数据类型MongoDB中的时间类型默认是MongoDate，MongoDate默认是按照UTC（世界标准时间）来存储。例如下面的两种使用方式: 1234567891011121314db.col.insert(&#123;"date": new Date(), num: 1&#125;)db.col.insert(&#123;"date": new Date().toLocaleString(), num: 2&#125;)db.col.find()&#123; "_id" : ObjectId("539944b14a696442d95eaf08"), "date" : ISODate("2014-06-12T06:12:01.500Z"), "num" : 1&#125;&#123; "_id" : ObjectId("539944b14a696442d95eaf09"), "date" : "Thu Jun 12 14:12:01 2014", "num" : 2&#125; note: 第一条数据存储的是一个Date类型，第二条存储存储的是String类型。两条数据的时间相差大约8个小时（忽略操作时间），第一条数据MongoDB是按照UTC时间来进行存储。 MongoDB中的一对多、多对多关系MongoDB的基本单元是Document（文档），通过文档的嵌套（关联）来组织、描述数据之间的关系。例如我们要表示一对多关系，在关系型数据库中我们通常会设计两张表A（一）、B（多），然后在B表中存入A的主键，以此做关联关系。然后查询的时候需要从两张表分别取数据。MongoDB中的Document是通过嵌套来描述数据之间的关系，例如： 1234567891011121314151617&#123; _id:ObjectId("akdjfiou23o4iu23oi5jktlksdjfa") teacherName: "foo", students: [ &#123; stuName: "foo", totalScore：100， otherInfo :[] ... &#125;,&#123; stuName: "bar", totalScore：90， otherInfo :[] ... &#125; ]&#125; 一次查询便可得到所有老师和同学的对应关系。 内嵌文档查询在MongoDB中文档的查询是与顺序有关的。例如： 1234567&#123; "address" : &#123; "province" : "河北省", "city" : "石家庄" &#125;, "number" : 2640613&#125; 要搜索province为“河北省”、city为“石家庄”可以这样: 12345678db.col.find( &#123; "address":&#123; "city" : "石家庄", "province" : "河北省" &#125; &#125;) 然而这样什么都不会查询到。事实上，这样的查询MongoDB会当做全量匹配查询，即document中所有属性与查询条件全部一致时才会被返回。当然这里的“全部一致”也包括属性的顺序。那么，上面的查询如果想搜索到之前的应该先补充number属性，然后更改address属性下的顺序。 在实际应用中我们当然不会这么来查询文档，尤其是需要查询内嵌文档的时候。MongoDB中提供”.”（点）表示法来查询内嵌文档。因此，上面的查询可以这样写： 12345db.col.find( &#123; "address.privince":"河北省" &#125;)]]></content>
      <tags>
        <tag>MongoDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SQL Server Slide Window partition]]></title>
    <url>%2F2017%2F06%2F22%2FSQL-Server-Slide-Window-partition%2F</url>
    <content type="text"><![CDATA[上篇文章详细介绍了表分区的概念和如何创建分区表。 一个分区表创建好之后，随着数据量的逐渐增长，历史数据越来越不受待见：查询和更新频率越来越低，那么把这些历史数据进行归档就十分必要了。如果使用常规的SELECT、INSERT、DELETE进行历史数据归档的话，会出现以下的一系列问题： DELETE效率太低 由于DELETE单个语句是一个事务性的语句，要么全部成功，要么全部失败。那么可想如果删除的是亿级别的数据，那么日志增长,IO负荷非常的大。 迁移过程表会被锁住，这样可能会出现死锁，一旦迁移失败，又会造成更大的IO问题。 这时候，在SQL Server中分区表有一个非常实用的语句 ALTER TABLE …SWITCH，这个DDL可以快速的将同文件组的表的某个分区迅速的转移到另外的表。这个是利用数据的位置偏移量的指针的转移到新表的方法来实现的。这种方案转移数据非常快。 滑动窗口 Sliding Window滑动窗口是为了在一个分区表上维持固定分区的方法，当新数据来的时候，创建新的数据分区、老的分区被移出分区表被删除或者被归档。由于滑动窗口操作在SQL Server中是元数据操作，所以速度会非常快。 下面以AdventureWorks数据库中表FactResellerSales出发，从0开始将该表进行分区，并且以滑动窗口的方式进行动态分区的维护并归档历史数据。 Table Partition实践创建测试数据库这个测试数据库有两个文件组，PRIMARY文件组在E盘，SECONDARY文件组在D盘，用于归档数据用。 12345678910111213141516CREATE DATABASE [PartitionDB]CONTAINMENT = NONEON PRIMARY(NAME = N'PartitionDB', FILENAME = N'E:\SQLServerData\PartitionDB.mdf', SIZE = 10240KB, FILEGROWTH = 1024KB ),FILEGROUP [SECONDARY](NAME = N'PartitionDBArchive', FILENAME = N'D:\SQLServerData\PartitionDBArchive.ndf', SIZE = 4096KB, FILEGROWTH = 1024KB )LOG ON(NAME = N'PartitionDB_log', FILENAME = N'E:\SQLServerData\PartitionDB_log.ldf', SIZE = 1024KB, FILEGROWTH = 10%)GOUSE [PartitionDB]GOIF NOT EXISTS (SELECT name FROM sys.filegroups WHERE is_default=1 AND name = N'PRIMARY') ALTER DATABASE [PartitionDB] MODIFY FILEGROUP [PRIMARY] DEFAULTGO 创建测试数据表以AdventureWorks的FactResellerSales表为例 12Select * INTO FactResellerSalesFROM [AdventureWorksDW2014].[dbo].[FactResellerSales] 在测试表数据导入之后，查看一下当前的表分区情况 12345SELECT o.name objectname,i.name indexname, partition_id, partition_number, [rows]FROM sys.partitions pINNER JOIN sys.objects o ON o.object_id=p.object_idINNER JOIN sys.indexes i ON i.object_id=p.object_id and p.index_id=i.index_idWHERE o.name LIKE '%FactResellerSales%' 如下图： 可以看见，现在这个表有一个默认的分区，并且全部数据都在这个分区里面。 在现有表上创建分区现在要在这个表上创建两个分区，根据OrderDate字段分为两个分区，一个小于2016-01-01，另外分区的数据大于等于这个时间(2016-01-01的数据分配到右面的分区)。按照上面文章写的步骤创建分区 步骤1： 创建分区函数 123CREATE PARTITION FUNCTION [myPartitionRange] (DATETIME) AS RANGE RIGHT FOR VALUES ('2016-01-01')GO 步骤2：创建分区方案 在上面的两个文件组(PRIMARY, SECONDARY)上创建分区方案 123CREATE PARTITION SCHEME myPartitionScheme AS PARTITION [myPartitionRange] TO ([SECONDARY],[PRIMARY]) 步骤3：在表上创建聚集索引并将分区方案作用在该字段上 这里由于要根据字段 OrderDate 进行分区并归档，所以在该字段上创建聚集索引。 12345CREATE CLUSTERED INDEX IX_FactResellerSales_OrderDate ON FactResellerSales (OrderDate) WITH (STATISTICS_NORECOMPUTE = OFF, IGNORE_DUP_KEY = OFF, ALLOW_ROW_LOCKS = ON, ALLOW_PAGE_LOCKS = ON) ON myPartitionScheme(OrderDate) -- AssignPartitionScheme 在创建好索引并应用分区方案后，再查看一下现在的表分区状态 12345SELECT o.name objectname,i.name indexname, partition_id, partition_number, [rows]FROM sys.partitions pINNER JOIN sys.objects o ON o.object_id=p.object_idINNER JOIN sys.indexes i ON i.object_id=p.object_id and p.index_id=i.index_idWHERE o.name LIKE '%FactResellerSales%' 可以看见，现在有两个分区：1、2，其中全部数据都在第一个分区里面，一共有60855条，也就是说60855条销售数据的OrderDate &lt; ‘2016-01-01’ 测试新数据现在分区已经创建好，向这个表插入三条新的2016-01-01之后的数据 1234567891011INSERT INTO dbo.FactResellerSales(ProductKey, OrderDateKey, DueDateKey, ShipDateKey, ResellerKey, EmployeeKey, PromotionKey, CurrencyKey ,SalesTerritoryKey , SalesOrderNumber, SalesOrderLineNumber, RevisionNumber, OrderQuantity, UnitPrice, ExtendedAmount, UnitPriceDiscountPct, DiscountAmount , ProductStandardCost, TotalProductCost, SalesAmount, TaxAmt, Freight, CarrierTrackingNumber, CustomerPONumber, OrderDate, DueDate, ShipDate)VALUES(592, 20160101, 20160101, 20160101, 490, 281, 16, 100, 4, 'SO71952', 42, 1, 3, 20, 60, 0, 0, 50, 60, 2, 0, 0, '9490-4552-81', 'PO9715163911', '2016-01-01 00:00:00.000', '2016-01-01 00:00:00.000', '2016-01-01 00:00:00.000'), (592, 20160101, 20160101, 20160101, 490, 281, 16, 100, 4, 'SO71952', 42, 1, 3, 20, 60, 0, 0, 50, 60, 2, 0, 0, '9490-4552-81', 'PO9715163911', '2016-01-02 00:00:00.000', '2016-01-02 00:00:00.000', '2016-01-02 00:00:00.000'), (592, 20160101, 20160101, 20160101, 490, 281, 16, 100, 4, 'SO71952', 42, 1, 3, 20, 60, 0, 0, 50, 60, 2, 0, 0, '9490-4552-81', 'PO9715163911', '2016-01-03 00:00:00.000', '2016-01-03 00:00:00.000', '2016-01-03 00:00:00.000') 在观察一下分区的情况，发现新插入的3条数据都在第二个分区里面 滑动窗口假设销售数据到了2017年，现在的任务就是把现有的两个分区合并，2016年以及以前的数据放到老的分区里面(D盘上面的那个文件组中)，2017年的数据插入到新的分区里面。 在split partition之前，必须使用alter partition scheme 指定一个NEXT USED FileGroup。如果Partiton Scheme没有指定 next used filegroup，那么alter partition function split range command 执行失败 12345678910111213141516171819202122232425262728DECLARE @CurrentYear DATETIME='2017-01-01'DECLARE @PrevMax DATETIME=(SELECT CONVERT(DATETIME,Value) FROM sys.partition_functions f INNER JOIN sys.partition_range_values r ON f.function_id = r.function_id WHERE f.name = 'myPartitionRange')IF @PrevMax&lt;@CurrentYearBEGIN TRYBEGIN TRAN-- Merge Old PartitionsALTER PARTITION FUNCTION myPartitionRange()MERGE RANGE (@PrevMax)-- Assign NEXT USED filegroupALTER PARTITION SCHEME myPartitionSchemeNEXT USED [PRIMARY]-- Split partitionALTER PARTITION FUNCTION myPartitionRange()SPLIT RANGE (@CurrentYear)COMMIT TRANPRINT 'COMITIINGGGGGG'END TRYBEGIN CATCHIF @@TRANCOUNT&gt;0ROLLBACK TRANEND CATCH 执行完上面的步骤后，你会发现，2016年的那3条数据也被合并到第一个分区里面了，新的分区2数据为空，留给2017年 再插入3条2017年的数据试验一下 1234567891011INSERT INTO dbo.FactResellerSales(ProductKey, OrderDateKey, DueDateKey, ShipDateKey, ResellerKey, EmployeeKey, PromotionKey, CurrencyKey, SalesTerritoryKey,SalesOrderNumber, SalesOrderLineNumber, RevisionNumber, OrderQuantity, UnitPrice, ExtendedAmount, UnitPriceDiscountPct, DiscountAmount, ProductStandardCost, TotalProductCost, SalesAmount, TaxAmt, Freight, CarrierTrackingNumber, CustomerPONumber, OrderDate, DueDate, ShipDate)VALUES(592, 20170101, 20170101, 20170101, 490, 281, 16, 100, 4, 'SO71952', 42, 1, 3, 20, 60, 0, 0, 50, 60, 2, 0, 0, '9490-4552-81', 'PO9715163911', '2017-01-01 00:00:00.000', '2017-01-01 00:00:00.000', '2017-01-01 00:00:00.000'), (592, 20170102, 20170102, 20170102, 490, 281, 16, 100, 4, 'SO71952', 42, 1, 3, 20, 60, 0, 0, 50, 60, 2, 0, 0, '9490-4552-81', 'PO9715163911', '2017-01-02 00:00:00.000', '2017-01-02 00:00:00.000', '2017-01-02 00:00:00.000'), (592, 20170103, 20170103, 20170103, 490, 281, 16, 100, 4, 'SO71952', 42, 1, 3, 20, 60, 0, 0, 50, 60, 2, 0, 0, '9490-4552-81', 'PO9715163911', '2017-01-03 00:00:00.000', '2017-01-03 00:00:00.000', '2017-01-03 00:00:00.000') 现在，2016年以及更久之前的数据保存在分区1中，文件组存储在D盘上，而2017年最新的数据在分区2中，存储在E盘上，这样，就把新老数据在存储上分开，利用更好存储设备的性能查询和更新操作等。而对于查询或者操作这个表的用户来说，逻辑上这还是一张表。 当然，还有另外一种方式进行老数据的归档操作，那就是两张物理表，一张仅存储最新数据，另一张存储归档数据]]></content>
      <tags>
        <tag>SQL Server</tag>
        <tag>Database</tag>
        <tag>表分区</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SQL Server表分区]]></title>
    <url>%2F2017%2F06%2F21%2FSQL-Server%E8%A1%A8%E5%88%86%E5%8C%BA%2F</url>
    <content type="text"><![CDATA[某种特定业务下，部分业务数据可能只保留比较短的时间，例如每日的流量日志数据，可能每天亿级别的数据增长，随着数据量的逐渐增长，你会发现数据库能性能越来越慢，查询速度会明显变慢，而这时要想提高数据库的查询速度，你肯定会想到索引这种方式，但是随着索引的引入，数据的插入和更新也会变慢，因为在数据插入的时候，索引也是需要重建的。那怎么办呢？ (其实我觉得这种应用场景更好的解决方法是用流式处理的方式) 一个最简单的解决方法就是把一个大表拆分成多个小表，这个就叫做表分区，表分区有两种： 水平分区 (行级) 垂直分区 (列级) 下面主要说的是水平分区。 表分区有以下优点： 改善查询性能：对分区对象的查询可以仅搜索自己关心的分区，提高检索速度。 增强可用性：如果表的某个分区出现故障，表在其他分区的数据仍然可用； 维护方便：如果表的某个分区出现故障，需要修复数据，只修复该分区即可； 均衡I/O：可以把不同的分区映射到磁盘以平衡I/O，改善整个系统性能。 分区表是把数据按某种标准划分成区域存储在不同的文件组中，使用分区可以快速而有效地管理和访问数据子集，从而使大型表或索引更易于管理。合理的使用分区会很大程度上提高数据库的性能。 创建分区需要如下个步骤： 创建文件组 创建分区函数 创建分区方案 创建或者修改使用分区方案的表 举一个按照时间分区的案例： 确定分区键列的类型(DATETIME)以及分区的边界值: 2011-01-01 2012-01-01 2013-01-01 N个边界值确定 N+1 个分区 创建文件组T-SQL语法 1alter database &lt;数据库名&gt; add filegroup &lt;文件组名&gt; 下面创建4个分区文件组 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960USE AdventureWorksDW2014; GO -- Adds four new filegroups to the AdventureWorksDW2014 database ALTER DATABASE AdventureWorksDW2014 ADD FILEGROUP test1fg; GO ALTER DATABASE AdventureWorksDW2014 ADD FILEGROUP test2fg; GO ALTER DATABASE AdventureWorksDW2014 ADD FILEGROUP test3fg; GO ALTER DATABASE AdventureWorksDW2014 ADD FILEGROUP test4fg; -- Adds one file for each filegroup. ALTER DATABASE AdventureWorksDW2014 ADD FILE ( NAME = test1dat1, FILENAME = 'E:\FileGroupsData\t1dat1.ndf', SIZE = 5MB, MAXSIZE = 100MB, FILEGROWTH = 5MB ) TO FILEGROUP test1fg; ALTER DATABASE AdventureWorksDW2014 ADD FILE ( NAME = test2dat2, FILENAME = 'E:\FileGroupsData\t2dat2.ndf', SIZE = 5MB, MAXSIZE = 100MB, FILEGROWTH = 5MB ) TO FILEGROUP test2fg; GO ALTER DATABASE AdventureWorksDW2014 ADD FILE ( NAME = test3dat3, FILENAME = 'E:\FileGroupsData\t3dat3.ndf', SIZE = 5MB, MAXSIZE = 100MB, FILEGROWTH = 5MB ) TO FILEGROUP test3fg; GO ALTER DATABASE AdventureWorksDW2014 ADD FILE ( NAME = test4dat4, FILENAME = 'E:\FileGroupsData\t4dat4.ndf', SIZE = 5MB, MAXSIZE = 100MB, FILEGROWTH = 5MB ) TO FILEGROUP test4fg; GO 执行上面的脚本，可以创建4个文件组 使用样例数据库的FactResellerSales表做一个分区的实验 123456IF OBJECT_ID('dbo.SalesOrders')IS NOT NULLDROP TABLE dbo.SalesOrdersGOSELECT * INTO SalesOrdersFROM [AdventureWorksDW2014].[dbo].[FactResellerSales] 创建分区函数指定分依据区列（依据列唯一），分区数据范围规则，分区数量，然后将数据映射到一组分区上。 创建语法： 12create partition function 分区函数名(&lt;分区列类型&gt;) as range [left/right]for values (每个分区的边界值,....) 1234567CREATE PARTITION FUNCTION PF_Orders_OrderDateRange(DATETIME) AS RANGE RIGHT FOR VALUES ( '2011-01-01', '2012-01-01', '2013-01-01' ) 左边界/右边界：就是把临界值划分给上一个分区还是下一个分区。这里2013-01-01就属于下一个分区 注意：只有没有应用到分区方案中的分区函数才能被删除。 创建分区方案指定分区对应的文件组。 创建语法： 12-- 创建分区方案语法create partition scheme &lt;分区方案名称&gt; as partition &lt;分区函数名称&gt; [all]to (文件组名称,....) 1234CREATE PARTITION SCHEME PS_Orders AS PARTITION PF_Orders_OrderDateRange TO (test1fg, test2fg, test3fg, test4fg) ;GO 注意：只有没有分区表，或索引使用该分区方案是，才能对其删除。 创建使用分区方案的表创建语法： 1234--创建分区表语法create table &lt;表名&gt; ( &lt;列定义&gt;)on&lt;分区方案名&gt;(分区列名) 下面在已有的表SalesOrders上面应用分区方案，并在OrderDate字段上创建聚集索引 123456CREATE CLUSTERED INDEX IX_FactResellerSales_OrderDate ON dbo.SalesOrders (OrderDate) WITH (STATISTICS_NORECOMPUTE = OFF, IGNORE_DUP_KEY = OFF, ALLOW_ROW_LOCKS = ON, ALLOW_PAGE_LOCKS = ON) ON PS_Orders(OrderDate) -- AssignPartitionScheme -- 这里使用[PS_Orders]分区方案，根据OrderDate列进行分区 在创建分区表后，需要创建聚集分区索引 根据订单表Orders 查询时经常使用OrderDate 范围条件来查询的特点，我们最好在Orders.OrderDate 列上建立聚集索引（clustered index）。为了便于进行分区切换（partition swtich)。大多数情况下，建议在分区表上建立分区索引。 查询分区表信息查看分区依据列的指定值所在的分区123-- 查询分区依据列为'2013-01-01'的数据在哪个分区上SELECT $partition.PF_Orders_OrderDateRange('2013-01-01') -- 返回值是3，表示此值存在第2个分区 查询每个非空分区存在的行数12345-- 查看分区表中，每个非空分区存在的行数SELECT $partition.PF_Orders_OrderDateRange(OrderDate) as partitionNum , count(*) as recordCountFROM dbo.SalesOrdersGROUP BY $partition.PF_Orders_OrderDateRange(OrderDate) 查询各个分区的数据信息1234567SELECT PARTITION = $PARTITION.PF_Orders_OrderDateRange(OrderDate), ROWS = COUNT(*), MinVal = MIN(OrderDate), MaxVal = MAX(OrderDate)FROM [dbo].[SalesOrders]GROUP BY $PARTITION.PF_Orders_OrderDateRange(OrderDate)ORDER BY PARTITION 分区的拆分合并拆分分区在分区函数中新增一个边界值，即可将一个分区变为2个。 123--分区拆分alter partition function PF_Orders_OrderDateRange()split range('2014-01-01') 注意：如果分区函数已经指定了分区方案，则分区数需要和分区方案中指定的文件组个数保持对应一致。 合并分区与拆分分区相反，去除一个边界值即可。 123--合并分区alter partition function PF_Orders_OrderDateRange()merge range('2011-01-01') 分区数据移动分区数据移动可以使用 ALTER TABLE ....... SWITCH 语句快速有效地移动数据子集： 将某个表中的数据移动到另一个表中； 将某个表作为分区添加到现存的已分区表中； 将分区从一个已分区表切换到另一个已分区表； 删除分区以形成单个表。 切换分区表的一个分区到普通数据表创建普通表SalesOrder_2012用于存放订单日期为2012年的所有数据。从分区到普通表的切换，最好满足以下条件： 普通表必须建立在分区表切换出的分区所在的文件组上 普通表的表结构和分区表一致 普通表上的索引要和分区表一致，包括聚集索引和非聚集索引 普通表必须是空表 切换分区为3的数据从分区表到归档表 12ALTER TABLE dbo.SalesOrders SWITCH PARTITION 3TO dbo.SalesOrders_2012 现在再查看一下分区表的分区状态，分区为3的数据已经不在了 切换普通表数据到分区表的一个分区中下面要把上面已经归档的2012年的数据切换回来 按照上面的那种写法先试一下： 12ALTER TABLE dbo.SalesOrders_2012 SWITCH TOdbo.SalesOrders PARTITION 3 这时候会遇到错误 1234Msg 4982, Level 16, State 1, Line 1ALTER TABLE SWITCH statement failed.Check constraints of source table &apos;AdventureWorksDW2014.dbo.SalesOrders_2012&apos;allow values that are not allowed by range defined by partition 3 on target table &apos;AdventureWorksDW2014.dbo.SalesOrders&apos;. 这是因为表dbo.SalesOrders 的数据经过分区函数的分区列定义, 各个分区的数据实际上已经经过了数据约束检查，符合分区边界范围(Range)的数据才会录入到各个分区中。但是在存档表dbo.SalesOrders_2012中的数据实际上是没有边界约束的，比如完全可以手动的插入一条其他年的数据，所以进行SWITCH时肯定是不会成功的，这时候需要增加一个数据约束检查 12ALTER TABLE dbo.SalesOrders_2012 ADD CONSTRAINT CK_SalesOrders_OrderDateCHECK(OrderDate&gt;='2012-01-01' AND OrderDate&lt;'2013-01-01') 这时候再SWITCH，2012年扥分区数据就会到了分区表中。 切换分区表数据到分区表新的存档分区表在结构上和源分区表是一致的，包括分区函数和分区方案，但是需要重新创建，不能简单地直接使用dbo.SalesOrders 表上的分区函和分区方案，因为他们之间有绑定关系 创建分区函数和分区方案 123456789101112131415161718192021IF EXISTS (SELECT * FROM sys.partition_schemes WHERE name = 'PS_SalesOrdersArchive')DROP PARTITION SCHEME PS_SalesOrdersArchiveGOIF EXISTS (SELECT * FROM sys.partition_functions WHERE name = 'PF_SalesOrdersArchive_OrderDateRange')DROP PARTITION FUNCTION PF_SalesOrdersArchive_OrderDateRangeGOCREATE PARTITION FUNCTION PF_SalesOrdersArchive_OrderDateRange(DATETIME)AS RANGE RIGHT FOR VALUES( '2011-01-01', '2012-01-01', '2013-01-01')GOCREATE PARTITION SCHEME PS_SalesOrdersArchiveAS PARTITION PF_SalesOrdersArchive_OrderDateRangeTO (test1fg, test2fg, test3fg, test4fg)GO 创建归档表 1234567891011CREATE TABLE [dbo].[SalesOrdersArchive]( [ProductKey] [int] NOT NULL, [OrderDateKey] [int] NOT NULL, ... [OrderDate] [datetime] NOT NULL, [DueDate] [datetime] NULL, [ShipDate] [datetime] NULL) ON PS_SalesOrdersArchive(OrderDate)CREATE CLUSTERED INDEX IXC_SalesOrdersArchive_OrderDate ON dbo.SalesOrdersArchive(OrderDate) 切换分区到归档表 123ALTER TABLE dbo.SalesOrders SWITCH PARTITION 1 TO dbo.SalesOrdersArchive PARTITION 1ALTER TABLE dbo.SalesOrders SWITCH PARTITION 2 TO dbo.SalesOrdersArchive PARTITION 2ALTER TABLE dbo.SalesOrders SWITCH PARTITION 3 TO dbo.SalesOrdersArchive PARTITION 3 切换完成后，观察一下原表和归档表的分区数据状况： 原表： 归档表： 总结分区表分区切换并没有真正去移动数据,而是SQL Server 在系统底层改变了表的元数据。因此分区表分区切换是高效、快速、灵活的。利用分区表的分区切换功能，我们可以快速加载数据到分区表、卸载分区数据到普通表，然后TRUNCATE普通表，以实现快速删除分区表数据，快速归档不活跃数据到历史表。 表分区的相关概念和实际操作就介绍到这儿，下一篇重点介绍一下如何实现表分区随着时间窗口的移动而自动维护。]]></content>
      <tags>
        <tag>SQL Server</tag>
        <tag>Database</tag>
        <tag>表分区</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[堆栈和队列]]></title>
    <url>%2F2017%2F06%2F13%2F%E5%A0%86%E6%A0%88%E5%92%8C%E9%98%9F%E5%88%97%2F</url>
    <content type="text"><![CDATA[栈 (Stack)是一种后进先出(last in first off，LIFO)的数据结构 队列(Queue)则是一种先进先出 (fisrt in first out，FIFO)的结构 栈12345678910111213141516171819202122232425262728class Stack: def __init__(self): self.items = [] def is_empty(self): return not self.items def push(self, item): """adds a new item to the top of the stack""" self.items.append(item) def pop(self): """ removes the top item from the stack, popping an empty stack (list) will result in an error """ if not self.is_empty(): return self.items.pop() return "Pop from empty stack" def peek(self): """returns the top item from the stack but does not remove it""" if not self.is_empty(): return self.items[len(self.items) - 1] return "Stack is Empty" def size(self): return len(self.items) 队列]]></content>
      <tags>
        <tag>Algorithm</tag>
        <tag>Data Structure</tag>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数组和链表]]></title>
    <url>%2F2017%2F06%2F13%2F%E6%95%B0%E7%BB%84%E5%92%8C%E9%93%BE%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[需要在内存中存储多项数据时，有两种基本方式：数组和链表 由于数组是连续存储的，在操作数组中的数据时就可以根据离首地址的偏移量直接存取相应位置上的数据，但是如果要在数据组中任意位置上插入一个元素，就需要先把后面的元素集体向后移一位为其空出存储空间。与之相反，链表是离散存储的，所以在插入一个数据时只要申请一片新空间，然后将其中的连接关系做一个修改就可以，但是显然在链表上查找一个数据时就要逐个遍历了。 数组的优势在于能够随机访问，而链表只能顺序访问。 链表的优势在于能够以较高的效率在任意位置插入或删除一个节点。 数组 链表 读取 O(1) O(n) 插入 O(n) O(1) 删除 O(n) O(1)]]></content>
      <tags>
        <tag>Algorithm</tag>
        <tag>Data Structure</tag>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[搜索和排序之搜索]]></title>
    <url>%2F2017%2F06%2F12%2F%E6%90%9C%E7%B4%A2%E5%92%8C%E6%8E%92%E5%BA%8F%E4%B9%8B%E6%90%9C%E7%B4%A2%2F</url>
    <content type="text"><![CDATA[未完成 基本概念查找（Searching）就是根据给定的某个值，在查找表中确定一个其关键字等于给定值的数据元素（或记录）。 查找表按照操作方式可分为： 静态查找表（Static Search Table）：只做查找操作的查找表。它的主要操作是： 查询某个“特定的”数据元素是否在表中 检索某个“特定的”数据元素和各种属性 动态查找表（Dynamic Search Table）：在查找中同时进行插入或删除等操作： 查找时插入数据 查找时删除数据 无序表查找在数据不排序的线性查找，遍历数据元素。 算法分析：最好情况是第一个位置就找到了，为O(1)；最坏情况在最后一个位置找到，为O(n)， 平均查找次数为 (n+1)/2, 时间复杂度为O(n) 123456789101112def sequential_search(lst, key): length = len(lst) for i in range(length): if lst[i] == key: return i else: return Falseif __name__ == '__main__': l = [1,5,8,124,22,54,7,99,300,222] result = sequential_search(l, 123) print result 快速选择算法 (quick selection algorithm)快速选择算法能够在平均O(n)时间内从一个无序数组中返回第k大的元素。算法实际上利用了快速排序的思想，将数组依照一个轴值分割成两个部分，左边元素都比轴值小，右边元素都比轴值大。由于轴值下标已知，则可以判断所求元素落在数组的哪一部分，并在那一部分继续进行上述操作，直至找到该元素。与快排不同，由于快速选择算法只在乎所求元素所在的那一部分，所以时间复杂度是O(n)。 12 有序表查找数据按某种方式进行过排序 二分查找 Binary Search算法内容：在查找表中不断取中间元素与查找值进行比较，以二分之一的倍率进行表范围的缩小。时间复杂度：O(logn) 12345678910111213141516def binary_search(lst, key): low, high = 0, len(lst)-1 while low &lt; high: mid = (low + high) / 2 if key &lt; lst[mid]: high = mid - 1 elif key &gt; lst[mid]: low = mid + 1 else: return mid return Falseif __name__ == '__main__': l = [1, 5, 7, 8, 22, 54, 99, 123, 200, 222, 444] result = binary_search(l, 123) print result 插值查找插值查找是二分查找演化而来，相比于二分查找(折半),该算法考虑的是每次折的时候折多少，即不一定是1/2。在二分查找中mid=(low+high)/2=low+1/2*(high-low)，插值查找就是对1/2(系数,或者说比例)进行改变，它将1/2变成 (key - array[low])/(array[high] - array[low]),其实就是计算线性比例。 时间复杂度：O(logn) note: 因为插值查找是依赖线性比例的，如果当前数组分布不是均匀的，那么该算法就不合适。 12345678910111213141516def interpolate_search(lst, key): low, high = 0, len(lst)-1 while low &lt; high: mid = low + (high - low) * (key - lst[low])/(lst[high] - lst[low]) if key &lt; lst[mid]: high = mid - 1 elif key &gt; lst[mid]: low = mid + 1 else: return mid return Falseif __name__ == '__main__': l = [1, 5, 7, 8, 22, 54, 99, 123, 200, 222, 444] result = binary_search(l, 123) print result 斐波那契查找查找算法：在斐波那契数列找一个等于略大于查找表中元素个数的数F(n)，将原查找表扩展为长度为F(n)(如果要补充元素，则补充重复最后一个元素，直到满足数组元素个数为F(n)个元素)，完成后进行斐波那契分割，即F(n)个元素分割为前半部分F(n-1)个元素，后半部分F(n-2)个元素，找出要查找的元素在那一部分并递归，直到找到。时间复杂度：O(logn)，平均性能优于二分查找。 利用斐波那契数列的性质，黄金分割的原理来确定mid的位置 线性索引查找对于海量的无序数据，为了提高查找速度，一般会为其构造索引表。索引就是把一个关键字与它相对应的记录进行关联的过程。一个索引由若干个索引项构成，每个索引项至少包含关键字和其对应的记录在存储器中的位置等信息。 索引按照结构可以分为：线性索引、树形索引和多级索引。线性索引：将索引项的集合通过线性结构来组织，也叫索引表。 线性索引可分为：稠密索引、分块索引和倒排索引 稠密索引分块索引倒排索引二叉排序树二叉排序树又称为二叉查找树。它或者是一颗空树，或者是具有下列性质的二叉树： 若它的左子树不为空，则左子树上所有节点的值均小于它的根结构的值； 若它的右子树不为空，则右子树上所有节点的值均大于它的根结构的值； 它的左、右子树也分别为二叉排序树。 二叉排序树的操作： 查找：对比节点的值和关键字，相等则表明找到了；小了则往节点的左子树去找，大了则往右子树去找，这么递归下去，最后返回布尔值或找到的节点。 插入：从根节点开始逐个与关键字进行对比，小了去左边，大了去右边，碰到子树为空的情况就将新的节点链接。 删除：如果要删除的节点是叶子，直接删；如果只有左子树或只有右子树，则删除节点后，将子树链接到父节点即可；如果同时有左右子树，则可以将二叉排序树进行中序遍历，取将要被删除的节点的前驱或者后继节点替代这个被删除的节点的位置。 二叉排序树总结： 二叉排序树以链式进行存储，保持了链接结构在插入和删除操作上的优点。 在极端情况下，查询次数为1，但最大操作次数不会超过树的深度。也就是说，二叉排序树的查找性能取决于二叉排序树的形状，也就引申出了后面的平衡二叉树。 给定一个元素集合，可以构造不同的二叉排序树，当它同时是一个完全二叉树的时候，查找的时间复杂度为O(log(n))，近似于二分查找。 当出现最极端的斜树时，其时间复杂度为O(n)，等同于顺序查找，效果最差。 平衡二叉树多路查找树 B树2-3树3-4树B树B+树散列表散列表：所有的元素之间没有任何关系。元素的存储位置，是利用元素的关键字通过某个函数直接计算出来的。这个一一对应的关系函数称为散列函数或Hash函数。 采用散列技术将记录存储在一块连续的存储空间中，称为散列表或哈希表（Hash Table）。关键字对应的存储位置，称为散列地址。 散列表是一种面向查找的存储结构。它最适合求解的问题是查找与给定值相等的记录。但是对于某个关键字能对应很多记录的情况就不适用，比如查找所有的“男”性。也不适合范围查找，比如查找年龄20~30之间的人。排序、最大、最小等也不合适。 因此，散列表通常用于关键字不重复的数据结构。比如python的字典数据类型。 设计出一个简单、均匀、存储利用率高的散列函数是散列技术中最关键的问题。但是，一般散列函数都面临着冲突的问题。冲突：两个不同的关键字，通过散列函数计算后结果却相同的现象。collision 散列函数构造好的散列函数：计算简单、散列地址分布均匀 直接定址法 例如取关键字的某个线性函数为散列函数：f(key) = a*key + b (a,b为常数） 数字分析法 抽取关键字里的数字，根据数字的特点进行地址分配 平方取中法 将关键字的数字求平方，再截取部分 折叠法 将关键字的数字分割后分别计算，再合并计算，一种玩弄数字的手段。 除留余数法 最为常见的方法之一。对于表长为m的数据集合，散列公式为：f(key) = key mod p (p&lt;=m)mod：取模（求余数）该方法最关键的是p的选择，而且数据量较大的时候，冲突是必然的。一般会选择接近m的质数。随机数法选择一个随机数，取关键字的随机函数值为它的散列地址。f(key) = random(key) 总结，实际情况下根据不同的数据特性采用不同的散列方法，考虑下面一些主要问题： 计算散列地址所需的时间 关键字的长度 散列表的大小 关键字的分布情况 记录查找的频率 处理散列冲突散列表查找实现散列表查找性能分析]]></content>
      <tags>
        <tag>Algorithm</tag>
        <tag>Data Structure</tag>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[搜索和排序之排序]]></title>
    <url>%2F2017%2F06%2F12%2F%E6%90%9C%E7%B4%A2%E5%92%8C%E6%8E%92%E5%BA%8F%E4%B9%8B%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[未完成 Bubble Sort 冒泡排序冒泡排序的原理非常简单，它重复地走访过要排序的数列，一次比较两个元素，如果他们的顺序错误就把他们交换过来。 步骤： 比较相邻的元素。如果第一个比第二个大，就交换他们两个。 对第0个到第n-1个数据做同样的工作。这时，最大的数就“浮”到了数组最后的位置上。 针对所有的元素重复以上的步骤，除了最后一个。 持续每次对越来越少的元素重复上面的步骤，直到没有任何一对数字需要比较。 12 针对上述代码还有两种优化方案。 Selection Sort 选择排序选择排序无疑是最简单直观的排序。它的工作原理如下。 步骤： 在未排序序列中找到最小（大）元素，存放到排序序列的起始位置。 再从剩余未排序元素中继续寻找最小（大）元素，然后放到已排序序列的末尾。 以此类推，直到所有元素均排序完毕。 12 Insertion Sort 插入排序插入排序的工作原理是，对于每个未排序数据，在已排序序列中从后向前扫描，找到相应位置并插入。 步骤： 从第一个元素开始，该元素可以认为已经被排序 取出下一个元素，在已经排序的元素序列中从后向前扫描 如果被扫描的元素（已排序）大于新元素，将该元素后移一位 重复步骤3，直到找到已排序的元素小于或者等于新元素的位置 将新元素插入到该位置后 重复步骤2~5 12 Shell Sort 希尔排序希尔排序，也称递减增量排序算法，实质是分组插入排序。由 Donald Shell 于1959年提出。希尔排序是非稳定排序算法。 希尔排序的基本思想是：将数组列在一个表中并对列分别进行插入排序，重复这过程，不过每次用更长的列（步长更长了，列数更少了）来进行。最后整个表就只有一列了。将数组转换至表是为了更好地理解这算法，算法本身还是使用数组进行排序。 12 Merge Sort 归并排序归并排序是采用分治法的一个非常典型的应用。归并排序的思想就是先递归分解数组，再合并数组。 先考虑合并两个有序数组，基本思路是比较两个数组的最前面的数，谁小就先取谁，取了后相应的指针就往后移一位。然后再比较，直至一个数组为空，最后把另一个数组的剩余部分复制过来即可。 再考虑递归分解，基本思路是将数组分解成left和right，如果这两个数组内部数据是有序的，那么就可以用上面合并数组的方法将这两个数组合并排序。如何让这两个数组内部是有序的？可以再二分，直至分解出的小组只含有一个元素时为止，此时认为该小组内部已有序。然后合并排序相邻二个小组即可。 12 Quick Sort 快速排序快速排序通常明显比同为Ο(n log n)的其他算法更快，因此常被采用，而且快排采用了分治法的思想，所以在很多笔试面试中能经常看到快排的影子。可见掌握快排的重要性。 步骤： 从数列中挑出一个元素作为基准数。 分区过程，将比基准数大的放到右边，小于或等于它的数都放到左边。 再对左右区间递归执行第二步，直至各区间只有一个数。 12 Heap Sort 堆排序堆排序在 top K 问题中使用比较频繁。堆排序是采用二叉堆的数据结构来实现的，虽然实质上还是一维数组。二叉堆是一个近似完全二叉树 。 二叉堆二叉堆具有以下性质： 父节点的键值总是大于或等于（小于或等于）任何一个子节点的键值。 每个节点的左右子树都是一个二叉堆（都是最大堆或最小堆）。 步骤： 构造最大堆（Build_Max_Heap）：若数组下标范围为0~n，考虑到单独一个元素是大根堆，则从下标n/2开始的元素均为大根堆。于是只要从n/2-1开始，向前依次构造大根堆，这样就能保证，构造到某个节点时，它的左右子树都已经是大根堆。 堆排序（HeapSort）：由于堆是用数组模拟的。得到一个大根堆后，数组内部并不是有序的。因此需要将堆化数组有序化。思想是移除根节点，并做最大堆调整的递归运算。第一次将heap[0]与heap[n-1]交换，再对heap[0…n-2]做最大堆调整。第二次将heap[0]与heap[n-2]交换，再对heap[0…n-3]做最大堆调整。重复该操作直至heap[0]和heap[1]交换。由于每次都是将最大的数并入到后面的有序区间，故操作完后整个数组就是有序的了。 最大堆调整（Max_Heapify）：该方法是提供给上述两个过程调用的。目的是将堆的末端子节点作调整，使得子节点永远小于父节点。 12 Bucket Sort 桶排序Counting Sort 计数排序## 总结 排序方法 平均情况 最好情况 最坏情况 辅助空间 稳定性 冒泡排序 O(n2) O(n) O(n2) O(1) 稳定 选择排序 O(n2) O(n2) O(n2) O(1) 不稳定 插入排序 O(n2) O(n) O(n2) O(1) 稳定 希尔排序 O(nlogn)~O(n2) O(n1.3) O(n2) O(1) 不稳定 堆排序 O(nlogn) O(nlogn) O(nlogn) O(1) 不稳定 归并排序 O(nlogn) O(nlogn) O(nlogn) O(n) 稳定 快速排序 O(nlogn) O(nlogn) O(n2) O(logn)~O(n) 不稳定]]></content>
      <tags>
        <tag>Algorithm</tag>
        <tag>Data Structure</tag>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(转)Python基础知识面试题]]></title>
    <url>%2F2017%2F06%2F12%2FPython%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E9%9D%A2%E8%AF%95%E9%A2%98%2F</url>
    <content type="text"><![CDATA[下面总结了一下常见的、易错的Python面试题 Question 1如下代码的输出是什么 1234567891011def extendList(val, list=[]): list.append(val) return listlist1 = extendList(10)list2 = extendList(123,[])list3 = extendList('a')print "list1 = %s" % list1print "list2 = %s" % list2print "list3 = %s" % list3 答案：123list1 = [10, &apos;a&apos;]list2 = [123]list3 = [10, &apos;a&apos;] 解释The new default list is created only once when the function is defined, and that same list is then used subsequently whenever extendList is invoked without a list argument being specified. This is because expressions in default arguments are calculated when the function is defined, not when it’s called. list1 and list3 are therefore operating on the same default list, whereas list2 is operating on a separate list that it created (by passing its own empty list as the value for the list parameter). The definition of the extendList function could be modified as follows, though, to always begin a new list when no list argument is specified, which is more likely to have been the desired behavior: 123456789def extendList(val, list=None): if list is None: list = [] list.append(val) return list# list1 = [10]# list2 = [123]# list3 = ['a'] Python的函数参数传递1234567891011a = 1def fun(a): a = 2fun(a)print a # 1a = []def fun(a): a.append(1)fun(a)print a # [1] 类型是属于对象的，而不是变量。而对象有两种, 可更改 (mutable) 与 不可更改(immutable)对象。在python中，strings, tuples, 和numbers是不可更改的对象，而list,dict等则是可以修改的对象。当一个引用传递给函数的时候,函数自动复制一份引用,这个函数里的引用和外边的引用没有半毛关系了.所以第一个例子里函数把引用指向了一个不可变对象,当函数返回的时候,外面的引用没半毛感觉.而第二个例子就不一样了,函数内的引用指向的是可变对象,对它的操作就和定位了指针地址一样,在内存里进行修改。 Question 2下面代码输出 1234def multipliers(): return [lambda x : i * x for i in range(4)] print [m(2) for m in multipliers()] The output of the above code will be [6, 6, 6, 6] (not [0, 2, 4, 6]). The reason for this is that Python’s closures are late binding. This means that the values of variables used in closures are looked up at the time the inner function is called. So as a result, when any of the functions returned by multipliers() are called, the value of i is looked up in the surrounding scope at that time. By then, regardless of which of the returned functions is called, the for loop has completed and i is left with its final value of 3. Therefore, every returned function multiplies the value it is passed by 3, so since a value of 2 is passed in the above code, they all return a value of 6 (i.e., 3 x 2). (Incidentally, as pointed out in The Hitchhiker’s Guide to Python, there is a somewhat widespread misconception that this has something to do with lambdas, which is not the case. Functions created with a lambda expression are in no way special and the same behavior is exhibited by functions created using an ordinary def.) Below are a few examples of ways to circumvent this issue. One solution would be use a Python generator as follows: 123def multipliers(): for i in range(4): yield lambda x : i * x Another solution is to create a closure that binds immediately to its arguments by using a default argument. For example: 12def multipliers(): return [lambda x, i=i : i * x for i in range(4)] Or alternatively, you can use the functools.partial function: 12345from functools import partialfrom operator import muldef multipliers(): return [partial(mul, i) for i in range(4)] 迭代器和生成器Question 3123456789101112131415161718class Parent(object): x = 1class Child1(Parent): passclass Child2(Parent): passprint Parent.x, Child1.x, Child2.xChild1.x = 2print Parent.x, Child1.x, Child2.xParent.x = 3print Parent.x, Child1.x, Child2.x# 1, 1, 1# 2, 1, 2# 3, 2, 3 解释in Python, class variables are internally handled as dictionaries. If a variable name is not found in the dictionary of the current class, the class hierarchy (i.e., its parent classes) are searched until the referenced variable name is found (if the referenced variable name is not found in the class itself or anywhere in its hierarchy, an AttributeError occurs). Therefore, setting x = 1 in the Parent class makes the class variable x (with a value of 1) referenceable in that class and any of its children. That’s why the first print statement outputs 1 1 1. Subsequently, if any of its child classes overrides that value (for example, when we execute the statement Child1.x = 2), then the value is changed in that child only. That’s why the second print statement outputs 1 2 1. Finally, if the value is then changed in the Parent (for example, when we execute the statement Parent.x = 3), that change is reflected also by any children that have not yet overridden the value (which in this case would be Child2). That’s why the third print statement outputs 3 2 3. Question 4在 Python2 中下面代码输出是什么 123456789101112131415def div1(x,y): print "%s/%s = %s" % (x, y, x/y) def div2(x,y): print "%s//%s = %s" % (x, y, x//y)div1(5,2)div1(5.,2)div2(5,2)div2(5.,2.)# 5/2 = 2# 5.0/2 = 2.5# 5//2 = 2# 5.0//2.0 = 2.0 By default, Python 2 automatically performs integer arithmetic if both operands are integers. As a result, 5/2 yields 2, while 5./2 yields 2.5. Note that you can override this behavior in Python 2 by adding the following import: 1from __future__ import division Also note that the “double-slash” (//) operator will always perform integer division, regardless of the operand types. That’s why 5.0//2.0 yields 2.0 even in Python 2. Python 3, however, does not have this behavior; i.e., it does not perform integer arithmetic if both operands are integers. Therefore, in Python 3, the output will be as follows: 12345/2 = 2.55.0/2 = 2.55//2 = 25.0//2.0 = 2.0 Question 51234list = ['a', 'b', 'c', 'd', 'e']print list[10:]# [] 输出为空list, 不会报 IndexError错误 As one would expect, attempting to access a member of a list using an index that exceeds the number of members (e.g., attempting to access list[10] in the list above) results in an IndexError. However, attempting to access a slice of a list at a starting index that exceeds the number of members in the list will not result in an IndexError and will simply return an empty list. What makes this a particularly nasty gotcha is that it can lead to bugs that are really hard to track down since no error is raised at runtime. Question 612345678list = [ [] ] * 5list # output?list[0].append(10)list # output?list[1].append(20)list # output?list.append(30)list # output? 答案1234[[], [], [], [], []][[10], [10], [10], [10], [10]][[10, 20], [10, 20], [10, 20], [10, 20], [10, 20]][[10, 20], [10, 20], [10, 20], [10, 20], [10, 20], 30] list = [ [ ] ] * 5 simply creates a list of 5 lists. However, the key thing to understand here is that the statement list = [ [ ] ] * 5 does NOT create a list containing 5 distinct lists; rather, it creates a a list of 5 references to the same list 12print id(list[0]) == id(list[1])# True list[0].append(10) appends 10 to the first list. But since all 5 lists refer to the same list, the output is: [[10], [10], [10], [10], [10]]. Similarly, list[1].append(20) appends 20 to the second list. But again, since all 5 lists refer to the same list, the output is now: [[10, 20], [10, 20], [10, 20], [10, 20], [10, 20]]. In contrast, list.append(30) is appending an entirely new element to the “outer” list, which therefore yields the output: [[10, 20], [10, 20], [10, 20], [10, 20], [10, 20], 30]. Question 7Given a list of N numbers, use a single list comprehension to produce a new list that only contains those values that are: even numbers, and from elements in the original list that had even indices 答案12345# 0 1 2 3 4 5 6 7 8list = [ 1 , 3 , 5 , 8 , 10 , 13 , 18 , 36 , 78 ]print [x for x in list[::2] if x%2==0]# [10, 18, 78] *args and **kwargs当你不确定你的函数里将要传递多少参数时你可以用*args.例如,它可以传递任意数量的参数: 12345678def print_everything(*args): for count, thing in enumerate(args): print '&#123;0&#125;. &#123;1&#125;'.format(count, thing)print_everything('apple', 'banana', 'cabbage')# 0. apple# 1. banana# 2. cabbage 相似的, **kwargs允许你使用没有事先定义的参数名: 1234567def table_things(**kwargs): for name, value in kwargs.items(): print '&#123;0&#125; = &#123;1&#125;'.format(name, value)table_things(apple = 'fruit', cabbage = 'vegetable')# cabbage = vegetable# apple = fruit 你也可以混着用.命名参数首先获得参数值然后所有的其他参数都传递给args和*kwargs.命名参数在列表的最前端.例如: def table_things(titlestring, kwargs)*args和kwargs可以同时在函数的定义中,但是args必须在*kwargs前面. 当调用函数时你也可以用和*语法.例如: 1234567def print_three_things(a, b, c): print 'a = &#123;0&#125;, b = &#123;1&#125;, c = &#123;2&#125;'.format(a,b,c)mylist = ['aardvark', 'baboon', 'cat']print_three_things(*mylist)a = aardvark, b = baboon, c = cat Python里的拷贝引用和copy(),deepcopy()的区别 1234567891011121314151617181920import copya = [1, 2, 3, 4, ['a', 'b']] #原始对象b = a #赋值，传对象的引用c = copy.copy(a) #对象拷贝，浅拷贝d = copy.deepcopy(a) #对象拷贝，深拷贝a.append(5) #修改对象aa[4].append('c') #修改对象a中的['a', 'b']数组对象print 'a = ', aprint 'b = ', bprint 'c = ', cprint 'd = ', d# 输出结果：# a = [1, 2, 3, 4, ['a', 'b', 'c'], 5]# b = [1, 2, 3, 4, ['a', 'b', 'c'], 5]# c = [1, 2, 3, 4, ['a', 'b', 'c']]# d = [1, 2, 3, 4, ['a', 'b']]]]></content>
      <tags>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SQL Server窗口函数使用]]></title>
    <url>%2F2017%2F06%2F12%2FSQL-Server%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[什么是窗口函数 Windows Function窗口函数属于集合函数，作用在行集上，下面这段关于窗口函数的介绍来自 PostgreSQL intro windows function A window function performs a calculation across a set of table rows that are somehow related to the current row. This is comparable to the type of calculation that can be done with an aggregate function. But unlike regular aggregate functions, use of a window function does not cause rows to become grouped into a single output row — the rows retain their separate identities. Behind the scenes, the window function is able to access more than just the current row of the query result. 窗口函数在SQL:2003标准中被添加，并在SQL:2008标准中被细化。传统的关系型数据库：Oracle、Sybase和DB2都已经支持窗口函数，像开源的PostgreSQL里面也已经有了对窗口函数的完整的实现。SQL Server 2005开始对窗口函数有了最初的支持，从SQL Server 2012开始，窗口函数也被SQL Server完全支持。 SQL Server窗口函数窗口函数的应用非常广泛，像分页、去重、分组的基础上返回 Top N 的行、计算 Running Totals、Gaps and islands、百分率, Hierarchy 排序、Pivoting 等等。 窗口函数是整个SQL语句最后被执行的部分，这意味着窗口函数是在SQL查询的结果集上进行的，因此不会受到Group By， Having，Where子句的影响 SQL Server 窗口函数主要用来处理由 OVER 子句定义的行集, 主要用来分析和处理 Running totals Moving averages Gaps and islands 在标准的SQL中，Window Function 的OVER语句中有三个非常重要的元素: Partitioning Ordering Framing 这三种元素的作用可以限制窗口集中的行，如果没有指定任何元素，那么窗口中包含的就是查询结果集中所有的行。 窗口函数的语法： 1234567-- Syntax for SQL Server, Azure SQL Database, and Azure SQL Data Warehouse OVER ( [ &lt;PARTITION BY clause&gt; ] [ &lt;ORDER BY clause&gt; ] [ &lt;ROW or RANGE clause&gt; ] ) Partition Divides the query result set into partitions. The window function is applied to each partition separately and computation restarts for each partition. 通过PARTITION BY 得到的窗口集是基于当前查询结果的当前行的一个集，比如说 PARTITION BY CustomerID，当前行的 CustomerID = 1，那么对于当前行的这个 Window 集就是在当前查询结果之上再加上 CustomerID = 1 的一个查询结果。 Order Defines the logical order of the rows within each partition of the result set. That is, it specifies the logical order in which the window function calculation is performed. Order By子句对于诸如Row_Number()，Rank()，Lead()，LAG()等函数是必须的，因为如果数据无序，这些函数的结果就没有任何意义 ROW / RANGE Further limits the rows within the partition by specifying start and end points within the partition. This is done by specifying a range of rows with respect to the current row either by logical association or physical association. Physical association is achieved by using the ROWS clause. ROWS 子句通过指定当前行之前或之后的固定数目的行，限制分区中的行数。 RANGE 子句通过指定针对当前行中的值的某一范围的值，从逻辑上限制分区中的行数 12345678910ROWS BETWEEN UNBOUNDED PRECEDING | &lt;n&gt; PRECEDING | &lt;n&gt; FOLLOWING | CURRENT ROWorROWS BETWEEN UNBOUNDED FOLLOWING | &lt;n&gt; PRECEDING | &lt;n&gt; FOLLOWING | CURRENT ROW UNBOUNDED PRECEDING 指的是相对于当前行来说之前的所有的行 UNBOUNDED FOLLOWING 指的是相对于当前行来说之后的所有的行 CURRENT ROW 就是当前行 简单的例子下面用一个简单的例子表示传统的聚合函数和窗口函数的区别 有一个需求：将AdventureWorks示例数据库中的Employee表按照性别进行聚合，希望得到的结果是：”登录名，性别，该性别所有员工的总数” 那么传统的写法是用子查询获得按照性别进行聚合的值，然后再关联 1234SELECT [LoginID] , [Gender] , (SELECT COUNT(*) FROM [AdventureWorks2012].[HumanResources].[Employee] a WHERE a.Gender=b.Gender) AS GenderTotalFROM [AdventureWorks2012].[HumanResources].[Employee] b 如果使用窗口函数完成这个功能，代码如下： 1234SELECT [LoginID] , [Gender] , COUNT(*) OVER(PARTITION BY Gender) AS GenderTotalFROM [AdventureWorks2012].[HumanResources].[Employee] 窗口函数与 Group, 子查询语句的比较对于Group来说，SELECT语句中的列必须是Group子句中出现的列或者是聚合列，那么如果需要同时在 SELECT 语句中查询其它的非 Group 或者非聚合列, 那么就需要额外的子查询。 一个和上面例子很相似的情景，比如要查询每个客户的每个订单的值，以及这个订单于这个订单客户的所有订单总和比，以及这个订单与这个客户所有订单平均值的差。 一个SELECT语句肯定是搞不定的，如下面代码： 12345678910111213141516WITH Aggregates AS( SELECT custid , SUM(val) AS sumval , AVG(val) AS avgval FROM Sales.OrderValues GROUP BY custid)SELECT O.orderid , O.custid , O.val , CAST(100. * O.val / A.sumval AS NUMERIC(5, 2)) AS pctcust , O.val - A.avgval AS diffcustFROM Sales.OrderValues AS OJOIN Aggregates AS AON O.custid = A.custid; 因为没有办法在一个Group查询中同时显示 Detail和汇总的信息 如果这时再加一个比 - 单个订单与总订单额/平均额比，这时汇总的级别又不相同了， 需要单独再汇总一次 额~ 又要添加一层子查询聚合 如果提出更多的聚合和比较，查询语句会越来越复杂，并且查询优化器也不能确定每次是否都访问的是同一个数据集，因此需要分别访问数据集，造成性能下降。 通过使用窗口函数可以很容易解决这些问题，因为可以为每一种聚合定义一个窗口上下文。 12345678SELECT orderid , custid , val , CAST(100.* val/ SUM(val) OVER(PARTITION BY custid) AS NUMERIC(5,2)) AS pctcut , val - AVG(val) OVER(PARTITION BY custid) AS diffcust , CAST(100.* val/ SUM(val) OVER() AS NUMERIC(5,2)) AS pctall , val - AVG(val) OVER() AS diffallFROM Sales.OrderValues 使用窗口函数的例子将 OVER 子句与 ROW_NUMBER 函数结合使用下面的脚本将 OVER 子句与 ROW_NUMBER 函数一起使用来显示分区内各行的行号，分区由 PARTITION BY PostalCode确定 1234567891011SELECT ROW_NUMBER() OVER(PARTITION BY PostalCode ORDER BY SalesYTD DESC) AS "Row Number" , p.LastName , s.SalesYTD , a.PostalCode FROM Sales.SalesPerson AS s INNER JOIN Person.Person AS p ON s.BusinessEntityID = p.BusinessEntityID INNER JOIN Person.Address AS a ON a.AddressID = p.BusinessEntityID WHERE TerritoryID IS NOT NULL AND SalesYTD &lt;&gt; 0 ORDER BY PostalCode; 用这种分配行号的方法，可以完成例如分页、去除重复元素、返回每组前N条数据等实际需求 将 OVER 子句与聚合函数结合使用12345678SELECT SalesOrderID, ProductID, OrderQty , SUM(OrderQty) OVER(PARTITION BY SalesOrderID) AS Total , AVG(OrderQty) OVER(PARTITION BY SalesOrderID) AS "Avg" , COUNT(OrderQty) OVER(PARTITION BY SalesOrderID) AS "Count" , MIN(OrderQty) OVER(PARTITION BY SalesOrderID) AS "Min" , MAX(OrderQty) OVER(PARTITION BY SalesOrderID) AS "Max" FROM Sales.SalesOrderDetail WHERE SalesOrderID IN(43659,43664); 生成移动平均值和累计合计下面的示例将 AVG 和 SUM 函数与 OVER 子句结合使用，以便为 Sales.SalesPerson 表中的每个地区提供年度销售额的累计合计。 数据按 TerritoryID 分区并在逻辑上按 SalesYTD 排序 12345678910111213SELECT BusinessEntityID , TerritoryID , DATEPART(yy,ModifiedDate) AS SalesYear , CONVERT(varchar(20),SalesYTD,1) AS SalesYTD , CONVERT(varchar(20),AVG(SalesYTD) OVER (PARTITION BY TerritoryID ORDER BY DATEPART(yy,ModifiedDate) ),1) AS MovingAvg , CONVERT(varchar(20),SUM(SalesYTD) OVER (PARTITION BY TerritoryID ORDER BY DATEPART(yy,ModifiedDate) ),1) AS CumulativeTotal FROM Sales.SalesPerson WHERE TerritoryID IS NULL OR TerritoryID &lt; 5 ORDER BY TerritoryID, SalesYear; 在 OVER 子句中指定的 ORDER BY 子句将确定应用 AVG 函数的逻辑顺序。 再往下，ORDER BY之后也可以指定 ROWS 子句进一步限制窗口的大小 12345678SELECT BusinessEntityID, TerritoryID , DATEPART(yy,ModifiedDate) AS SalesYear , CONVERT(varchar(20),SalesYTD,1) AS SalesYTD , CONVERT(varchar(20),SUM(SalesYTD) OVER (PARTITION BY TerritoryID ORDER BY DATEPART(yy,ModifiedDate) ROWS BETWEEN CURRENT ROW AND 1 FOLLOWING ),1) AS CumulativeTotal FROM Sales.SalesPerson WHERE TerritoryID IS NULL OR TerritoryID &lt; 5; 在这个例子里面， ROWS子句 ROWS BETWEEN CURRENT ROW AND 1 FOLLOWING 限制窗口为： 当前行的行 对其 下面1行 所以查询结果为： 123456789101112131415SELECT t.OrderYear , t.OrderMonth , t.TotalDue , SUM(t.TotalDue) OVER(PARTITION BY OrderYear, OrderMonth ORDER BY t.OrderYear, t.OrderMonth ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS 'RunningTotal'FROM( SELECT YEAR(OrderDate) AS 'OrderYear' , MONTH(OrderDate) AS 'OrderMonth' , SalesPersonID , TotalDue FROM Sales.SalesOrderHeader ) AS tWHERE t.SalesPersonID = 274 AND t.OrderYear = 2005 在这个例子中，窗口被限制为：第一行 (UNBOUNDED PRECEDING) 到当前行 (CURRENT ROW) 查询结果为： 所以11月份的累计总和为4723 和 7140(4723.1073+2417.4793) 如果把ROWS限制改成RANGE会怎么样呢? 结果如下： RANGE选项包含窗口里的所有行，和当前行有相同ORDER BY值。上面的例子里面，对于2005年11月的2条记录你拿到同个汇总，因为这2行有同样的ORDER BY值（2005年11月） note: 使用ROWS选项你在物理级别定义在你窗口里有多少行。使用RANGE选项取决于ORDER BY值在窗口里有多少行被包含]]></content>
      <tags>
        <tag>SQL Server</tag>
        <tag>SQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Anaconda解决python包管理与环境管理]]></title>
    <url>%2F2017%2F06%2F08%2FAnaconda%E8%A7%A3%E5%86%B3python%E5%8C%85%E7%AE%A1%E7%90%86%E4%B8%8E%E7%8E%AF%E5%A2%83%E7%AE%A1%E7%90%86%2F</url>
    <content type="text"><![CDATA[Anaconda是一个用于科学计算的Python发行版，支持 Linux, Mac, Windows系统，提供了包管理与环境管理的功能，可以很方便地解决多版本python并存、切换以及各种第三方包安装问题。 conda可以理解为一个工具，也是一个可执行命令，其核心功能是包管理与环境管理 提供包管理，功能类似于 pip，Windows 平台安装第三方包经常失败的场景得以解决。 提供虚拟环境管理，功能类似于 virtualenv，解决了多版本Python并存问题 Anaconda具有跨平台、包管理、环境管理的特点，因此很适合快速在新的机器上部署Python环境。 Anaconda的下载页参见 官网下载 Anaconda安装成功之后，可以检查所安装的版本 1conda --version 由于不可名状的原因，需要修改其包管理镜像为国内源 12conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/conda config --set show_channel_urls yes Conda的环境管理Conda的环境管理功能允许我们同时安装若干不同版本的Python，并能自由切换。对于上述安装过程，假设我们采用的是Python 2.7对应的安装包，那么Python 2.7就是默认的环境 现在，创建一个自定义的Python环境 1conda create -n py27 python=2.7 其中py27是新添加环境的名字，可以自定义修改。 之后通过activate py27和deactivate py27命令激活、退出该环境 1activate py27 现在把创建的环境都列出来，其中当前使用的环境前面用*号标注 1conda info --envs 123456789101112131415161718# 创建一个名为py3的环境，指定Python版本是3.4（不用管是3.4.x，conda会为我们自动寻找3.4.x中的最新版本）conda create --name py3 python=3.4 # 安装好后，使用activate激活某个环境activate py3 # for Windowssource activate py3 # for Linux &amp; Mac# 激活后，会发现terminal输入的地方多了py3的字样，实际上，此时系统做的事情就是把默认2.7环境从PATH中去除，再把3.4对应的命令加入PATH # 此时，再次输入python --version# 可以得到`Python 3.4.5 :: Anaconda 4.1.1 (64-bit)`，即系统已经切换到了3.4的环境 # 如果想返回默认的python 2.7环境，运行deactivate py3 # for Windowssource deactivate py3 # for Linux &amp; Mac # 删除一个已有的环境conda remove --name py3 --all Conda的包管理现在要使用conda来管理包了，以前常用的是Python的pip包管理工具。 1234567# 安装scipyconda install scipy# conda会从从远程搜索scipy的相关信息和依赖项目，对于python 3.4，conda会同时安装numpy和mkl（运算加速的库） # 查看已经安装的packagesconda list# 最新版的conda是从site-packages文件夹中搜索已经安装的包，不依赖于pip，因此可以显示出通过各种方式安装的包 实例：让Python2和3在Jupyter Notebook中共存多版本的Python或者R等语言，在Jupyter中被称作kernel。 如果这个Python版本已经存在（比如我们刚才添加的py27环境），那么你可以直接为这个环境安装 ipykernel包 1conda install -n py27 ipykernel note: -n 后面的名字为所要安装到的环境名 然后激活这个环境 1python -m ipykernel install --user 如果所需版本并不是已有的环境，可以直接在创建环境时便为其预装 ipykernel。 1conda create -n py27 python=2.7 ipykernel 打开jupyter notebook 1jupyter notebook 最后两个字：省心 附： conda cheat sheet]]></content>
      <tags>
        <tag>Python</tag>
        <tag>Anaconda</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[T-SQL查询语句执行顺序]]></title>
    <url>%2F2017%2F06%2F06%2FT-SQL%E6%9F%A5%E8%AF%A2%E8%AF%AD%E5%8F%A5%E6%89%A7%E8%A1%8C%E9%A1%BA%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[数据库查询语句可以说是基础中的基础，查询语句是后面查询性能优化的基础，但是很多人并不能很准确的说出数据库查询的逻辑流程。 SQL 语句中元素逻辑查询处理，指的是标准SQL定义的如何处理查询和返回最终结果的概念性路径。和其他的语言代码执行顺序不同，SQL 查询不是按照代码顺序来进行逻辑查询。下面这段SQL 查询 12345678SELECT empid , YEAR(orderdate) AS OrderYear , COUNT(*) AS NumOrdersFROM Sales.OrdersWHERE custid = 71GROUP BY empid, YEAR(orderdate)HAVING COUNT(*) &gt; 1ORDER BY empid, OrderYear 这段SQL 语句其实是按照下面的顺序进行的逻辑处理： FROM WHERE GROUP BY HAVING SELECT ORDER BY FROM 子句指定要查询的表名称和进行多表运算的表运算符(JOIN)； WHERE 子句可以指定一个谓词或者逻辑表达式来筛选由 FROM阶段返回的行； GROUP BY阶段允许用户把前面阶段返回的行排列到组中； HAVING 子句可以指定一个谓词来筛选前面GROUP出的组，而不是筛选单个行； SELECT 子句用户指定要返回到查询结果表中属性(列)； ORDER BY 子句允许对输出行进行排序。 流程图这里引用 Itzik Ben-Gan 的流程图 分步分析FROM 子句FROM阶段标识出查询的来源表，并处理表运算符。在涉及到联接运算的查询中（各种join），主要有以下几个步骤： 求笛卡尔积。不论是什么类型的联接运算，首先都是执行交叉连接（cross join），求笛卡儿积，生成虚拟表VT1-J1。 ON筛选器。这个阶段对上个步骤生成的VT1-J1进行筛选，根据ON子句中出现的谓词进行筛选，让谓词取值为true的行通过了考验，插入到VT1-J2。 添加外部行。如果指定了outer join，还需要将VT1-J2中没有找到匹配的行，作为外部行添加到VT1-J2中，生成VT1-J3。 经过以上步骤，FROM阶段就完成了。概括地讲，FROM阶段就是进行预处理的，根据提供的运算符对语句中提到的各个表进行处理（除了join，还有apply，pivot，unpivot） WHERE 子句WHERE阶段是根据中条件对VT1中的行进行筛选，让条件成立的行才会插入到VT2中。 GROUP BY阶段GROUP阶段按照指定的列名列表，将VT2中的行进行分组，生成VT3。最后每个分组只有一行。 HAVING阶段该阶段根据HAVING子句中出现的谓词对VT3的分组进行筛选，并将符合条件的组插入到VT4中。 SELECT阶段这个阶段是投影的过程，处理SELECT子句提到的元素，产生VT5。这个步骤一般按下列顺序进行 计算SELECT列表中的表达式，生成VT5-1。 若有DISTINCT，则删除VT5-1中的重复行，生成VT5-2 若有TOP，则根据ORDER BY子句定义的逻辑顺序，从VT5-2中选择签名指定数量或者百分比的行，生成VT5-3 ORDER BY阶段根据ORDER BY子句中指定的列明列表，对VT5-3中的行，进行排序，生成游标VC6. 当然SQL SERVER在实际的查询过程中，有查询优化器来生成实际的工作计划。以何种顺序来访问表，使用什么方法和索引，应用哪种联接方法，都是由查询优化器来决定的。优化器一般会生成多个工作计划，从中选择开销最小的那个去执行。逻辑查询处理都有非常特定的顺序，但是优化器常常会走捷径。]]></content>
      <tags>
        <tag>SQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo command Cheatsheet]]></title>
    <url>%2F2017%2F06%2F05%2FHexo-command-Cheatsheet%2F</url>
    <content type="text"><![CDATA[hexo123npm install hexo -g #安装 npm update hexo -g #升级 hexo init #初始化 简写12345hexo n "我的博客" # == hexo new "我的博客" #新建文章hexo p # == hexo publish #发布草稿hexo g # == hexo generate#生成hexo s # == hexo server #启动服务预览hexo d # == hexo deploy#部署 服务器12345678hexo server #Hexo 会监视文件变动并自动更新，您无须重启服务器。hexo server -s #静态模式hexo server -p 5000 #更改端口hexo server -i 192.168.1.1 #自定义 IPhexo clean #清除缓存 网页正常情况下可以忽略此条命令hexo g #生成静态网页hexo d #开始部署 监视文件变动12hexo generate #使用 Hexo 生成静态文件快速而且简单hexo generate --watch #监视文件变动 完成后部署两个命令的作用是相同的 12345hexo generate --deployhexo deploy --generatehexo deploy -ghexo server -g 草稿1hexo publish [layout] &lt;title&gt; 模版123456789hexo new "postName" #新建文章hexo new page "pageName" #新建页面hexo generate #生成静态页面至public目录hexo server #开启预览访问端口（默认端口4000，'ctrl + c'关闭server）hexo deploy #将.deploy目录部署到GitHubhexo new [layout] &lt;title&gt;hexo new photo "My Gallery"hexo new "Hello World" --lang tw 变量 描述 layout 布局 title 标题 date 文件建立日期]]></content>
      <tags>
        <tag>hexo</tag>
        <tag>Cheat Sheet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo设置主题和其他配置]]></title>
    <url>%2F2017%2F06%2F05%2FHexo%E8%AE%BE%E7%BD%AE%E4%B8%BB%E9%A2%98%E5%92%8C%E5%85%B6%E4%BB%96%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[博客的主题很多，我选择一个非常流行的主题 NexT，作为我博客的主题，这个主题简约，并且文档和维护都很好。 下面记录一下我的Hexo站点的配置，以及NexT主题的配置，以备日后查找 站点配置博客根目录下的_config.yml 文件是站点的配置文件。 为了能够使Hexo部署到GitHub上，需要安装一个插件： 1npm install hexo-deployer-git --save 设置站点配置文件指定部署的位置 1234deploy: type: git repo: https://github.com/lvraikkonen/lvraikkonen.github.io.git branch: master 在 站点配置文件 中找到theme字段，把值改为 next 主题NexT配置选择主题样式Scheme 是 NexT 提供的一种特性，借助于 Scheme，NexT 为你提供多种不同的外观。同时，几乎所有的配置都可以 在 Scheme 之间共用。目前 NexT 支持三种 Scheme，他们是： Muse - 默认 Scheme，这是 NexT 最初的版本，黑白主调，大量留白 Mist - Muse 的紧凑版本，整洁有序的单栏外观 Pisces - 双栏 Scheme，小家碧玉似的清新 (我的) 123#scheme: Muse#scheme: Mistscheme: Pisces 头像设置编辑 主题配置文件， 修改字段 avatar， 值设置成头像的链接地址。其中，头像的链接地址可以是： 地址 值 完整的互联网URI http://example.com/avatar.png 站内地址 - 将头像放置主题目录下的 source/uploads/ 或者 放置在 source/images/ 目录下 配置为：avatar: /images/avatar.png 添加标签页和分类页12hexo new page "tags"hexo new page "categories" 同时，在/source目录下会生成一个tags文件夹和categories文件夹，里面各包含一个index.md文件。 修改/source/tags目录下的 index.md文件 12345title: tagsdate: 2017-05-29 18:16:02type: "tags"--- 修改/source/categories目录下的 index.md文件 12345title: categoriesdate: 2015-09-29 18:17:14type: "categories"--- 修改 主题配置文件， 去掉相应的注释 1234567menu: home: / #主页 categories: /categories #分类页（需手动创建） #about: /about #关于页面（需手动创建） archives: /archives #归档页 tags: /tags #标签页（需手动创建） #commonweal: /404.html #公益 404 （需手动创建） 设置网站的图标Favicon从网上找一张 icon 图标文件，放在 source 目录下就可以了 添加友情链接在 站点配置文件 中添加参数： 1234links_title: 友情链接links: #百度: http://www.baidu.com/ #新浪: http://example.com/ 设置代码高亮NexT 使用 Tomorrow Theme 作为代码高亮，共有5款主题供你选择。 NexT 默认使用的是 白色的 normal 主题，可选的值有 normal，night， night blue， night bright， night eighties 更改 highlight_theme 字段，将其值设定成你所喜爱的高亮主题 配置Algolia 搜索官网 注册一个账号，可以用Github账户注册 登录进入Dashboard控制台页面，创建一个新Index 进入 API Keys 界面，拷贝 Application ID 、Search-Only API Key 和 Admin API Key 编辑 站点配置文件 ，新增以下配置： 123456algolia: applicationID: 'your applicationID' apiKey: 'your Search-Only API Key' adminApiKey: 'your Admin API Key' indexName: 'your newcreated indexName' chunkSize: 5000 安装Hexo Algolia在Hexo根目录执行如下指令，进行Hexo Algolia的安装： 1npm install --save hexo-algolia 到Hexo的根目录，在其中找到package.json文件，修改其中的hexo-algolia属性值为^0.2.0，如下图所示： 当配置完成，在站点根目录下执行hexo algolia 来更新Index 1hexo algolia 注意： 如果发现没有上传数据，这时候可以先 hexo clean 然后再 hexo algolia _ 在 主题配置文件中，找到Algolia Search 配置部分： 123456789# Algolia Searchalgolia_search: enable: true hits: per_page: 10 labels: input_placeholder: Search for Posts hits_empty: "We didn't find any results for the search: $&#123;query&#125;" hits_stats: "$&#123;hits&#125; results found in $&#123;time&#125; ms" 将 enable 改为true 即可，根据需要你可以调整labels 中的文本。 配置来必力评论评论插件，最出名的是 Disqus，但是对于国内用户来说，自带梯子好些。改用多说，路边社消息，多说好像要完蛋了。发现了个叫 LiveRe（来必力）的评论插件，韩国出的，用着感觉还不错。 来必力官网注册账号 LiveRe 有两个版本： City 版：是一款适合所有人使用的免费版本； Premium 版：是一款能够帮助企业实现自动化管理的多功能收费版本。 选择City版就可以了 获取 LiveRe UID。 编辑 主题配置文件， 编辑 livere_uid 字段，设置如下： 1livere_uid: #your livere_uid NexT 已经支持来必力，这样就能在页面显示评论了 添加阅读次数统计这里使用 LeanCloud 为文章添加统计功能 注册账号登陆以后获得 AppID以及 AppKey这两个参数即可正常使用文章阅读量统计的功能了。 创建应用 配置应用 在应用的数据配置界面，左侧下划线开头的都是系统预定义好的表。在弹出的选项中选择创建Class来新建Class用来专门保存我们博客的文章访问量等数据:为了保证我们前面对NexT主题的修改兼容，此处的新建Class名字为 Counter 复制 AppID以及 AppKey 修改 主题配置文件 12345# You can visit https://leancloud.cn get AppID and AppKey.leancloud_visitors: enable: true app_id: #&lt;app_id&gt; app_key: #&lt;app_key&gt; 重新生成并部署博客就可以显示阅读量了 note: 记录文章访问量的唯一标识符是 文章的发布日期以及文章的标题，因此请确保这两个数值组合的唯一性，如果你更改了这两个数值，会造成文章阅读数值的清零重计 后台可以看到，刚才创建的Counter类 添加字数统计功能首先在博客目录下使用 npm 安装插件 1npm install hexo-wordcount --save 在 主题配置文件中打开wordcount 统计功能 123456# Post wordcount display settings# Dependencies: https://github.com/willin/hexo-wordcountpost_wordcount: item_text: true wordcount: true min2read: false 找到..\themes\next\layout_macro\post.swig 文件，将”字”、”分钟” 字样添加到如下位置 1234567&lt;span title=&quot;&#123;&#123; __(&apos;post.wordcount&apos;) &#125;&#125;&quot;&gt; &#123;&#123; wordcount(post.content) &#125;&#125; 字&lt;/span&gt; ...&lt;span title=&quot;&#123;&#123; __(&apos;post.min2read&apos;) &#125;&#125;&quot;&gt; &#123;&#123; min2read(post.content) &#125;&#125; 分钟&lt;/span&gt;]]></content>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SQL Server索引]]></title>
    <url>%2F2017%2F06%2F02%2FSQL-Server%E7%B4%A2%E5%BC%95%2F</url>
    <content type="text"><![CDATA[索引是与表或视图关联的磁盘上结构，可以加快从表或视图中检索行的速度。在SQL Server中，索引和表（这里指的是加了聚集索引的表）的存储结构是一样的,都是B树，B树是一种用于查找的平衡多叉树。 索引的利弊查询执行的大部分开销是I/O，使用索引提高性能的一个主要目标是避免全表扫描，因为全表扫描需要从磁盘上读取表的每一个数据页，如果有索引指向数据值，则查询只需要读少数次的磁盘就行啦。所以合理的使用索引能加速数据的查询。但是索引并不总是提高系统的性能，带索引的表需要在数据库中占用更多的存储空间，同样用来增删数据的命令运行时间以及维护索引所需的处理时间会更长。所以我们要合理使用索引，及时更新去除次优索引。 数据表的基本结构一个新表被创建之时，系统将在磁盘中分配一段以8K为单位的连续空间，当字段的值从内存写入磁盘时，就在这一既定空间随机保存，当一个 8K用完的时候，数据库指针会自动分配一个8K的空间。这里，每个8K空间被称为一个数据页（Page），又名页面或数据页面，并分配从0-7的页号, 每个文件的第0页记录引导信息，叫文件头（File header）；每8个数据页（64Ｋ）的组合形成扩展区（Extent），称为扩展。全部数据页的组合形成堆（Heap） 聚集索引和非聚集索引在SQL SERVER中，聚集索引的存储是以B树存储，B树的叶子直接存储聚集索引的数据 非聚集索引与聚集索引具有相同的 B 树结构，它们之间的显著差别在于以下两点： 基础表的数据行不按非聚集键的顺序排序和存储。 非聚集索引的叶层是由索引页而不是由数据页组成。 非聚集索引也是一个B树结构，与聚集索引不同的是，B树的叶子节点存的是指向堆或聚集索引的指针。 索引的设计原则]]></content>
      <tags>
        <tag>SQL Server</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[看懂SQL Server执行计划]]></title>
    <url>%2F2017%2F06%2F02%2F%E7%9C%8B%E6%87%82SQL-Server%E6%89%A7%E8%A1%8C%E8%AE%A1%E5%88%92%2F</url>
    <content type="text"><![CDATA[当我们写的SQL语句传到SQL Server的时候，查询分析器会将语句依次进行解析（Parse）、绑定（Bind）、查询优化（Optimization，有时候也被称为简化）、执行（Execution）。除去执行步骤外，前三个步骤之后就生成了执行计划，也就是SQL Server按照该计划获取物理数据方式，最后执行步骤按照执行计划执行查询从而获得结果。 执行计划 查询优化器对输入的 T-SQL 查询语句通过”计算”而选择出效率最高的一种执行方案，这个执行方案就是执行计划。 执行计划可以告诉你这个查询将会被如何执行或者已经被如何执行过，可以通过执行计划看到 SQL 代码中那些效率比较低的地方。 查看执行计划的方式我们可以通过图形化的界面，或者文本，或者XML格式查看，这样会比较方便理解执行计划要表达出来的意思。 当一个查询被提交到 SQL Server 后，服务器端很多进程实际上要做很多事情来确保数据出入的完整性。 对于 T-SQL 来说, 处理的主要有两个阶段：关系引擎阶段( relational engine)和存储引擎阶段( storage engine)。 关系引擎主要要做的事情就是首先确保 Query 语句能够正确的解析，然后交给查询优化并产生执行计划，然后执行计划就以二进制格式发到存储引擎来更新或者读取数据。 存储引擎主要处理的比如像锁、索引的维护和事务等 所以对于执行计划，重点的是关注关系引擎。 估算执行计划和实际执行计划Estimated Execution Plans vs. Actual Execution Plans 它们之间的区别就是: 估算的执行计划 是从查询优化器来的，是输入关系引擎的，它的执行步骤包括一些运算符等等都是通过一系列的逻辑分析出来的，是一种通过逻辑推算出来的计划，只能代表查询优化器的观点；实际执行计划 是真实的执行了”估算执行计划”后的一种真实的结果，是实实在在真实的执行反馈, 是属于存储引擎。 以上描述了关于执行计划的概念，下面以实际案例去解读一些基本语句， 例如SELECT, UPDATE,INSERT, DELETE 等查询的执行计划。 ————————————我是分隔符———————————– 有大约78个执行计划中的操作符，可以去 MSDN Book Online 随时查 下表表示一下常见的执行计划元素 Select (Result) Sort Spool Clustered Index Scan Key Lookup Eager Spool NonClustered Index Scan Compute Scalar Stream Aggregate Clustered Index Seek Constant Scan Distribute Streams NonClustered Index Seek Table Scan Repartition Streams Hash Match RID Lookup Gather Streams Nested Loops Filter Bitmap Merge Join Lazy Spool Split 操作符分为阻断式 blocking 和非阻断式non-blocking 常见操作符的执行计划解释Table Scan 表扫描 当表中没有聚集索引，又没有合适索引的情况下，会出现这个操作。这个操作是很耗性能的，他的出现也意味着优化器要遍历整张表去查找你所需要的数据 Clustered Index Scan / Index Scan 聚集索引扫描/非聚集索引扫描 这个图标两个操作都可以使用，一个聚集索引扫描，一个是非聚集索引扫描。 聚集索引扫描：聚集索引的数据体积实际是就是表本身，也就是说表有多少行多少列，聚集所有就有多少行多少列，那么聚集索引扫描就跟表扫描差不多，也要进行全表扫描，遍历所有表数据，查找出你想要的数据。 非聚集索引扫描：非聚集索引的体积是根据你的索引创建情况而定的，可以只包含你要查询的列。那么进行非聚集索引扫描，便是你非聚集中包含的列的所有行进行遍历，查找出你想要的数据。 看下面这个查询 12SELECT ct.*FROM Person.ContactType AS ct; 这个表有一个聚簇索引PK_ContactType_ContactTypeID，聚簇索引的叶子结点是存储数据的，所以对于这个聚簇索引的扫描和全表扫面基本类似，基本也是一行一行地进行扫描来满足查询。 如果在执行计划中遇到索引扫描，说明查询有可能返回比需要更多的行，这时候建议使用 WHERE语句去优化查询，确保只是需要的那些行被返回。 Clustered Index Seek / Index Seek 聚集索引查找/非聚集索引查找 聚集索引查找和非聚集索引查找都是使用该图标。 聚集索引查找：聚集索引包含整个表的数据，也就是在聚集索引的数据上根据键值取数据。 非聚集索引查找：非聚集索引包含创建索引时所包含列的数据，在这些非聚集索引的数据上根据键值取数据。 123SELECT ct.*FROM Person.ContactType AS ctWHERE ct.ContactTypeID = 7 这个表有一个聚簇索引PK_ContactType_ContactTypeID，建在ContactTypeID字段上，查询使用了这个聚簇索引来查找指定的数据。 索引查找和索引扫描不同，使用查找可以让优化器准确地通过键值找到索引的位置。 以上几种查询的性能对比： [Table Scan] 表扫描（最慢）：对表记录逐行进行检查 [Clustered Index Scan] 聚集索引扫描（较慢）：按聚集索引对记录逐行进行检查 [Index Scan] 索引扫描（普通）：根据索引滤出部分数据在进行逐行检查 [Index Seek] 索引查找（较快）：根据索引定位记录所在位置再取出记录 [Clustered Index Seek] 聚集索引查找（最快）：直接根据聚集索引获取记录 Key Lookup 键值查找 首先需要说的是查找，查找与扫描在性能上完全不是一个级别的，扫描需要遍历整张表，而查找只需要通过键值直接提取数据，返回结果，性能要好。 当你查找的列没有完全被非聚集索引包含，就需要使用键值查找在聚集索引上查找非聚集索引不包含的列。 RID Lookup RID查找 跟键值查找类似，只不过RID查找，是需要查找的列没有完全被非聚集索引包含，而剩余的列所在的表又不存在聚集索引，不能键值查找，只能根据行表示Rid来查询数据。 123456SELECT p.BusinessEntityID , p.LastName , p.FirstName , p.NameStyleFROM Person.Person AS pWHERE p.LastName LIKE 'Jaf%'; Person.Person 表有非聚簇索引 IX_Person_LastName_FirstName_MiddleName作用在LastName、FirstName和MiddleName列上面，而列 NameStyle并没有被非聚集索引所包含，所以需要使用 KeyLookUp在聚集索引上查找不包含的列。如果这个列所在的表不存在聚集索引，那就只能通过RId，也就是行号在查询了。 Sort对数据集合进行排序，需要注意的是，有些数据集合在索引扫描后是自带排序的。 Filter根据出现在having之后的操作运算符，进行筛选 Computer Scalar在需要查询的列中需要自定义列，比如count(*) as cnt , select name+’’+age 等会出现此符号。 JOIN 连接查询当多表连接时，SQL Server会采用三类不同的连接方式：散列连接，循环嵌套连接，合并连接 Hash Join 这个图标有两种地方用到，一种是表关联，一种是数据聚合运算时 (GROUP BY) 下面有两个概念： Hashing：在数据库中根据每一行的数据内容，转换成唯一符号格式，存放到临时哈希表中，当需要原始数据时，可以给还原回来。类似加密解密技术，但是他能更有效的支持数据查询。 Hash Table：通过hashing处理，把数据以key/value的形式存储在表格中，在数据库中他被放在tempdb中。 Hash Join是做大数据集连接时的常用方式，优化器使用两个表中较小（相对较小）的表利用Join Key在内存中建立散列表 Hash Table，然后扫描较大的表并探测散列表，找出与Hash表匹配的行。这种方式适用于较小的表完全可以放于内存中的情况 如果在执行计划中见到Hash Match Join，也许应该检查一下是不是缺少或者没有使用索引、没有用到WHERE等等。 Nested Loops Join 这个操作符号，把两个不同列的数据集汇总到一张表中。提示信息中的Output List中有两个数据集，下面的数据集（inner set）会一一扫描与上面的数据集（out set），直到扫描完为止，这个操作才算是完成。 对于被连接的数据子集较小的情况，嵌套循环连接是个较好的选择。在嵌套循环中，内表被外表驱动，外表返回的每一行都要在内表中检索找到与它匹配的行，因此整个查询返回的结果集不能太大 Merge Join 这种关联算法是对两个已经排过序的集合进行合并。如果两个聚合是无序的则将先给集合排序再进行一一合并，由于是排过序的集合，左右两个集合自上而下合并效率是相当快的。 通常情况下散列连接的效果都比排序合并连接要好，然而如果行源已经被排过序，在执行排序合并连接时不需要再排序了，这时排序合并连接的性能会优于散列连接。Merge join 用在没有索引，并且数据已经排序的情况。 12345SELECT c.CustomerIDFROM Sales.SalesOrderDetail odJOIN Sales.SalesOrderHeader ohON od.SalesOrderID = oh.SalesOrderIDJOIN Sales.Customer c ON oh.CustomerID = c.CustomerID 由于没有使用WHERE语句，所以优化器对Customer表使用聚集索引扫描，对SalesOrderHeader表使用非聚集索引扫描 Customer表和SalesOrderHeader表使用Merge Join操作符进行关联，关联字段是CustomerID字段，这个字段在上面的索引扫描之后都是有序的。如果不是有序的，优化器会在前面进行排序或者是直接将两个表进行Hash Join连接。 根据执行计划细节要做的优化操作 如果select * 通常情况下聚集索引会比非聚集索引更优。 如果出现Nested Loops，需要查下是否需要聚集索引，非聚集索引是否可以包含所有需要的列。 Hash Match连接操作更适合于需要做Hashing算法集合很小的连接。 Merge Join时需要检查下原有的集合是否已经有排序，如果没有排序，使用索引能否解决。 出现表扫描，聚集索引扫描，非聚集索引扫描时，考虑语句是否可以加where限制，select * 是否可以去除不必要的列。 出现Rid查找时，是否可以加索引优化解决。 在计划中看到不是你想要的索引时，看能否在语句中强制使用你想用的索引解决问题，强制使用索引的办法Select CluName1,CluName2 from Table with(index=IndexName)。 看到不是你想要的连接算法时，尝试强制使用你想要的算法解决问题。强制使用连接算法的语句：select * from t1 left join t2 on t1.id=t2.id option(Hash/Loop/Merge Join) 看到不是你想要的聚合算法是，尝试强制使用你想要的聚合算法。强制使用聚合算法的语句示例：select age ,count(age) as cnt from t1 group by age option(order/hash group) 看到不是你想要的解析执行顺序是，或这解析顺序耗时过大时，尝试强制使用你定的执行顺序。option（force order） 看到有多个线程来合并执行你的sql语句而影响到性能时，尝试强制是不并行操作。option（maxdop 1） 在存储过程中，由于参数不同导致执行计划不同，也影响啦性能时尝试指定参数来优化。option（optiomize for（@name=’zlh’）） 不操作多余的列，多余的行，不做务必要的聚合，排序。]]></content>
      <tags>
        <tag>SQL Server</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Markdown Cheatsheet]]></title>
    <url>%2F2017%2F05%2F31%2FMarkdown-Cheatsheet%2F</url>
    <content type="text"><![CDATA[开始写博客后，Markdown成为一种常用的文档格式，Markdown 的语法全由一些符号所组成，这些符号经过精挑细选，其作用一目了然，里面的语法记下来，为了以后查询方便。 文本样式1234*斜体*或_斜体_**粗体*****加粗斜体***~~删除线~~ 斜体 或 斜体 粗体 加粗斜体 删除线 This text will be italic This will also be italic This text will be bold This will also be bold You can combine them 引用使用大于号 &gt; 表示引用内容： As Grace Hopper said: I’ve always been more interestedin the future than in the past. 分级标题第一种写法 1234这是一个一级标题============================这是一个二级标题-------------------------------------------------- 第二种写法 123456# 一级标题## 二级标题### 三级标题#### 四级标题##### 五级标题###### 六级标题 超链接Markdown 支持两种形式的链接语法： 行内式和参考式两种形式，行内式一般使用较多。 行内式语法说明： []里写链接文字，()里写链接地址, ()中的”“中可以为链接指定title属性，title属性可加可不加。title属性的效果是鼠标悬停在链接上会出现指定的 title文字。链接文字’这样的形式。链接地址与链接标题前有一个空格。 12欢迎来到[我的博客](https://lvraikkonen.github.io/)欢迎来到[我的博客](https://lvraikkonen.github.io/ &quot;博客名&quot;) 欢迎来到我的博客 欢迎来到我的博客 参考式参考式超链接一般用在学术论文上面，或者另一种情况，如果某一个链接在文章中多处使用，那么使用引用 的方式创建链接将非常好，它可以让你对链接进行统一的管理。 语法说明：参考式链接分为两部分，文中的写法 [链接文字][链接标记]，在文本的任意位置添加[链接标记]:链接地址 “链接标题”，链接地址与链接标题前有一个空格。 如果链接文字本身可以做为链接标记，你也可以写成[链接文字][][链接文字]：链接地址的形式，见代码的最后一行。 123456我经常去的几个网站[Google][1]、[Leanote][2]以及[自己的博客][3][Leanote 笔记][2]是一个不错的[网站][]。[1]:http://www.google.com &quot;Google&quot;[2]:http://www.leanote.com &quot;Leanote&quot;[3]:https://lvraikkonen.github.io &quot;lvraikkonen&quot;[网站]:https://lvraikkonen.github.io 我经常去的几个网站Google、Leanote 以及自己的博客，Leanote 笔记是一个不错的网站。 自动链接语法说明：Markdown 支持以比较简短的自动链接形式来处理网址和电子邮件信箱，只要是用&lt;&gt;包起来， Markdown 就会自动把它转成链接。一般网址的链接文字就和链接地址一样，例如： 12&lt;http://example.com/&gt;&lt;address@example.com&gt; 显示效果： http://example.com/ &#97;&#100;&#x64;&#x72;&#101;&#115;&#x73;&#x40;&#x65;&#x78;&#x61;&#109;&#112;&#x6c;&#x65;&#x2e;&#99;&#111;&#109; 列表无序列表使用星号 *、加号 +或是减号 - 作为列表标记 Red Green Blue 有序列表则使用数字接着一个英文句点： Bird McHale Parish 列表项目可以包含多个段落，每个项目下的段落都必须缩进 4 个空格或是 1 个制表符： This is a list item with two paragraphs. Lorem ipsum dolorsit amet, consectetuer adipiscing elit. Aliquam hendreritmi posuere lectus. Vestibulum enim wisi, viverra nec, fringilla in, laoreetvitae, risus. Donec sit amet nisl. Aliquam semper ipsumsit amet velit. Suspendisse id sem consectetuer libero luctus adipiscing. 如果要在列表项目内放进引用，那 &gt; 就需要缩进： A list item with a blockquote: This is a blockquoteinside a list item. 图像和链接很相似的语法来标记图片，同样也允许两种样式： 行内式和参考式 行内式 一个惊叹号 ! 接着一个方括号，里面放上图片的替代文字 接着一个普通括号，里面放上图片的网址，最后还可以用引号包住并加上 选择性的 ‘title’ 文字 123![Alt text](/path/to/img.jpg)![Alt text](/path/to/img.jpg &quot;Optional title&quot;) Inline-style: 参考式id是图片参考的名称，图片参考的定义方式则和链接参考一样：12![Alt text][id][id]: url/to/image &quot;Optional title attribute&quot; Reference-style: 代码如果要标记一小段行内代码，你可以用反引号把它包起来 code 也可以使用三个反引号包裹一段代码，并指定一种语言 12var s = "JavaScript syntax highlighting";alert(s); 12s = "Python syntax highlighting"print s 12No language indicated, so no syntax highlighting.But let&apos;s throw in a &lt;b&gt;tag&lt;/b&gt;. 表格 Tables Are Cool col 3 is right-aligned $1600 col 2 is centered $12 zebra stripes are neat $1 以下是Github支持的markdown语法： To-Do listTASK LISTS [x] this is a complete item [ ] this is an incomplete item [x] @mentions, #refs, links, formatting, and tags supported [x] list syntax required (any unordered or ordered list supported) EMOJIGitHub supports emoji!:+1: :sparkles: :camel: :tada::rocket: :metal: :octocat:]]></content>
      <tags>
        <tag>Cheat Sheet</tag>
        <tag>Markdown</tag>
        <tag>备忘</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计短链接TinyURL]]></title>
    <url>%2F2017%2F05%2F27%2F%E8%AE%BE%E8%AE%A1%E7%9F%AD%E9%93%BE%E6%8E%A5TinyURL%2F</url>
    <content type="text"></content>
      <tags>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python Collection集合模块]]></title>
    <url>%2F2017%2F05%2F27%2FPython-Collection%E9%9B%86%E5%90%88%E6%A8%A1%E5%9D%97%2F</url>
    <content type="text"><![CDATA[1import collections collections是Python内建的一个集合模块，提供了许多有用的集合类。Python拥有一些内置的数据类型，比如str, int, list, tuple, dict等， collections模块在这些内置数据类型的基础上，提供了几个额外的数据类型： namedtuple(): 生成可以使用名字来访问元素内容的tuple子类 deque: 双端队列，可以快速的从另外一侧追加和推出对象 Counter: 计数器，主要用来计数 OrderedDict: 有序字典 defaultdict: 带有默认值的字典 namedtuplenamedtuple主要用来产生可以使用名称来访问元素的数据对象，通常用来增强代码的可读性。 namedtuple是一个函数，它用来创建一个自定义的tuple对象，并且规定了tuple元素的个数，并可以用属性而不是索引来引用tuple的某个元素。 1234567891011from collections import namedtuple# namedtuple('名称', [属性list])Point = namedtuple('Point', ['x', 'y'])p = Point(1, 2)print p.x, p.y# 1, 2isinstance(p, Point)# Trueisinstance(p, tuple)# True 用namedtuple可以很方便地定义一种数据类型，它具备tuple的不变性，又可以根据属性来引用 dequedeque其实是double-ended queue的缩写，翻译过来就是双端队列，它最大的好处就是实现了从队列头部快速增加和取出对象: .popleft(), .appendleft() list对象的这两种用法的时间复杂度是 O(n) ，也就是说随着元素数量的增加耗时呈 线性上升。而使用deque对象则是 O(1) 的复杂度 123456&gt;&gt;&gt; from collections import deque&gt;&gt;&gt; q = deque(['a', 'b', 'c'])&gt;&gt;&gt; q.append('x')&gt;&gt;&gt; q.appendleft('y')&gt;&gt;&gt; qdeque(['y', 'a', 'b', 'c', 'x']) deque提供了很多方法，例如 rotate 123456789101112131415161718192021# -*- coding: utf-8 -*-"""下面这个是一个有趣的例子，主要使用了deque的rotate方法来实现了一个无限循环的加载动画"""import sysimport timefrom collections import dequefancy_loading = deque('&gt;--------------------')while True: print '\r%s' % ''.join(fancy_loading), fancy_loading.rotate(1) sys.stdout.flush() time.sleep(0.08)# Result:# 一个无尽循环的跑马灯# -------------&gt;------- CounterCounter是一个简单的计数器，例如，统计字符出现的个数 12345678910&gt;&gt;&gt; from collections import Counter&gt;&gt;&gt; c = Counter()&gt;&gt;&gt; for ch in 'programming':... c[ch] = c[ch] + 1...&gt;&gt;&gt; cCounter(&#123;'g': 2, 'm': 2, 'r': 2, 'a': 1, 'i': 1, 'o': 1, 'n': 1, 'p': 1&#125;)&gt;&gt;&gt; # 获取出现频率最高的5个字符&gt;&gt;&gt; print c.most_common(5) Counter实际上也是dict的一个子类，上面的结果可以看出，字符’g’、’m’、’r’各出现了两次，其他字符各出现了一次。 defaultdict使用dict时，如果引用的Key不存在，就会抛出KeyError。如果希望key不存在时，返回一个默认值，就可以用defaultdict : 如果使用defaultdict，只要你传入一个默认的工厂方法，那么请求一个不存在的key时， 便会调用这个工厂方法使用其结果来作为这个key的默认值 1234567&gt;&gt;&gt; from collections import defaultdict&gt;&gt;&gt; dd = defaultdict(lambda: 'N/A')&gt;&gt;&gt; dd['key1'] = 'abc'&gt;&gt;&gt; dd['key1'] # key1存在'abc'&gt;&gt;&gt; dd['key2'] # key2不存在，返回默认值'N/A' OrderedDict使用dict时，Key是无序的。在对dict做迭代时，我们无法确定Key的顺序。 如果要保持Key的顺序，可以用OrderedDict 1234567&gt;&gt;&gt; from collections import OrderedDict&gt;&gt;&gt; d = dict([('a', 1), ('b', 2), ('c', 3)])&gt;&gt;&gt; d # dict的Key是无序的&#123;'a': 1, 'c': 3, 'b': 2&#125;&gt;&gt;&gt; od = OrderedDict([('a', 1), ('b', 2), ('c', 3)])&gt;&gt;&gt; od # OrderedDict的Key是有序的OrderedDict([('a', 1), ('b', 2), ('c', 3)]) Note: OrderedDict的Key会按照插入的顺序排列，不是Key本身排序]]></content>
      <tags>
        <tag>Python</tag>
        <tag>Data Structure</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SQL Server数据库分页]]></title>
    <url>%2F2017%2F05%2F27%2FSQL-Server%E6%95%B0%E6%8D%AE%E5%BA%93%E5%88%86%E9%A1%B5%2F</url>
    <content type="text"><![CDATA[在编写Web应用程序等系统时，会涉及到与数据库的交互，如果数据库中数据量很大的话，一次检索所有的记录，会占用系统很大的资源，因此常常采用分页语句：需要多少数据就只从数据库中取多少条记录。 常见的对大数据量查询的解决方案有以下两种： 将全部数据先查询到内存中，然后在内存中进行分页，这种方式对内存占用较大，必须限制一次查询的数据量。 采用存储过程在数据库中进行分页，这种方式对数据库的依赖较大，不同的数据库实现机制不通，并且查询效率不够理想。以上两种方式对用户来说都不够友好。 使用ROW_NUMBER()函数分页SQL Server 2005之后引入了 ROW_NUMBER() 函数，通过该函数根据定好的排序字段规则，产生记录序号 123SELECT ROW_NUMBER() OVER ( ORDER BY dbo.Products.ProductID DESC ) AS rownum , *FROM dbo.Products 12345678SELECT *FROM ( SELECT TOP ( @pageSize * @pageIndex ) ROW_NUMBER() OVER ( ORDER BY dbo.Products.UnitPrice DESC ) AS rownum , * FROM dbo.Products ) AS tempWHERE temp.rownum &gt; ( @pageSize * ( @pageIndex - 1 ) )ORDER BY temp.UnitPrice 使用OFFSET FETCH子句分页SQL Server 2012中引入了OFFSET-FETCH语句，可以通过使用OFFSET-FETCH过滤器来实现分页 12345SELECT * FROM dbo.Products ORDER BY UnitPrice DESC OFFSET ( @pageSize * ( @pageIndex - 1 )) ROWS FETCH NEXT @pageSize ROWS ONLY;]]></content>
      <tags>
        <tag>Database</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[二叉树的遍历]]></title>
    <url>%2F2017%2F05%2F27%2F%E4%BA%8C%E5%8F%89%E6%A0%91%E7%9A%84%E9%81%8D%E5%8E%86%2F</url>
    <content type="text"><![CDATA[二叉树的遍历分为： 深度优先搜索(Depth First Search) 是沿着树的深度遍历树的节点，尽可能深的搜索树的分支。深度优先搜索二叉树是先访问根结点，然后遍历左子树接着是遍历右子树，因此我们可以利用堆栈的先进后出的特点，先将右子树压栈，再将左子树压栈，这样左子树就位于栈顶，可以保证结点的左子树先与右子树被遍历。 广度优先搜索(Breadth First Search) 是从根结点开始沿着树的宽度搜索遍历，可以利用队列实现广度优先搜索 二叉树的深度优先遍历的非递归的通用做法是采用栈，广度优先遍历的非递归的通用做法是采用队列 深度优先实现深度优先遍历又分为：前序、中序、后序遍历 前序遍历：根节点-&gt;左子树-&gt;右子树 中序遍历：左子树-&gt;根节点-&gt;右子树 后序遍历：左子树-&gt;右子树-&gt;根节点 note: 二叉搜索树BST的中序遍历，返回的结果是按顺序排列的 递归实现前序遍历伪代码： 123456preorder(node) if (node = null) return visit(node) preorder(node.left) preorder(node.right) 根节点-&gt;左子树-&gt;右子树 python实现 123456def preorder(self, node): """前序遍历""" if node: print node.data self.preorder(node.left) self.preorder(node.right) 中序遍历伪代码： 123456inorder(node) if (node = null) return inorder(node.left) visit(node) inorder(node.right) 左子树-&gt;根节点-&gt;右子树 123456def inorder(self, node): """中序遍历""" if node: self.inorder(node.left) print node.data self.inorder(node.right) 后序遍历伪代码： 123456postorder(node) if (node = null) return postorder(node.left) postorder(node.right) visit(node) 左子树-&gt;右子树-&gt;根节点 123456def postorder(self, node): """后序遍历""" if node: self.postorder(node.left) self.postorder(node.right) print node.data 非递归实现因为当遍历过根节点之后还要回来，所以必须将其存起来。考虑到后进先出的特点，选用栈存储。 前序遍历伪代码： 123456789iterativePreorder(node) parentStack = empty stack while (not parentStack.isEmpty() or node ≠ null) if (node ≠ null) visit(node) if (node.right ≠ null) parentStack.push(node.right) node = node.left else node = parentStack.pop() 123456789101112131415def preorderTraversal__iterative(root): """ :type root: TreeNode """ node = root stack = [] while node or stack: if node: print node.val if node.right: stack.append(node.right) node = node.left else: node = stack.pop() return 中序遍历伪代码： 12345678910iterativeInorder(node) s ← empty stack while (not s.isEmpty() or node ≠ null) while (node ≠ null) s.push(node) node ← node.left else node ← s.pop() visit(node) node ← node.right 1234567891011121314def inorderTraversal_iterative(root): """ :type root: TreeNode """ node = root stack = [] while node or stack: while node: stack.append(node) node = node.left node = stack.pop() print node.val node = node.right return result 后序遍历 后序遍历伪代码： 12345678910111213141516iterativePostorder(node) s ← empty stack lastNodeVisited ← null while (not s.isEmpty() or node ≠ null) if (node ≠ null) s.push(node) node ← node.left else peekNode ← s.peek() // if right child exists and traversing node // from left child, then move right if (peekNode.right ≠ null and lastNodeVisited ≠ peekNode.right) node ← peekNode.right else visit(peekNode) lastNodeVisited ← s.pop() 123456789101112131415161718def postorderTraversal(node): if node is None: return [] stack = [] result = [] lastNodeVisited = None while stack or node: if node: stack.append(node) node = node.left else: peekNode = stack[-1] if peekNode.right and lastNodeVisited != peekNode.right: node = peekNode.right else: result.append(peekNode) lastVisitedNode = stack.pop() return result 广度优先实现伪代码 12345678910levelorder(root) q ← empty queue q.enqueue(root) while (not q.isEmpty()) node ← q.dequeue() visit(node) if (node.left ≠ null) q.enqueue(node.left) if (node.right ≠ null) q.enqueue(node.right)]]></content>
      <tags>
        <tag>Data Structure</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python内置数据结构]]></title>
    <url>%2F2017%2F05%2F27%2FPython%E5%86%85%E7%BD%AE%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F</url>
    <content type="text"><![CDATA[Python 内置数据类型包括 list, tuple, dict, set ListPython内置的一种数据类型是列表：list。list是一种有序的集合，可以随时添加和删除其中的元素 列表常用操作及其复杂度 Operation big O description index [] O(1) 索引操作 index assignment O(1) 索引赋值操作 append O(1) 在列表末尾添加新的对象 pop() O(1) 移除列表中的一个元素（默认最后一个元素），并且返回该元素的值 pop(i) O(n) 移除列表中索引位置的值，并且返回该元素的值 insert(i,item) O(n) 将对象插入列表索引i位置 del operator O(n) 删除列表的的元素 iteration O(n) contains (in) O(n) get slice [x:y] O(k) del slice O(n) set slice O(n+k) reverse O(n) 反向列表中元素 remove O(n) 移除列表中某个值的第一个匹配项 concatenate O(k) sort O(n log n) 列表排序 multiply O(nk) DictionaryPython内置了字典：dict的支持，dict全称dictionary，在其他语言中也称为map，使用键-值（key-value）存储，具有极快的查找速度。dict内部存放的顺序和key放入的顺序是没有关系的。dict的key必须是不可变对象。这是因为dict根据key来计算value的存储位置，如果每次计算相同的key得出的结果不同，那dict内部就完全混乱了。这个通过key计算位置的算法称为哈希算法（Hash） dict: {&#39;A&#39;: 1, &#39;Z&#39;: -1} 创建方式 1234567a = dict(A=1, Z=-1)b = &#123;'A': 1, 'Z': -1&#125;c = dict(zip(['A', 'Z'], [1, -1]))d = dict([('A', 1), ('Z', -1)])e = dict(&#123;'Z': -1, 'A': 1&#125;)a == b == c == d == e# True 和list比较，dict有以下几个特点： 查找和插入的速度极快，不会随着key的增加而变慢； 需要占用大量的内存，内存浪费多。 而list相反： 查找和插入的时间随着元素的增加而增加； 占用空间小，浪费内存很少。 所以，dict是用空间来换取时间的一种方法 遍历一个dict，实际上是在遍历它的所有的Key的集合，然后用这个Key来获得对应的Value 12345678910111213d = &#123;'Adam': 95, 'Lisa': 85, 'Bart': 59, 'Paul': 75&#125;print d['Adam']# 95print d.get('Jason')# Nonefor key in d : print key, ':', d.get(key)# Lisa : 85# Paul : 75# Adam : 95# Bart : 59 Tupletuple和list非常类似，但是tuple一旦初始化就不能修改 Tuple 的不可变性元组一旦创建，它的元素就是不可变的， 例如如下： 1234t = ('a', 'b', ['A', 'B'])t[2][0] = 'X't[2][1] = 'Y'print t 当我们把list的元素’A’和’B’修改为’X’和’Y’后，tuple变为： Tuple的每个元素，指向永远不变，其中如果某个元素本身是可变的，那么元素内部也是可变的，但是元组的指向却是没有变化 Setset和dict类似，也是一组key的集合，但不存储value。由于key不能重复，所以，在set中，没有重复的key。 set可以看成数学意义上的无序和无重复元素的集合，因此，两个set可以做数学意义上的交集、并集等操作： 1234567s1 = set([1, 2, 3])s2 = set([2, 3, 4])s1 &amp; s2# set([2, 3])s1 | s2# set([1, 2, 3, 4]) set和dict的唯一区别仅在于没有存储对应的value，但是，set的原理和dict一样，所以，同样不可以放入可变对象，因为无法判断两个可变对象是否相等，也就无法保证set内部“不会有重复元素”。试试把list放入set，看看是否会报错。]]></content>
      <tags>
        <tag>Python</tag>
        <tag>Data Structure</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git 常用命令总结]]></title>
    <url>%2F2016%2F09%2F20%2Fgit%E4%BD%BF%E7%94%A8%E5%B0%8F%E7%BB%93%2F</url>
    <content type="text"><![CDATA[Git，目前主流的版本控制工具，git命令是一些命令行工具的集合，它可以用来跟踪，记录文件的变动。比如你可以进行保存，比对，分析，合并等等。 日常使用，一般记住一下6个命令就好了 Workspace：工作区 Index / Stage：暂存区 Repository：仓库区（或本地仓库） Remote：远程仓库 基本的git工作流 在工作目录中修改文件 暂存文件，将文件的快照放入暂存区域 提交更新，找到暂存区域的文件，将快照永久性存储到 Git 仓库目录 新建代码库12345678# 在当前目录新建一个Git代码库$ git init# 新建一个目录，将其初始化为Git代码库$ git init [project-name]# 下载一个项目和它的整个代码历史$ git clone [url] 配置Git的设置文件为 .gitconfig，它可以在用户主目录下（全局配置），也可以在项目目录下（项目配置）。 123456789# 显示当前的Git配置$ git config --list# 编辑Git配置文件$ git config -e [--global]# 设置提交代码时的用户信息$ git config [--global] user.name "[name]"$ git config [--global] user.email "[email address]" 增加/删除文件123456789101112131415161718192021# 添加指定文件到暂存区$ git add [file1] [file2] ...# 添加指定目录到暂存区，包括子目录$ git add [dir]# 添加当前目录的所有文件到暂存区$ git add .# 添加每个变化前，都会要求确认# 对于同一个文件的多处变化，可以实现分次提交$ git add -p# 删除工作区文件，并且将这次删除放入暂存区$ git rm [file1] [file2] ...# 停止追踪指定文件，但该文件会保留在工作区$ git rm --cached [file]# 改名文件，并且将这个改名放入暂存区$ git mv [file-original] [file-renamed] 提交版本现在我们已经添加了这些文件，然后我们将它们提交到仓库。 1$ git commit -m "Adding files" 如果您不使用-m，会出现编辑器来让你写自己的注释信息。当我们修改了很多文件，而不想每一个都add，想commit自动来提交本地修改，我们可以使用-a标识。 1$ git commit -a -m "Changed some files" git commit 命令的-a选项可将所有被修改或者已删除的且已经被git管理的文档提交到仓库中。 分支当你在做一个新功能的时候，最好是在一个独立的区域上开发，通常称之为分支。分支之间相互独立，并且拥有自己的历史记录。这样做的原因是： 稳定版本的代码不会被破坏 不同的功能可以由不同开发者同时开发 开发者可以专注于自己的分支，不用担心被其他人破坏了环境 在不确定之前，同一个特性可以拥有几个版本，便于比较 123456789101112131415161718192021222324252627282930313233343536373839404142# 列出所有本地分支$ git branch# 列出所有远程分支$ git branch -r# 列出所有本地分支和远程分支$ git branch -a# 新建一个分支，但依然停留在当前分支$ git branch [branch-name]# 新建一个分支，并切换到该分支$ git checkout -b [branch]# 新建一个分支，指向指定commit$ git branch [branch] [commit]# 新建一个分支，与指定的远程分支建立追踪关系$ git branch --track [branch] [remote-branch]# 切换到指定分支，并更新工作区$ git checkout [branch-name]# 切换到上一个分支$ git checkout -# 建立追踪关系，在现有分支与指定的远程分支之间$ git branch --set-upstream [branch] [remote-branch]# 合并指定分支到当前分支$ git merge [branch]# 选择一个commit，合并进当前分支$ git cherry-pick [commit]# 删除分支$ git branch -d [branch-name]# 删除远程分支$ git push origin --delete [branch-name]$ git branch -dr [remote/branch] 查看信息123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960# 显示有变更的文件$ git status# 显示当前分支的版本历史$ git log# 显示commit历史，以及每次commit发生变更的文件$ git log --stat# 搜索提交历史，根据关键词$ git log -S [keyword]# 显示某个commit之后的所有变动，每个commit占据一行$ git log [tag] HEAD --pretty=format:%s# 显示某个commit之后的所有变动，其"提交说明"必须符合搜索条件$ git log [tag] HEAD --grep feature# 显示某个文件的版本历史，包括文件改名$ git log --follow [file]$ git whatchanged [file]# 显示指定文件相关的每一次diff$ git log -p [file]# 显示过去5次提交$ git log -5 --pretty --oneline# 显示所有提交过的用户，按提交次数排序$ git shortlog -sn# 显示指定文件是什么人在什么时间修改过$ git blame [file]# 显示暂存区和工作区的差异$ git diff# 显示暂存区和上一个commit的差异$ git diff --cached [file]# 显示工作区与当前分支最新commit之间的差异$ git diff HEAD# 显示两次提交之间的差异$ git diff [first-branch]...[second-branch]# 显示今天你写了多少行代码$ git diff --shortstat "@&#123;0 day ago&#125;"# 显示某次提交的元数据和内容变化$ git show [commit]# 显示某次提交发生变化的文件$ git show --name-only [commit]# 显示某次提交时，某个文件的内容$ git show [commit]:[filename]# 显示当前分支的最近几次提交$ git reflog 远程同步1234567891011121314151617181920212223# 下载远程仓库的所有变动$ git fetch [remote]# 显示所有远程仓库$ git remote -v# 显示某个远程仓库的信息$ git remote show [remote]# 增加一个新的远程仓库，并命名$ git remote add [shortname] [url]# 取回远程仓库的变化，并与本地分支合并$ git pull [remote] [branch]# 上传本地指定分支到远程仓库$ git push [remote] [branch]# 强行推送当前分支到远程仓库，即使有冲突$ git push [remote] --force# 推送所有分支到远程仓库$ git push [remote] --all 撤销12345678910111213141516171819202122232425262728293031# 恢复暂存区的指定文件到工作区$ git checkout [file]# 恢复某个commit的指定文件到暂存区和工作区$ git checkout [commit] [file]# 恢复暂存区的所有文件到工作区$ git checkout .# 重置暂存区的指定文件，与上一次commit保持一致，但工作区不变$ git reset [file]# 重置暂存区与工作区，与上一次commit保持一致$ git reset --hard# 重置当前分支的指针为指定commit，同时重置暂存区，但工作区不变$ git reset [commit]# 重置当前分支的HEAD为指定commit，同时重置暂存区和工作区，与指定commit一致$ git reset --hard [commit]# 重置当前HEAD为指定commit，但保持暂存区和工作区不变$ git reset --keep [commit]# 新建一个commit，用来撤销指定commit# 后者的所有变化都将被前者抵消，并且应用到当前分支$ git revert [commit]# 暂时将未提交的变化移除，稍后再移入$ git stash$ git stash pop 转自阮一峰： http://www.ruanyifeng.com/blog/2015/12/git-cheat-sheet.html]]></content>
      <categories>
        <category>备忘</category>
      </categories>
      <tags>
        <tag>git</tag>
        <tag>github</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Scrapy编写爬虫]]></title>
    <url>%2F2016%2F09%2F18%2F%E4%BD%BF%E7%94%A8Scrapy%E7%BC%96%E5%86%99%E7%88%AC%E8%99%AB%2F</url>
    <content type="text"><![CDATA[Scrapy是Python开发的一个快速,高层次的屏幕抓取和web抓取框架，用于抓取web站点并从页面中提取结构化的数据。Scrapy用途广泛，可以用于数据挖掘、监测和自动化测试。 Scrapy Home Site 1pip install scrapy Scrapy 处理流程图借个图简单介绍下Scrapy处理的流程(这就是框架，帮我们完成了大部分工作) 引擎(Scrapy Engine)，用来处理整个系统的数据流处理，触发事务。 调度器(Scheduler)，用来接受引擎发过来的请求，压入队列中，并在引擎再次请求的时候返回。 下载器(Downloader)，用于下载网页内容，并将网页内容返回给蜘蛛。 蜘蛛(Spiders)，蜘蛛是主要干活的，用它来制订特定域名或网页的解析规则。编写用于分析response并提取item(即获取到的item)或额外跟进的URL的类。 每个spider负责处理一个特定(或一些)网站。 项目管道(Item Pipeline)，负责处理有蜘蛛从网页中抽取的项目，他的主要任务是清晰、验证和存储数据。当页面被蜘蛛解析后，将被发送到项目管道，并经过几个特定的次序处理数据。 下载器中间件(Downloader Middlewares)，位于Scrapy引擎和下载器之间的钩子框架，主要是处理Scrapy引擎与下载器之间的请求及响应。 蜘蛛中间件(Spider Middlewares)，介于Scrapy引擎和蜘蛛之间的钩子框架，主要工作是处理蜘蛛的响应输入和请求输出。 调度中间件(Scheduler Middlewares)，介于Scrapy引擎和调度之间的中间件，从Scrapy引擎发送到调度的请求和响应。 接下来简单介绍一下爬虫的写法，从开发爬虫到部署，把爬取的数据写入mongodb: 创建新的Scrapy工程 定义要抽取的Item对象 编写一个Spider来爬取某个网站并提取出Item对象 编写Item Pipeline来存储提取出来的Item对象 使用Scrapyd-client部署Spider项目 把Aqicn上的北京空气质量的数据爬取下来，为日后分析做准备 创建Scrapy工程在目录下执行命令 1scrapy startproject projectName 创建如下的项目文件夹，目录结构如下 定义Item对象创建一个scrapy.Item类，将所要爬取的字段定义好 12345678910111213141516171819202122import scrapyclass AirqualityItem(scrapy.Item): date = scrapy.Field() hour = scrapy.Field() city = scrapy.Field() # area = scrapy.Field() aqivalue = scrapy.Field() aqilevel = scrapy.Field() pm2_5 = scrapy.Field() pm10 = scrapy.Field() co = scrapy.Field() no2 = scrapy.Field() o3 = scrapy.Field() so2 = scrapy.Field() temp = scrapy.Field() dew = scrapy.Field() pressure = scrapy.Field() humidity = scrapy.Field() wind = scrapy.Field() # add field to log spider crawl time crawl_time = scrapy.Field() 编写SpiderSpider类里面定义如何从一个domain组中爬取数据，包括：初始化url列表、如何跟踪url和如何解析页面提取Item，定义一个Spider，需要继承scrapy.Spider类 name: 定义Spider的名称，以后调用爬虫应用时候使用; start_url: 初始化url; parse(): 解析下载后的Response对象，解析并返回页面数据并提取出相应的Item对象 抽取Item对象内容Scrapy Selector是Scrapy提供的一套选择器，通过特定的XPath或者CSS表达式来选择HTML文件中某个部分 (note: Chrome浏览器自带的copy XPath或者CSS功能非常好用)，在开发过程中，可以使用Scrapy内置的Scrapy-Shell来debug选择器。 爬虫的代码如下 123456789101112131415161718192021222324252627282930313233class AirQualitySpider(CrawlSpider): name = "AqiSpider" download_delay = 2 allowed_domains = ['aqicn.org'] start_urls = ['http://aqicn.org/city/beijing/en/'] def parse(self, response): sel = Selector(response) pm25 = int(sel.xpath('//*[@id="cur_pm25"]/text()').extract()[0]) pm10 = int(sel.xpath('//*[@id="cur_pm10"]/text()').extract()[0]) o3 = int(sel.xpath('//*[@id="cur_o3"]/text()').extract()[0]) no2 = int(sel.xpath('//*[@id="cur_no2"]/text()').extract()[0]) so2 = int(sel.xpath('//*[@id="cur_so2"]/text()').extract()[0]) co = int(sel.xpath('//*[@id="cur_co"]/text()').extract()[0]) item = AirqualityItem() item['date'] = updatetime.strftime("%Y%m%d") item['hour'] = updatetime.hour # strftime("%H%M%S") item['city'] = city item['aqivalue'] = aqivalue item['aqilevel'] = aqilevel item['pm2_5'] = pm25 item['pm10'] = pm10 item['co'] = co item['no2'] = no2 item['o3'] = o3 item['so2'] = so2 item['temp'] = temp item['dew'] = dew item['pressure'] = pressure item['humidity'] = humidity item['wind'] = wind item['crawl_time'] = cur_time yield item 数据存储到MongoDB在Item已经被爬虫抓取之后，Item被发送到Item Pipeline去做更复杂的处理，比如存储到文件中或者数据库中。Item Pipeline常见的用途如下 清洗抓取来的HTML数据 验证抓取来的数据 查询与去除重复数据 将Item存储到数据库中 12345678910111213141516class AirqualityPipeline(object): def __init__(self): connection = pymongo.MongoClient(settings['MONGODB_SERVER'], settings['MONGODB_PORT']) db = connection[settings['MONGODB_DB']] self.collection = db[settings['MONGODB_COLLECTION']] def process_item(self, item, spider): # save data into mongodb valid = True if not item: valid = False raise DropItem("Missing &#123;0&#125;".format(item)) if valid: self.collection.insert(dict(item)) log.msg("an aqi data added to MongoDB database!", level=log.DEBUG, spider=spider) return item 接下来需要在setting.py文件中配置Item Pipeline与数据库信息 12345678ITEM_PIPELINES = &#123; 'AirQuality.pipelines.AirqualityPipeline': 300,&#125;MONGODB_SERVER = "localhost"MONGODB_PORT = 27017MONGODB_DB = "aqihistoricaldata"MONGODB_COLLECTION = "aqidata" 到此，简单的爬虫就已经写好了，可以使用以下命令来抓取相关页面来测试一下这个爬虫 1scrapy crawl AqiSpider 其中，AqiSpider就是在Spider程序中设置的Spider的name属性 防止爬虫被禁的几种方法很多网站都有反爬虫的机制，对于这些网站，可以采用以下的一些办法来绕开反爬虫机制： 使用User Agent池，每次发送请求的时候从池中选取不一样的浏览器头信息 禁止Cookie，有些网站会根据Cookie识别用户身份 设置dowload_delay，频繁请求数据肯定会被禁 使用Scrapyd和Scrapyd-client部署爬虫scrapyd是一个用于部署和运行scrapy爬虫的程序，它允许你通过JSON API来部署爬虫项目和控制爬虫运行。crapyd是一个守护进程，监听爬虫的运行和请求，然后启动进程来执行它们。 安装12pip install scrapydpip install scrapyd-client 启动服务1scrapyd 配置服务器信息编辑scrapy.cfg文件，添加如下内容 123[deploy:MySpider]url = http://localhost:6800/project = AirQuality 其中，MySpider是服务器名称， url是服务器地址 检查配置，列出当前可用的服务器 1scrapyd-deploy -l 部署Spider项目 1scrapyd-deploy MySpider -p AirQuality 部署完成后，在http://localhost:6800 可以看到如下的爬虫部署的监控信息 可以使用curl命令去调用爬虫，也可以使用contab命令来定时的去调用爬虫，来实现定时爬取的任务。 版权声明： 除非注明，本博文章均为原创，转载请以链接形式标明本文地址。]]></content>
      <categories>
        <category>Python</category>
        <category>框架</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何拍摄星空]]></title>
    <url>%2F2016%2F09%2F07%2F%E5%A6%82%E4%BD%95%E6%8B%8D%E6%91%84%E6%98%9F%E7%A9%BA%2F</url>
    <content type="text"><![CDATA[好久没写博客了，重新拾起来吧。今天说点技术无关的话题——摄影 我是一个纯业余的风光狗，向往美丽的大自然，喜欢仰望天空，把深邃的星空拍摄下来，以免子孙后代在严重的光污染中忘记了这篇美丽星空的存在。下面简单介绍一下如何拍星空： 来自知乎我的回答我曾经在西藏拍摄过星空，对于拍摄有一定的体会和经验，我说说拍摄方法吧： 首先，光污染问题。国内很多的地方，尤其是大城市，肉眼几乎看不到几颗星星，更不用说银河了。所以要拍银河星空的话，必须要到完全没有光污染的旷野，当然半夜在荒郊野岭拍照对于心理是个很大的挑战。 关于拍摄器材。要有个大光圈的广角镜头，最好用单反或者SONY的高级微单来拍，要的是机身的优秀高感也就是ISO能力；光圈的话，当然越大越好，因为外界环境很黑暗，所以需要长时间曝光，进光量一定要够；角度越广角越好，广角更能拍出宽阔感。推荐佳能的14L镜头，这个绝对是星空专用镜头。（一定要用三脚架，越稳定越好） 关于拍摄参数。M档，我的常用参数是，光圈f2.8，快门15-20秒，感光度ISO1600-6400关于对焦。我来纠正一下很多人误传的一条知识：手动对焦值无穷远再往回拧一点点。这是绝对的错误，这个一点点到底是多少？没人能说清。所以正确的对焦方法是：打开相机的实时取景功能LV，放大到10倍，然后看星点是否对上焦（也就是实心点不发虚），然后调整曝光参数，试拍两张，再调整感光度，达到满意为止。掌握这几点之后，你肯定能拍出肯漂亮的星空银河了，不过没有前景的话，照片就会很枯燥。所以找个漂亮的前景也很重要，因为使用大光圈，所以前景在构图时候不要离得太近，否则会虚掉。至于补光，我只能说多试几次。关于补光的话，不要一直开着手电筒对着前景照射，否则会强烈过曝，正确的方式是：在按下快门到曝光结束这段时间段的最后一秒，打开手电筒对着前景闪一下，这就足够了。 祝大家都能拍出漂亮的星空照附上在西藏拍的银河和泸沽湖拍的星空 西藏定日县城外小河沟拍的银河 云南泸沽湖边的星空 版权声明： 除非注明，本博文章均为原创，转载请以链接形式标明本文地址。]]></content>
      <tags>
        <tag>摄影</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[备份Hexo源文件至GitHub]]></title>
    <url>%2F2016%2F05%2F31%2F%E5%A4%87%E4%BB%BDHexo%E6%BA%90%E6%96%87%E4%BB%B6%E8%87%B3GitHub%2F</url>
    <content type="text"><![CDATA[本文转自：http://www.leyar.me/backup-your-blog-to-github/ Hexo是一款基于Node.js的静态博客框架，前一阵俺的笔记本泡水直接退役，但是博客的原文件还在那台死去的机器上，所以备份啊。。。本质上，Hexo是将本地的md文件编译成静态文件上传到github上（或者其他），所以建议是将本地的整个Hexo项目（blog）原件同步提交到github或者其他代码托管的站点。 下面记录一下备份、以及在另外的电脑上恢复博客的过程，为了以后备查。 前提已创建有 GitHub 仓库，并且已使用 hexo-deployer-git 部署到 master 分支。（发布博文并托管到Github上）如果不满足请自行 google hexo 部署到 GitHub 的操作方法。 备份过程在Github网站创建一个新仓库(或者使用Github托管博客的仓库，在该仓库下创建一个新的分支)，比如我新建的仓库名为 HEXOSource 在本地hexo根目录中， 初始化git仓库 1git init 创建并切换到名为 hexo_source 的分支 1git checkout -b hexo_source 创建忽略规则文件 .gitignore 1vi .gitignore 按需添加如下内容： 12345678.DS_StoreThumbs.dbdb.json *.log.deploy*/node_modules/.npmignorepublic/ 上面最后一行 public 目录，因其已被 hexo 插件同步到 master 分支里，因此不需要再同步，deploy 是 hexo 的 git 配置存放目录，也不需要同步。其他内容可选择忽略也可以选择同步。 添加内容到仓库并提交到远程仓库 1234git add .git commit -m "first commit"git remote add origin git@github.com:lvraikkonen/HEXOSource.git # 后面仓库目录改成自己新建的。git push -u origin hexo_source 按照以上的步骤就进行了 hexo 源文件的初次备份。以后每次修改了内容之后，都可通过以下几条命令实现同步。 123git add .git commit -m "..." # 双引号内填写更新内容git push origin hexo_source # 或者 git push 通过 git submodule 来同步第三方主题我们一般会选择第三方主题的仓库直接git clone下来。这是一个非常不好的习惯，正确做法是：Fork该第三方主题仓库，这样就会在自己账号下生成一个同名的仓库，并对应一个url，我们应该git clone自己账号下的url。 这样做的原因是：我们很有可能在原来主题基础上做一些自定义的小改动，为了保持多终端的同步，我们需要将这些改动提交到远程仓库。而第三方仓库我们是无法直接push的。 这样就会出现git仓库的嵌套问题，我们通过git submodule来解决这个问题。 1git submodule add git@github.com:lvraikkonen/hexo-theme-next.git themes/next 我们修改主题后: 12git commit -am "refine themes"git push origin hexo_source 然后就完成了第三方主题的备份 在其他电脑同步源文件时，需要执行如下命令来同步主题 12git submodule init // 这句很重要git submodule update 新机器同步在一个新机器上写博客，用以下步骤同步至最新状态 新建博客文件夹 hexo_blog 在该文件夹下初始化git仓库 1git init 为本地仓库添加远程仓库 1git remote add origin git@github.com:lvraikkonen/HEXOSource.git 切换至hexo_source分支 1git checkout -b hexo_source 获取hexo_source分支源文件 1git pull origin hexo_source 然后就是写博客，并将.md博客文件放至_posts文件夹，然后添加修改到本地仓库 123git add .git commit -m "写了一篇博客"git push origin hexo_source 至此，已经完成了博客的撰写并修改了远端仓库的博客源文件，然后使用hexo g和hexo d更新博客就OK啦！ 新机器安装npm失败解决方案由于众所周知的原因，好多东西无法安装，可以添加第三方源来解决 12345678# 添加淘宝源npm install -g cnpm --registry=https://registry.npm.taobao.org# nrm类似包管理器cnpm install nrm -gnrm ls# 使用淘宝nrm use taobaonpm install -g hexo-cli 参考 关于博客同步的解决办法 使用Git Submodule管理子模块]]></content>
      <tags>
        <tag>hexo</tag>
        <tag>备忘</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用d3.js绘图]]></title>
    <url>%2F2015%2F11%2F28%2F%E4%BD%BF%E7%94%A8d3.js%E7%BB%98%E5%9B%BE%E6%B5%81%E6%B0%B4%E8%B4%A6%2F</url>
    <content type="text"><![CDATA[D3的全称是（Data-Driven Documents），是一个Javascript的函数库，主要用途是用HTML和SVG展现数据。下面简单回顾一下我从0出发把csv文件画在HTML页面上的过程。 0. 引入d3.js库引入js库可以直接引用网站上host的js库，也可以下载到本地folder下引入 1&lt;script src="http://d3js.org/d3.v3.min.js" charset="utf-8"&gt;&lt;/script&gt; 1. 创建SCG画布在 SVG 画布的预定义元素里，有六种基本图形： 矩形 圆形 椭圆 线段 折线 多边形 另外，还有一种比较特殊，也是功能最强的元素： 路径 画布中的所有图形，都是由以上七种元素组成。在绘制数据图表的时候，都是操作这几种图形元素。 123456789var margin = &#123;top: 20, right: 20, bottom: 30, left: 50&#125;, width = 960 - margin.left - margin.right, height = 500 - margin.top - margin.bottom;var svg = d3.select("body").append("svg") .attr("width", width + margin.left + margin.right) .attr("height", height + margin.top + margin.bottom) .append("g") .attr("transform", "translate(" + margin.left + "," + margin.top + ")"); 上面代码的意思是，选取HTML代码中的body元素，再后面添加svg画布元素，然后设置宽度高度等属性，svg的g元素类似于div，在这里作为一组元素的容器，后面加入的元素都放在g里面，g可以设置统一的css，里面的子元素会继承可继承css属性。margin和position对g的定位不起作用，只能使用translate通过位移来定位。 2. 定义比例尺对于画布或者图形的长度，不可能全部写死，需要通过数据的大小关系来动态确定，参考地图的比例尺。d3.js中，比例尺需要定义定义域和值域两个属性 有线性比例尺 d3.scale.linear() 和序数比例尺 d3.scale.ordinal() ，线性比例尺针对连续的定义域和值域，序数比例尺针对离散的。 12var xScale = d3.time.scale().range([0, width]);var yScale = d3.scale.linear().range([height, 0]); 这里先定义比例尺的值域，由于定义域需要根据数据来确定，所以写到了后面读取数据的部分。 3. 定义坐标轴d3.js中的坐标轴由 d3.svg.axis() 来实现，svg的坐标原点是左上角，向右为正，向下为正。 123456var xAxis = d3.svg.axis() .scale(xScale) .orient("bottom");var yAxis = d3.svg.axis() .scale(yScale) .orient("left"); x轴是日期，这里使用d3.time在时间和字符串之间做转换。y轴使用普通的线性缩放坐标轴。 4. 读取数据与绑定数据d3.js 中自带了读取csv、json等文件的方法。 123d3.json("data.json", function(error, json)&#123; // process data&#125;; d3.js 中是通过以下两个函数来绑定数据的： datum()：绑定一个数据到选择集上 data()：绑定一个数组到选择集上，数组的各项值分别与选择集的各元素绑定 12345678var data = json;// format date field to datedata.forEach(function(d)&#123; d.date = new Date(d.date); d.close = d.close;&#125;);xScale.domain(d3.extent(data, function(d)&#123; return d.date;&#125;));yScale.domain(d3.extent(data, function(d)&#123; return d.close;&#125;)); 在这里，data数据是一个列表对象，需要对列表中每一条数据的字段数据类型进行定义，之后需要做的是上面提到的定义x轴y轴的比例尺的定义域的定义。 5. 画线图形的主题是一条线，需要添加 path 元素，path的属性决定了线的路径，下面方法定义线的路径属性。 1234567var line = d3.svg.line() .x(function(d) &#123; return xScale(d.date); &#125;) .y(function(d) &#123; return yScale(d.close); &#125;);svg.append("path") .datum(data) .attr("class", "line") .attr("d", function(d)&#123; return line(d);&#125;); 6. 添加坐标轴call() 函数，其参数是前面定义的坐标轴 axis 1234567891011121314// add axissvg.append("g") .attr("class", "x axis") .attr("transform", "translate(0," + height + ")") .call(xAxis);svg.append("g") .attr("class", "y axis") .call(yAxis) .append("text") .attr("transform", "rotate(-90)") .attr("y", 6) .attr("dy", ".71em") .style("text-anchor", "end") .text("Price ($)"); 7. 生成图表上面的js脚本写好了之后，理论上就可以生成折线图了。不过在本地调试中，发现报错：文件没有找到。这个是因为由于安全考虑，浏览器不允许js脚本访问本地文件，解决方法有两个： 在本地开启一个web service 修改浏览器属性，允许访问本地文件 由于以后是要在网站上展示数据，所以我是用Flask在后台开启一个web服务，把d3所需要的数据生成出来 123456789101112131415161718from flask import Flaskfrom flask import render_templateimport jsonimport pandas as pdapp = Flask(__name__)data_path = './sampleData'@app.route("/")def index(): return render_template("line_chart.html")@app.route('/data')def get_data(): with open(data_path + '/line_chart.tsv') as data_file: sample_data = pd.read_csv(data_file, sep='\t') return sample_data.to_json(orient='records') 这样，在本地运行Flask，就可以展现出折线图了。这大约就是d3.js数据可视化的基本过程。 版权声明： 除非注明，本博文章均为原创，转载请以链接形式标明本文地址。]]></content>
      <tags>
        <tag>d3.js</tag>
        <tag>visualization</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Understanding TF-IDF]]></title>
    <url>%2F2015%2F11%2F27%2F2015-11-27-understanding-tf-idf%2F</url>
    <content type="text"></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>Scikit-Learn</tag>
        <tag>Text Mining</tag>
        <tag>Feature Extraction</tag>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Bag of Words Model]]></title>
    <url>%2F2015%2F11%2F26%2F2015-11-26-bag-of-words-model%2F</url>
    <content type="text"></content>
      <categories>
        <category>算法</category>
        <category>Kaggle</category>
      </categories>
      <tags>
        <tag>Scikit-Learn</tag>
        <tag>Kaggle</tag>
        <tag>Text Mining</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Cross-Validation in Scikit-Learn]]></title>
    <url>%2F2015%2F09%2F21%2F2015-09-21-cross-validation-in-scikit-learn%2F</url>
    <content type="text"></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>Python</tag>
        <tag>Scikit-Learn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[About Regularization]]></title>
    <url>%2F2015%2F09%2F18%2F2015-09-18-about-regularization%2F</url>
    <content type="text"><![CDATA[正则化防止过拟合]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>Scikit-Learn</tag>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Logistic Regression原理以及应用]]></title>
    <url>%2F2015%2F08%2F19%2F2015-08-12-logistic-regression-classifier-on-hands%2F</url>
    <content type="text"><![CDATA[逻辑回归算法是一个很有用的分类算法，这篇文章总结一下逻辑回归算法的相关内容。数据使用scikit-learn自带的Iris数据集。 Iris datasetIris数据集，里面包含3种鸢尾花品种的4各属性，这个分类问题可以描述成使用鸢尾花的属性，来判断这个品种倒地属于哪个品种类别。为了简单，这里使用两个类别：Setosa和Versicolor，两个属性：Length和Width 12345678910111213from sklearn import datasetsimport matplotlib.pyplot as pltimport pandas as pdimport numpy as npdata = datasets.load_iris()X = data.data[:100, : 2]y = data.target[:100]setosa = plt.scatter(X[:50, 0], X[:50, 1], c='b')versicolor = plt.scatter(X[50:, 0], X[50:, 1], c='r')plt.xlabel("Sepal Length")plt.ylabel("Seqal Width")plt.legend((setosa, versicolor), ("Setosa", "Versicolor")) 可以看出来，两个品种可以被区分开，接下来要使用一种算法，让计算机把这两个类别区分开。可以想象，可以使用线性回归，也就是画一条线来把两个类别分开，但是这种分割很粗暴，准确性也不高，所以接下来要使用的算法要使用概率的方法区分两个类别，比如，算法返回0.9，那么代表属于类别A的概率是90% 逻辑函数 Logistic Function这里使用的逻辑函数正好符合概率的定义，即函数返回值在[0, 1]区间内，函数又被称作sigmod函数： $$y=\dfrac {1} {1+e^{-x}}$$ 12345x_values = np.linspace(-5, 5, 100)y_values = [1 / (1 + np.exp(-x)) for x in x_values]plt.plot(x_values, y_values)plt.xlabel("X")plt.ylabel("y") 将逻辑函数应用到数据上现在，数据集有两个属性Sepal Length和Sepal Width，这两个属性可以写到如下的等式中： $$x=\theta_{0}+\theta_{1}SW +\theta_{2}SL$$ SL代表Sepal Length这个特征，SW代表Sepal Width这个特征，假如神告诉我们 $\theta_{0} = 1$，$\theta_{1} = 2$，$\theta_{2} = 4$，那么，长度为5并且宽度为3.5的这个品种，$x=1+\left( 2\ast 3.5\right) +\left( 4\ast 5\right) = 28 $，代入逻辑函数： $$\dfrac{1} {1+e^{-28}}=0.99$$ 说明这个品种数据Setosa的概率为99%。那么，告诉我们 $\theta$的取值的神是谁呢？ 算法学习Cost Function在学习线性回归时候，当时使用的是Square Error作为损失函数，那么在逻辑回归中能不能也用这种损失函数呢？当然可以，不过在逻辑回归算法中，使用Square Error作为损失函数是非凸函数，也就是说有多个局部最小值，不能取到全局最小值，所以这里应该使用其他的损失函数。 想象一下，我们假设求出来一个属性的结果值是1，也就是预测为Setosa类别，那么预测为Versicolor类别的概率为0，在全部的数据集上，假设数据都是独立分布的，那么我们的目标就是：把每个单独类别的概率结果值累乘起来，并求最大值： $$\prod_{Setosa}\frac{1}{1 + e^{-(\theta_{0} + \theta{1}SW + \theta_{2}SL)}}\prod_{Versicolor}1 - \frac{1}{1 + e^{-(\theta_{0} + \theta{1}SW + \theta_{2}SL)}}$$ 参考上面定义的逻辑函数： $$h(x) = \frac{1}{1 + e^{-x}}$$ 那么我们的目标函数就是求下面函数的最大值： $$\prod_{Setosa}h(x)\prod_{Versicolor}1 - h(x)$$ 解释一下，加入类别分别为0和1，回归结果$h_\theta(x)$表示样本属于类别1的概率，那么样本属于类别0的概率为 $1-h_\theta(x)$，则有 $$p(y=1|x,\theta)=h_\theta(x)$$ $$p(y=0|x,\theta)=1-h_\theta(x)$$ 可以写为下面公式，含义为：某一个观测值的概率 $$p(y|x,\theta)=h_\theta(x)^y(1-h_\theta(x))^{1-y}$$ 由于各个观测值相互独立，那么联合分布可以表示成各个观测值概率的乘积： $$L(\theta)=\prod_{i=1}^m{h_\theta(x^{(i)})^{y^{(i)}}(1-h_\theta(x^{(i)}))^{1-y^{(i)}}}$$ 上式称为n个观测的似然函数。我们的目标是能够求出使这一似然函数的值最大的参数估计。对上面的似然函数取对数 $$\begin{aligned} l(\theta)=log(L(\theta))=log(\prod_{i=1}^m{h_\theta(x^{(i)})^{y^{(i)}}(1-h_\theta(x^{(i)}))^{1-y^{(i)}}})=\sum_{i=1}^m{(y^{(i)}log(h_\theta(x^{(i)}))+(1-y^{(i)})log(1-h_\theta(x^{(i)})))} \end{aligned}$$ 最大化似然函数，使用梯度下降法，求出$\theta$值，稍微变换一下，那就是求下面式子的最小值 $$J\left( \theta \right) = -\sum_{i=1}^{m}y^{(i)}log(h(x^{(i)})) + (1-y^{(i)})log(1-h(x^{(i)}))$$ 梯度下降算法梯度下降算法为： $$\begin{aligned} \theta_j:=\theta_j-\alpha\frac{\partial J(\theta)}{\partial\theta_j} \end{aligned}$$ 梯度下降算法的推导对$\theta$参数求导，可得 $$\begin{aligned} \frac{\partial logh_\theta(x^{(i)})}{\partial\theta_j}&amp;=\frac{\partial log(g(\theta^T x^{(i)}))}{\partial\theta_j}\&amp;=\frac{1}{g(\theta^T x^{(i)})}{ g(\theta^T x^{(i)})) (1-g(\theta^T x^{(i)}))x_j^{(i)}}\&amp;=(1- g(\theta^T x^{(i)}))) x_j^{(i)}\&amp;=(1-h_\theta(x^{(i)}))x_j^{(i)} \end{aligned}$$ 同理可得， $$\begin{aligned} \frac{\partial(1-logh_\theta(x^{(i)}))}{\partial\theta_j}=-h_\theta(x^{(i)})x_j^{(i)} \end{aligned}$$ 所以 $$\begin{aligned} \frac{\partial l(\theta)}{\partial\theta_j}&amp;=\sum_{i=1}^m{(y^{(i)}(1-h_\theta(x^{(i)}))x_j^{(i)}+(1-y^{(i)})(-h_\theta(x^{(i)})x_j^{(i)}))}\&amp;=\sum_{i=1}^m{(y^{(i)}-h_\theta(x^{(i)}))x_j^{(i)}} \end{aligned}$$ 那么，最终梯度下降算法为： $$\begin{aligned} \theta_j:=\theta_j-\alpha\sum_{i=1}^m{(y^{(i)}-h_\theta(x^{(i)}))x_j^{(i)}} \end{aligned}$$ 注：虽然得到的梯度下降算法表面上看去与线性回归一样，但是这里 的 $h_{\theta }\left( x\right) =\dfrac {1} {1+e^{-\theta ^{T}x}}$ 与线性回归中不同。 梯度下降算法的技巧 变量缩放 (Normalize Variable) $\alpha$选择 设定收敛条件 实现Logigtic Regrssion算法以上，介绍了Logistic Regression算法的详细推导过程，下面就用Python来实现这个算法 首先是逻辑回归函数，也就是sigmoid函数 12def sigmoid(theta, x): return 1.0 / (1 + np.exp(-x.dot(theta))) 然后使用梯度下降算法估算$\theta$值，首先是gradient值 $$(y^{(i)}-h_\theta(x^{(i)}))x_j^{(i)}$$ 123def gradient(theta, x, y): first_part = sigmoid(theta, x) - np.squeeze(y) return first_part.T.dot(x) 损失函数cost function $$-\sum_{i=1}^{m}y^{(i)}log(h(x^{(i)})) + (1-y^{(i)})log(1-h(x^{(i)}))$$ 123456def cost_function(theta, x, y): h_theta = sigmoid(theta, x) y = np.squeeze(y) first = y * np.log(h_theta) second = (1 - y) * np.log(1 - h_theta) return np.mean(-first - second) 梯度下降算法，这种梯度下降算法也叫批量梯度下降(Batch gradient descent) $$\begin{aligned} \theta_j:=\theta_j-\alpha\sum_{i=1}^m{(y^{(i)}-h_\theta(x^{(i)}))x_j^{(i)}} \end{aligned}$$ 123456789101112def gradient_descent(theta, X, y, alpha=0.001, converge): X = (X - np.mean(X, axis=0)) / np.std(X, axis=0) cost_iter = [] cost = cost_function(theta, X, y) cost_iter.append([0, cost]) i = 1 while(change_cost &gt; converge): theta = theta - (alpha * gradient(theta, X, y)) cost = cost_function(theta, X, y) cost_iter.append([i, cost]) i += 1 return theta, np.array(cost_iter) 预测方法 12345def predict_function(theta, x): x = (x - np.mean(x, axis=0)) / np.std(x, axis=0) pred_prob = sigmoid(theta, x) pred_value = np.where(pred_prob &gt;= 0.5, 1, 0) return pred_value 损失函数变化趋势画出cost function的变化趋势，看看是不是已经收敛了 看来梯度下降算法已经收敛了。 使用Scikit-Learn中的Logistic Regression算法Scikit-Learn库中，已经包含了逻辑回归算法，下面用这个工具集来体验一下这个算法。 1234from sklearn import linear_modelmodel = linear_model.LogisticRegression()model.fit(X, y)model.predict(X_test) 其他优化算法 BFGS 随机梯度下降 Stochastic gradient descent L-BFGS Conjugate Gradient]]></content>
      <categories>
        <category>算法</category>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>Python</tag>
        <tag>Scikit-Learn</tag>
        <tag>Algorithm</tag>
        <tag>Logistic Regression</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[单变量线性回归]]></title>
    <url>%2F2015%2F08%2F13%2F2015-08-19-linear-regression-with-one-variable%2F</url>
    <content type="text"><![CDATA[1月份的时候，参加了Coursera上面Andrew Ng的Machine Learning课程，课程断断续续的学，没有透彻的理解、推导，再加上作业使用Octave完成，并且还是有模版的，不是从头到尾做出来的，所以效果很差，虽然拿到了完成证书，但是过后即忘。我觉得是时候从头学习一遍，并且用Python实现所有的作业内容了。 这里写个系列，就当作为这门课程的课程笔记。 机器学习的本质是首先将训练集“喂给”学习算法，进而学习到一个假设(Hypothesis)，然后将特征值作为输入变量输入给Hypothesis，得出输出结果。 线性回归先说一元线性回归，这里只有一个特征值x，Hypothesis可以写为 $$h_{\theta }\left( x\right) =\theta_{0}+\theta_{1}x$$ 代价函数 Cost Function现在假设已经有了这个假设，那么如何评价这个假设的准确性呢，这里用模型预测的值减去训练集中实际值来衡量，这个叫建模误差。线性回归的目标就是使建模误差最小化，从而找出能使建模误差最小化的模型参数。代价函数为： $$J(\theta) = \frac{1}{2m}\sum_{i=1}^m (h_\theta(x^{(i)})-y^{(i)})^2$$ 则目标为求出使得$J(\theta)$最小的$\theta$参数 梯度下降算法现在使用梯度下降算法来求出$\theta$参数，梯度下降算法的推导如下： $$\begin{aligned} \theta_j:=\theta_j-\alpha\frac{\partial J(\theta)}{\partial\theta_j} \end{aligned}$$ 对$\theta_0$的偏导数为： $$\begin{aligned} \frac{\partial}{\partial \theta_0}J(\theta_0, \theta_1) = \frac{1}{m} \sum_{i=1}^m (h_\theta(x^{(i)})-y^{(i)})\end{aligned}$$ 对$\theta_1$的偏导数为： $$\begin{aligned} \frac{\partial}{\partial \theta_1}J(\theta_0, \theta_1) = \frac{1}{m} \sum_{i=1}^m (h_\theta(x^{(i)})-y^{(i)})x^{(i)}\end{aligned}$$ 使用Python实现一元线性回归为了提高性能，使用 numpy 包来实现向量化计算， 12345678import numpy as npdef hypothesis(theta, x): return np.dot(x, theta)def cost_function(theta, x, y): loss = hypothesis(theta, x) - y return np.sum(loss ** 2) / (2 * len(y)) 实现梯度下降算法，需要计算下面四个部分： 计算假设Hypothesis 计算损失 loss = hypothesis - y，然后求出square root 计算Gradient = X’ * loss / m 更新参数theta -= alpha * gradient 1234567891011121314def gradient_descent(alpha, x, y, iters): # number of training dataset m = x.shape[0] theta = np.zeros(2) cost_iter = [] for iter in range(iters): h_theta = hypothesis(theta, x) loss = h_theta - y J = np.sum(loss ** 2) / (2 * m) cost_iter.append([iter, J]) # print "iter %s | J: %.3f" % (iter, J) gradient = np.dot(x.T, loss) / m theta -= alpha * gradient return theta, cost_iter 接下来造一些假数据试验一下 1234567from sklearn.datasets.samples_generator import make_regressionx, y = make_regression(n_samples=100, n_features=1, n_informative=1, random_state=0, noise=35)m, n = np.shape(x)x = np.c_[np.ones(m), x] ## add column value 1 as x0alpha = 0.01theta, cost_iter = gradient_descent(alpha, x, y, 1000)print theta 求出的参数值为[-2.8484052 , 43.202331] 将训练集数据和Hypothesis函数画出来 1234for i in range(x.shape[1]): y_predict = theta[0] + theta[1] * xplt.plot(x[:, 1], y, 'o')plt.plot(x, y_predict, 'k-') 接下来画出代价函数在每次迭代过程中的变化趋势，可以看出算法是否收敛 123plt.plot(cost_iter[:500, 0], cost_iter[:500, 1])plt.xlabel("Iteration Number")plt.ylabel("Cost") 接下来有必要好好复习一下线性代数、numpy和向量化计算。]]></content>
      <categories>
        <category>算法</category>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>Python</tag>
        <tag>Algorithm</tag>
        <tag>Coursera</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[利用Spark进行单词计数]]></title>
    <url>%2F2015%2F08%2F12%2F2015-08-12-word-count-example-in-spark%2F</url>
    <content type="text"><![CDATA[这里就不再介绍Spark了，这篇文章主要记录一下关于Spark的核心RDD的相关操作以及以单词计数这个简单的例子，描述一下Spark的处理流程。 Spark RDDSpark是以RDD概念为中心运行的。RDD是一个容错的、可以被并行操作的元素集合。创建一个RDD有两个方法：在你的驱动程序中并行化一个已经存在的集合；从外部存储系统中引用一个数据集，这个存储系统可以是一个共享文件系统，比如HDFS、HBase或任意提供了Hadoop输入格式的数据来源。 RDD支持两类操作： 转换(Transform) 动作(Action) 还是不翻译的好，下面都用英文描述。Transform：用于从已有的数据集转换产生新的数据集，Transform的操作是Lazy Evaluation的，也就是说这条语句过后，转换并没有发生，而是在下一个Action调用的时候才会返回结果。Action：用于计算结果并向驱动程序返回结果。 演示一下上面两种基本操作： 123lines = sc.textFile("data.txt")lineLength = line.map(lambda x: len(x))totalLength = lineLength.reduce(lambda x, y: x + y) 第一行是有外部存储系统中创建一个RDD对象，第二行定义map操作，是一个Transform操作，由于Lazy Evaluation，对象lineLength并没有立即计算得到。第三行，reduce是一个Action操作，这时，Spark将整个计算过程划分成许多任务在多台机器上并行执行，每台机器运行自己部分的map操作和reduce操作，最终将自己部分的运算结果返回给驱动程序。 12lineLength.persist()# lineLength.cache() 这一行，Spark将lineLength对象保存在内存中，以便后面计算中使用。Spark的一个重要功能就是在将数据集持久化（或缓存）到内存中以便在多个操作中重复使用。 以上就是RDD的一些基本操作，API文档中写的都很清楚，我就不多说了。 统计一篇文档中单词的个数首先，写一个函数，用来计算单词个数 12345def wordCount(wordListRDD): wordCountsCollected = wordListRDD .map(lambda x: (x, 1)) .reduceByKey(lambda x, y: x + y) return wordCountsCollected 使用正则表达式清理原始文本 123456import reimport stringdef removePunctuation(text): regex = re.compile('[%s]' % re.escape(string.punctuation)) return regex.sub('', text).lower().strip()print removePunctuation(' No under_score!') 去读文件内容到RDD中 1234567import os.pathbaseDir = os.path.join('data')inputPath = os.path.join('cs100', 'lab1', 'shakespeare.txt')fileName = os.path.join(baseDir, inputPath)# shakespeareRDD = (sc.textFile(fileName, 8).map(removePunctuation))print '\n'.join(shakespeareRDD.zipWithIndex().map(lambda (l, num): '&#123;0&#125;: &#123;1&#125;'.format(num,l)).take(15)) 这时候，需要把单词通过空格隔开，然后过滤掉为空的内容 1234shakespeareWordsRDD = shakespeareRDD.flatMap(lambda x: x.split())shakespeareWordCount = shakespeareWordsRDD.count()print shakespeareWordsRDD.top(5)shakeWordsRDD = shakespeareWordsRDD 统计出出现次数前15多的单词以及个数： 12top15WordAndCounts = wordCount(shakeWordsRDD).takeOrdered(15, key=lambda (k, v): -v)print '\n'.join(map(lambda (w, c): '&#123;0&#125;: &#123;1&#125;'.format(w, c), top15WordsAndCounts)) 输出结果为： word count the: 27361 and: 26028 i: 20681 to: 19150 of: 17463 a: 14593 you: 13615 my: 12481 in: 10956 that: 10890 is: 9134 not: 8497 with: 7771 me: 7769 it: 7678 Spark是用Scala写出来的，所以可想而知如果用Scala写的效率会比Python高一些，在这儿顺便贴一个Scala版写的WordCount： 123val wordCounts = textFile.flatMap(line =&gt; line.split(" ")) .map(word =&gt; (word, 1)) .reduceByKey((a, b) =&gt; a + b) 真是简洁，Spark真好，嘿嘿~]]></content>
      <categories>
        <category>Big Data</category>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Big Data</tag>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[朴素贝叶斯算法的一些细节和小技巧]]></title>
    <url>%2F2015%2F08%2F11%2F2015-08-11-some-note-about-naive-bayes%2F</url>
    <content type="text"><![CDATA[某特征属性的条件概率为0当特征属性为离散值时，只要统计训练样本中各个划分在每个类别中出现的频率即可用来估计P(a|y)，若某一特征值的概率为0则会使整个概率乘积变为0，这会让分类器的准确性大幅下降。 这时候使用Laplace校准：即假定训练数据库很大，以至于对每个计数加1造成的估计概率的变化忽略不计。 连续分布假定值服从高斯分布(正态分布)当特征属性为连续值时，通常假定其值服从高斯分布，即： $$p\left( x_{i}|y\right) =\dfrac {1} {\sqrt {2\pi \sigma_{y}^{2}}}exp\left( -\dfrac {\left( x_{i}-\mu_{y}\right) ^{2}} {2\sigma_{y}^{2}}\right)$$ 所以，对于连续分布的样本特征的训练就是计算其均值和方差 小数连续相乘实际项目中，概率P往往是值很小的小数，连续的微小小数相乘容易造成下溢出使乘积为0或者得不到正确答案。一种解决办法就是对乘积取自然对数，将连乘变为连加，$\ln \left( AB\right) =\ln A+\ln B$。采用自然对数处理不会带来任何损失，可以避免下溢出或者浮点数舍入导致的错误。]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>Algorithm</tag>
        <tag>Bayes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[配置Octopress支持LaTex数学公式]]></title>
    <url>%2F2015%2F08%2F08%2F2015-08-08-adding-support-for-math-formula%2F</url>
    <content type="text"><![CDATA[Octopress 默认不支持 LaTex 写数学公式需要更改配置才可以。 设置需要使用kramdown来支持LaTex写数学公式 用kramdown替换rdiscount 安装kramdown 1$ sudo gem install kramdown 修改_config.yml配置文件，将所有rdiscount替换成kramdown 修改Gemfile，把gem &#39;rdiscount&#39;换成gem &#39;kramdown&#39; 添加MathJax配置在/source/_includes/custom/head.html文件中，添加如下代码： 123456789101112131415&lt;!-- mathjax config similar to math.stackexchange --&gt;&lt;script type="text/x-mathjax-config"&gt;MathJax.Hub.Config(&#123; jax: ["input/TeX", "output/HTML-CSS"], tex2jax: &#123; inlineMath: [ ['$', '$'] ], displayMath: [ ['$$', '$$']], processEscapes: true, skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'] &#125;, messageStyle: "none", "HTML-CSS": &#123; preferredFont: "TeX", availableFonts: ["STIX","TeX"] &#125;&#125;);&lt;/script&gt;&lt;script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"&gt;&lt;/script&gt; 修复 MathJax 右击页面空白 bug修改~/sass/base/theme.scss文件，如下代码变为： 1234&gt; div#main &#123; background: $sidebar-bg $noise-bg; border-bottom: 1px solid $page-border-bottom; &gt; div &#123; 随之出现的问题，以及解决方法将rdiscount替换成kramdown之后，以前写的博客里面，很多内容都不能正确显示了，并且在rake generate时候，会报错，内容大约是：Error: Pygments can&#39;t parse unknown language: &lt;/p&gt; 原生的语法高亮插件Pygments很强大，支持语言也很多，但是这时候报的错误让人一头雾水。 找出原因报错部分代码在/plugins/pygments_code.rb文件中， 12345678910111213def self.pygments(code, lang) if defined?(PYGMENTS_CACHE_DIR) path = File.join(PYGMENTS_CACHE_DIR, "#&#123;lang&#125;-#&#123;Digest::MD5.hexdigest(code)&#125;.html") if File.exist?(path) highlighted_code = File.read(path) else begin highlighted_code = Pygments.highlight(code, :lexer =&gt; lang, :formatter =&gt; 'html', :options =&gt; &#123;:encoding =&gt; 'utf-8', :startinline =&gt; true&#125;) rescue MentosError raise "Pygments can't parse unknown language: #&#123;lang&#125;." end File.open(path, 'w') &#123;|f| f.print(highlighted_code) &#125; end 修改一下代码，将出问题的代码高亮部分抛出来， 1raise &quot;Pygments can&apos;t parse unknown language: #&#123;lang&#125;#&#123;code&#125;.&quot; Google了一下原因， 原来是因为最新版的pygments这个插件对于Markdown的书写要求更严格了： Some of my older blog posts did not contain a space between the triple-backtick characters and the name of the language being highlighted. Earlier versions of pygments did not care, but the current version is a stickler. pygments appears to want a blank line between any triple-backtick line and any other text in the blog post. 好吧，以后写文章要更细心一点了。:) 参考： Octopress中使用Latex写数学公式 Pygments Unknown Language]]></content>
      <categories>
        <category>备忘</category>
      </categories>
      <tags>
        <tag>Octopress</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[朴素贝叶斯分类器实践]]></title>
    <url>%2F2015%2F08%2F07%2F2015-08-07-naive-bayes-classifier%2F</url>
    <content type="text"><![CDATA[实际案例举个运动员的例子： 如果我问你Brittney Griner的运动项目是什么，她有6尺8寸高，207磅重，你会说“篮球”；我再问你对此分类的准确度有多少信心，你会回答“非常有信心”。我再问你Heather Zurich，6尺1寸高，重176磅，你可能就不能确定地说她是打篮球的了，至少不会像之前判定Brittney那样肯定。因为从Heather的身高体重来看她也有可能是跑马拉松的。最后，我再问你Yumiko Hara的运动项目，她5尺4寸高，95磅重，你也许会说她是跳体操的，但也不太敢肯定，因为有些马拉松运动员也是类似的身高体重。 ——选自A Programmer’s Guide to Data Mining 这里所说的分类，就用到了所谓的概率模型。 朴素贝叶斯算法朴素贝叶斯算法使用每个属性(特征)属于某个类的概率做出预测，这是一个监督性学习算法，对一个预测性问题进行概率建模。训练模型的过程可以看做是对条件概率的计算，何以计算每个类别的相应条件概率来估计分类结果。 这个算法基于一个假设：所有特征相互独立，任意特征的值和其他特征的值没有关联关系，这种假设在实际生活中几乎不存在，但是朴素贝叶斯算法在很多领域，尤其是自然语言处理领域很成功。其他的典型应用还有垃圾邮件过滤等等。 贝叶斯分类器的基本原理 图片引用自Matt Buck 贝叶斯定理给出贝叶斯定理： $$p\left( h|D\right) =\dfrac {p\left( D|h\right) .p\left( h\right) } {p\left( D\right) }$$ 这个公式是贝叶斯方法论的基石。拿分类问题来说，h代表分类的类别，D代表已知的特征d1, d2, d3…，朴素贝叶斯算法的朴素之处在于，假设了特征d1, d2, d3…相互独立，所以贝叶斯定理又能写成： $$p\left( h|f_{1},f_{2}…f_{n}\right) =\dfrac {p\left( h\right) \prod_{i=1}^n p\left( f_{i}|h\right) } {p\left( f_{1},f_{2}…f_{n}\right) }$$ 由于$P\left( f_{1},\ldots f_{n}\right) $ 可视作常数，类变量$h$的条件概率分布就可以表达为： $$p\left( h|f_{1},…,f_{n}\right) =\dfrac {1} {Z}.p\left( h\right) \prod _{i=1}^{n}p\left( F_{i}|h\right)$$ 从概率模型中构造分类器以上，就导出了独立分布的特征模型，也就是朴素贝叶斯模型，使用最大后验概率MAP(Maximum A Posteriori estimation)选出条件概率最大的那个分类，这就是朴素贝叶斯分类器： $$\widehat {y}=\arg \max_{y}p\left( y\right) \prod_{i=1}^{n}p\left( x_{i}|y\right) $$ 参数估计所有的模型参数都可以通过训练集的相关频率来估计。常用方法是概率的最大似然估计，类的先验概率可以通过假设各类等概率来计算（先验概率 = 1 / (类的数量)），或者通过训练集的各类样本出现的次数来估计（A类先验概率=（A类样本的数量）/(样本总数)）。为了估计特征的分布参数，我们要先假设训练集数据满足某种分布或者非参数模型。 常见的分布模型：高斯分布(Gaussian naive Bayes)、多项分布(Multinomial naive Bayes)、伯努利分布(Bernoulli naive Bayes)等. 使用Scikit-learn进行文本分类目的：使用Scikit-learn库自带的新闻信息数据来进行试验，该数据集有19,000个新闻信息组成，通过新闻文本的内容，使用scikit-learn中的朴素贝叶斯算法，来判断新闻属于什么主题类别。参考：Scikit-learn Totorial 数据集123from sklearn.datesets import fetch_20newsgroupsnews = fetch_20newsgroups(subset='all')print news.keys() 查看一下第一条新闻的内容和分组 [‘description’, ‘DESCR’, ‘filenames’, ‘target_names’, ‘data’, ‘target’] From: Mamatha Devineni Ratnam &#x6d;&#x72;&#52;&#x37;&#x2b;&#64;&#x61;&#110;&#x64;&#x72;&#x65;&#119;&#x2e;&#99;&#109;&#117;&#x2e;&#101;&#x64;&#117;Subject: Pens fans reactionsOrganization: Post Office, Carnegie Mellon, Pittsburgh, PA… 10 rec.sport.hockey 划分训练集和测试集，分为80%训练集，20%测试集 123456split_rate = 0.8split_size = int(len(news.data) * split_rate)X_train = news.data[:split_size]y_train = news.target[:split_size]X_test = news.data[split_size:]y_test = news.target[split_size:] 特征提取为了使机器学习算法应用在文本内容上，首先应该把文本内容装换为数字特征。这里使用词袋模型(Bags of words) 词袋模型在信息检索中，Bag of words model假定对于一个文本，忽略其词序和语法，句法，将其仅仅看做是一个词集合，或者说是词的一个组合，文本中每个词的出现都是独立的，不依赖于其他词是否出现，或者说当这篇文章的作者在任意一个位置选择一个词汇都不受前面句子的影响而独立选择的。 Scikit-learn提供了一些实用工具(sklearn.feature_extraction.text)可以从文本内容中提取数值特征 1234567891011from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizerfrom sklearn.feature_extraction.text import TfidfTransformer# Tokenizing textcount_vect = CountVectorizer()X_train_counts = count_vect.fit_transform(X_train)# Tftf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)X_train_tf = tf_transformer.transform(X_train_counts)# Tf_idftfidf_transformer = TfidfTransformer()X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts) 稀疏性大多数文档通常只会使用语料库中所有词的一个子集，因而产生的矩阵将有许多特征值是0（通常99%以上都是0）。例如，一组10,000个短文本（比如email）会使用100,000的词汇总量，而每个文档会使用100到1,000个唯一的词。为了能够在内存中存储这个矩阵，同时也提供矩阵/向量代数运算的速度，通常会使用稀疏表征例如在scipy.sparse包中提供的表征。 训练模型上面使用文本中词的出现次数作为数值特征，可以使用多项分布估计这个特征，使用sklearn.naive_bayes模块的MultinomialNB类来训练模型。 12345678910from sklearn.naive_bayes import MultinomialNB# create classifierclf = MultinomialNB().fit(X_train_tfidf, y_train)docs_new = ['God is love', 'OpenGL on the GPU is fast']X_new_counts = count_vect.transform(docs_new)X_new_tfidf = tfidf_transformer.transform(X_new_counts)# using classifier to predictpredicted = clf.predict(X_new_tfidf)for doc, category in zip(docs_new, predicted): print('%r =&gt; %s' % (doc, news.target_names[category])) 使用Pipline这个类构建复合分类器Scikit-learn为了使向量化 =&gt; 转换 =&gt; 分类这个过程更容易，提供了Pipeline类来构建复合分类器，例如： 1234from sklearn.pipeline import Pipelinetext_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),]) 创建新的训练模型 1234567891011121314151617from sklearn.naive_bayes import MultinomialNBfrom sklearn.pipeline import Pipelinefrom sklearn.feature_extraction.text import TfidfVectorizer, HashingVectorizer, CountVectorizer #nbc means naive bayes classifiernbc_1 = Pipeline([ ('vect', CountVectorizer()), ('clf', MultinomialNB()),])nbc_2 = Pipeline([ ('vect', HashingVectorizer(non_negative=True)), ('clf', MultinomialNB()),])nbc_3 = Pipeline([ ('vect', TfidfVectorizer()), ('clf', MultinomialNB()),])# classifiernbcs = [nbc_1, nbc_2, nbc_3] 交叉验证下面是一个交叉验证函数： 1234567891011from sklearn.cross_validation import cross_val_score, KFoldfrom scipy.stats import semimport numpy as np# cross validation functiondef evaluate_cross_validation(clf, X, y, K): # create a k-fold croos validation iterator of k folds cv = KFold(len(y), K, shuffle=True, random_state=0) # by default the score used is the one returned by score method of the estimator (accuracy) scores = cross_val_score(clf, X, y, cv=cv) print scores print ("Mean score: &#123;0:.3f&#125; (+/-&#123;1:.3f&#125;)").format(np.mean(scores), sem(scores)) 将训练集分为10份，输出验证分数： 12for nbc in nbcs: evaluate_cross_validation(nbc, X_train, y_train, 10) 结果为： CountVectorizer Mean score: 0.849 (+/-0.002) HashingVectorizer Mean score: 0.765 (+/-0.006) TfidfVectorizer Mean score: 0.848 (+/-0.004) 可以看出：CountVectorizer和TfidfVectorizer特征提取的方法要比HashingVectorizer效果好。 优化模型优化单词提取在使用TfidfVectorizer特征提取时候，使用正则表达式，默认的正则表达式是u&#39;(?u)\b\w\w+\b&#39;，使用新的正则表达式ur&quot;\b[a-z0-9_\-\.]+[a-z][a-z0-9_\-\.]+\b&quot; 1234567nbc_4 = Pipeline([ ('vect', TfidfVectorizer( token_pattern=ur"\b[a-z0-9_\-\.]+[a-z][a-z0-9_\-\.]+\b",) ), ('clf', MultinomialNB()),])evaluate_cross_validation(nbc_4, X_train, y_train, 10) 分数是：Mean score: 0.861 (+/-0.004) ，结果好了一点 排除停止词TfidfVectorizer的一个参数stop_words，这个参数指定的词将被省略不计入到标记词的列表中，这里使用鼎鼎有名的NLTK语料库。 1234567891011import nltk# nltk.download()stopwords = nltk.corpus.stopwords.words('english')nbc_5 = Pipeline([ ('vect', TfidfVectorizer( stop_words=stop_words, token_pattern=ur"\b[a-z0-9_\-\.]+[a-z][a-z0-9_\-\.]+\b", )), ('clf', MultinomialNB()),])evaluate_cross_validation(nbc_5, X_train, Y_train, 10) 分数是：Mean score: 0.879 (+/-0.003)，结果又提高了 调整贝叶斯分类器的alpha参数MultinomialNB有一个alpha参数，该参数是一个平滑参数，默认是1.0，我们将其设为0.01 12345678nbc_6 = Pipeline([ ('vect', TfidfVectorizer( stop_words=stopwords, token_pattern=ur"\b[a-z0-9_\-\.]+[a-z][a-z0-9_\-\.]+\b", )), ('clf', MultinomialNB(alpha=0.01)),])evaluate_cross_validation(nbc_6, X_train, y_train, 10) 分数为：Mean score: 0.917 (+/-0.002)，哎呦，好像不错哦！不过问题来了，调整参数优化不能靠蒙，如何寻找最好的参数，使得交叉验证的分数最高呢？ 使用Grid Search优化参数使用GridSearch寻找vectorizer词频统计, tfidftransformer特征变换和MultinomialNB classifier的最优参数 Scikit-learn上关于GridSearch的介绍 1234567891011121314151617pipeline = Pipeline([('vect',CountVectorizer()),('tfidf',TfidfTransformer()),('clf',MultinomialNB()),]);parameters = &#123; 'vect__max_df': (0.5, 0.75), 'vect__max_features': (None, 5000, 10000), 'tfidf__use_idf': (True, False), 'clf__alpha': (1, 0.1, 0.01, 0.001, 0.0001),&#125;grid_search = GridSearchCV(pipeline, parameters, n_jobs=1)from time import timet0 = time()grid_search.fit(X_train, y_train)print "done in %0.3fs" % (time() - t0)print "Best score: %0.3f" % grid_search.best_score_ 输出最优参数1234567891011from sklearn import metricsbest_parameters = dict()best_parameters = grid_search.best_estimator_.get_params()for param_name in sorted(parameters.keys()): print "\t%s: %r" % (param_name, best_parameters[param_name])pipeline.set_params(clf__alpha = 1e-05, tfidf__use_idf = True, vect__max_df = 0.5, vect__max_features = None)pipeline.fit(X_train, y_train)pred = pipeline.predict(X_test) 经过漫长的等待，终于找出了最优参数： done in 1578.965sBest score: 0.902 clfalpha: 0.01tfidfuse_idf: Truevectmax_df: 0.5vectmax_features: None 在测试集上的准确率为：0.915，分类效果还是不错的 1print np.mean(pred == y_test) 评价分类效果在测试集上测试朴素贝叶斯分类器的分类效果 12345678from sklearn import metricsimport numpy as np#print X_test[0], y_test[0]for i in range(20): print str(i) + ": " + news.target_names[i]predicted = pipeline.fit(X_train, y_train).predict(X_test)print np.mean(predicted == y_test)print metrics.classification_report(y_test, predicted) 结果是这样的： id groupname 0 alt.atheism 1 comp.graphics 2 comp.os.ms-windows.misc 3 comp.sys.ibm.pc.hardware 4 comp.sys.mac.hardware 5 comp.windows.x 6 misc.forsale 7 rec.autos 8 rec.motorcycles 9 rec.sport.baseball 10 rec.sport.hockey 11 sci.crypt 12 sci.electronics 13 sci.med 14 sci.space 15 soc.religion.christian 16 talk.politics.guns 17 talk.politics.mideast 18 talk.politics.misc 19 talk.religion.misc 准确率：0.922811671088 id precision recall f1-score support 0 0.94 0.87 0.91 175 1 0.85 0.87 0.86 199 2 0.91 0.84 0.88 221 3 0.81 0.87 0.84 179 4 0.87 0.92 0.89 177 5 0.91 0.92 0.91 179 6 0.88 0.79 0.83 205 7 0.94 0.95 0.94 228 8 0.96 0.98 0.97 183 9 0.96 0.95 0.96 197 10 0.98 1.00 0.99 204 11 0.96 0.98 0.97 218 12 0.93 0.92 0.92 172 13 0.93 0.95 0.94 200 14 0.96 0.96 0.96 198 15 0.93 0.97 0.95 191 16 0.92 0.97 0.94 173 17 0.98 0.99 0.98 184 18 0.95 0.92 0.94 172 19 0.83 0.78 0.81 115 avg / total 0.92 0.92 0.92 3770 参考 JasonDing的 【机器学习实验】使用朴素贝叶斯进行文本的分类； Scikit-Learn Totorial]]></content>
      <categories>
        <category>算法</category>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>Python</tag>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[开始使用Scikit-Learn]]></title>
    <url>%2F2015%2F07%2F23%2F2015-07-23-start-to-use-scikit-learn%2F</url>
    <content type="text"><![CDATA[Python和R是做数据分析、数据挖掘、机器学习非常好的两门语言，在这儿不去讨论谁更好这个问题，没有最好，只有合适上手。对于码农出身，非科班统计学的我来说，使用Python相当习惯和顺手。 Python数据科学栈Python有很多做数据的类库，先列出常用的几个： Numpy、Scipy 基础数据类型 Matplotlib 绘图库 Pandas Ipython notebook Scikit-learn、MLlib 机器学习库 使用Scikit-learn的过程数据加载首先，将数据加载到内存中 12345678import numpy as npimport urlliburl = "http://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data"# download the fileraw_data = urllib.urlopen(url)dataset = np.loadtxt(raw_data, delimiter=',')X = dataset[:, 0:7]y = dataset[:, 8] X为特征数组，y为目标变量 数据标准化 (Data Normalization)大多数的梯度算法对数据的缩放很敏感，比如列A是体重数据(50kg等等)，列B是身高数据(170cm等等)，简单地说就是两个属性尺度不一样，所以在运行算法前要进行标准化或者叫归一化。 123from sklearn import preprocessing# normalize the data attributesnormalized_X = preprocessing.normalize(X) 特征选取虽然特征工程是一个相当有创造性的过程，有时候更多的是靠直觉和专业的知识，但对于特征的选取，已经有很多的算法可供直接使用。Scikit-Learn中的Recursive Feature Elimination Algorithm算法： 12345678from sklearn.feature_selection import RFEfrom sklearn.linear_model import LogisticRegressionmodel = LogisticRegression()# create RFE model and select 3 attributesrfe = RFE(model, 3)rfe = rfe.fit(X, y)print rfe.support_print rfe.ranking_ 机器学习算法看一看Scikit-learn库中所带的算法： 逻辑回归算法 Logistic Regression 朴素贝叶斯算法 Naive Bayes k-最邻算法 KNN 决策树 Decision Tree 支持向量机 SVM 逻辑回归 大多数情况下被用来解决分类问题（二元分类），但多类的分类（所谓的一对多方法）也适用。这个算法的优点是对于每一个输出的对象都有一个对应类别的概率。 1234567891011from sklearn import metricsfrom sklearn.linear_model import LogisticRegressionmodel = LogisticRegression()model.fit(X, y)print(model)# make predictionsexpected = ypredicted = model.predict(X)# summarize the fit of the modelprint(metrics.classification_report(expected, predicted))print(metrics.confusion_matrix(expected, predicted)) 朴素贝叶斯 它也是最有名的机器学习的算法之一，它的主要任务是恢复训练样本的数据分布密度。这个方法通常在多类的分类问题上表现的很好。 1234567891011from sklearn import metricsfrom sklearn.naive_bayes import GaussianNBmodel = GaussianNB()model.fit(X, y)print(model)# make predictionsexpected = ypredicted = model.predict(X)# summarize the fit of the modelprint(metrics.classification_report(expected, predicted))print(metrics.confusion_matrix(expected, predicted)) k-最近邻 kNN（k-最近邻）方法通常用于一个更复杂分类算法的一部分。例如，我们可以用它的估计值做为一个对象的特征。有时候，一个简单的kNN算法在良好选择的特征上会有很出色的表现。当参数（主要是metrics）被设置得当，这个算法在回归问题中通常表现出最好的质量。 123456789101112from sklearn import metricsfrom sklearn.neighbors import KNeighborsClassifier# fit a k-nearest neighbor model to the datamodel = KNeighborsClassifier()model.fit(X, y)print(model)# make predictionsexpected = ypredicted = model.predict(X)# summarize the fit of the modelprint(metrics.classification_report(expected, predicted))print(metrics.confusion_matrix(expected, predicted)) 决策树 分类和回归树（CART）经常被用于这么一类问题，在这类问题中对象有可分类的特征且被用于回归和分类问题。决策树很适用于多类分类。 123456789101112from sklearn import metricsfrom sklearn.tree import DecisionTreeClassifier# fit a CART model to the datamodel = DecisionTreeClassifier()model.fit(X, y)print(model)# make predictionsexpected = ypredicted = model.predict(X)# summarize the fit of the modelprint(metrics.classification_report(expected, predicted))print(metrics.confusion_matrix(expected, predicted)) 支持向量机SVM SVM（支持向量机）是最流行的机器学习算法之一，它主要用于分类问题。同样也用于逻辑回归，SVM在一对多方法的帮助下可以实现多类分类。 123456789101112from sklearn import metricsfrom sklearn.svm import SVC# fit a SVM model to the datamodel = SVC()model.fit(X, y)print(model)# make predictionsexpected = ypredicted = model.predict(X)# summarize the fit of the modelprint(metrics.classification_report(expected, predicted))print(metrics.confusion_matrix(expected, predicted)) 评价算法TBD 评价算法的好坏大约有几个方面： precision recall F1 score 优化算法的参数TBD]]></content>
      <categories>
        <category>算法</category>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>Python</tag>
        <tag>Scikit-Learn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Install Spark on Mac OSX Yosemite]]></title>
    <url>%2F2015%2F07%2F17%2F2015-07-17-install-spark-on-mac-osx-yosemite%2F</url>
    <content type="text"><![CDATA[Spark是个好东西。 Spark有以下四种运行模式： local: 本地单进程模式，用于本地开发测试Spark代码 standalone：分布式集群模式，Master-Worker架构，Master负责调度，Worker负责具体Task的执行 on yarn/mesos: ‌运行在yarn/mesos等资源管理框架之上，yarn/mesos提供资源管理，spark提供计算调度，并可与其他计算框架(如MapReduce/MPI/Storm)共同运行在同一个集群之上 on cloud(EC2): 运行在AWS的EC2之上 在Spark上又有多个应用，尤其是MLlib，Spark SQL和DataFrame，提供给数据科学家们无缝接口去搞所谓Data Science 本文记录一下我在Mac上安装Spark单机为分布式的过程 1.安装环境Spark依赖JDK 6.0以及Scala 2.9.3以上版本，安装好Java和Scala，然后配置好Java、Scala环境，最后再用java -version和scala -version验证一下 在~/.bash_profile中加入： 123# Setting PATH for scalaexport SCALA_HOME=/usr/local/Cellar/scala/2.11.6export PATH=$SCALA_HOME/bin:$PATH 别忘了 1source ~/.bash_profile 生效 由于在后面学习中主要会用到Spark的Python接口pyspark，所以在这儿也需要配置Python的环境变量： 1234# Setting PATH for Python 2.7# The orginal version is saved in .bash_profile.pysavePATH="/usr/local/Cellar/python/2.7.9/bin:$&#123;PATH&#125;"export PATH 2.伪分布式安装Spark的安装和简单，只需要将Spark的安装包download下来，加入PATH即可。这里我用的是Spark 1.4.0 当然，这里也可以使用Homebrew安装，那就更轻松了，直接 1$ brew install apache-spark 就搞定了，不过Homebrew安装没办法自己控制箱要安装的版本 这里我使用下载对Hadoop2.6的预编译版本安装 123cd /usr/local/Cellar/wget http://www.apache.org/dyn/closer.cgi/spark/spark-1.4.0/spark-1.4.0-bin-hadoop2.6.tgztar zxvf spark-1.4.0-bin-hadoop2.6.tgz 设置Spark环境变量，~/.bash_profile： 123456export SPARK_MASTER=localhostexport SPARK_LOCAL_IP=localhostexport SPARK_HOME=/usr/local/Cellar/spark-1.4.0-bin-hadoop2.6export PATH=$PATH:$SCALA_HOME/bin:$SPARK_HOME/binexport PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.8.2.1-src.zip:$PYTHONPATHexport PATH="/usr/local/sbin:$PATH" 安装完成，貌似也没什么安装哈~ 跑起来执行Spark根目录下的pyspark就可以以交互的模式使用Spark了，这也是他的一个优点 出现Spark的标志，那就说明安装成功了。下面再小配置下，让画面的log简单一点。在$SPARK_HOME/conf/下配置一下log4j的设置。 把log4j.properties.template文件复制一份，并删掉.template的扩展名 把这个文件中的INFO内容全部替换成WARN 在IPython中运行Spark说Spark好，那么IPython更是一大杀器，这个以后再介绍。先说设置 首先，创建IPython的Spark配置 1$ ipython profile create pyspark 然后创建文件$HOME/.ipython/profile_spark/startup/00-pyspark-setup.py并添加： 12345678import osimport sysspark_home = os.environ.get('SPARK_HOME', None)if not spark_home: raise ValueError('SPARK_HOME environment variable is not set')sys.path.insert(0, os.path.join(spark_home, 'python'))sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.8.2.1-src.zip'))execfile(os.path.join(spark_home, 'python/pyspark/shell.py')) 在IPython notebook中跑Spark 1$ ipython notebook --profile=pyspark 开始学习Spark吧！ 参考： Getting Started with Spark (in Python)]]></content>
      <categories>
        <category>Big Data</category>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Big Data</tag>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Install Hadoop on Yosemite]]></title>
    <url>%2F2015%2F07%2F17%2F2015-07-17-install-hadoop-on-mac-osx-yosemite%2F</url>
    <content type="text"><![CDATA[终于进入正题，开始写一写我在大数据方面走过的路，自认为被其他人甩下了，所以一定要紧追而上。 首先现在我的Mac上装上单节点的Hadoop玩玩，个人感觉Apache系列的项目，只要download下来，再配置以下参数就能玩了。 在这里感谢如下教程： INSTALLING HADOOP ON MAC Writing an Hadoop MapReduce Program in Python 下面开始吧 准备这个阶段主要就是准备一下JAVA的环境，Mac默认是安装了Java的，不过版本就不知道了，这个还是自己安装一下并且写到环境变量里来得踏实 Java Download 安装完之后，Java被装到了这个位置1/Library/Java/JavaVirtualMachines/jdk1.7.0_79.jdk/Contents/Home ，把这个地址写到系统的环境变量文件.bash_profile里 123# Setting PATH for javaexport JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.7.0_79.jdk/Contents/Homeexport PATH=$JAVA_HOME/bin:$PATH 配置SSH Nothing needs to be done here if you have already generated ssh keys. To verify just check for the existance of ~/.ssh/id_rsa and the ~/.ssh/id_rsa.pub files. If not the keys can be generated using 1$ ssh-keygen -t rsa Enable Remote Login “System Preferences” -&gt; “Sharing”. Check “Remote Login”Authorize SSH KeysTo allow your system to accept login, we have to make it aware of the keys that will be used 1$ cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys Let’s try to login. 123$ ssh localhostLast login: Fri Mar 6 20:30:53 2015$ exit 安装Homebrew在Mac上，最好的包安装工具就是Homebrew，执行下面代码安装： 1$ ruby -e "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)" 安装Hadoop我去，这么简单，直接 1$ brew install hadoop 就搞定了。。。这样，Hadoop会被安装在/usr/local/Cellar/hadoop目录下 下面才是重点，配置Hadoop 配置Hadoophadoop-env.sh该文件在/usr/local/Cellar/hadoop/2.6.0/libexec/etc/hadoop/hadoop-env.sh 找到如下这行： 1export HADOOP_OPTS="$HADOOP_OPTS -Djava.net.preferIPv4Stack=true" 改为： 1export HADOOP_OPTS="$HADOOP_OPTS -Djava.net.preferIPv4Stack=true -Djava.security.krb5.realm= -Djava.security.krb5.kdc=" Core-site.xml该文件在/usr/local/Cellar/hadoop/2.6.0/libexec/etc/hadoop/core-site.xml 123456789&lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/usr/local/Cellar/hadoop/hdfs/tmp&lt;/value&gt; &lt;description&gt;A base for other temporary directories.&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;fs.default.name&lt;/name&gt; &lt;value&gt;hdfs://localhost:9000&lt;/value&gt; &lt;/property&gt; mapred-site.xml文件在/usr/local/Cellar/hadoop/2.6.0/libexec/etc/hadoop/mapred-site.xml 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapred.job.tracker&lt;/name&gt; &lt;value&gt;localhost:9010&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; hdfs-site.xml文件在/usr/local/Cellar/hadoop/2.6.0/libexec/etc/hadoop/hdfs-site.xml 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 这就配置好了 添加启动关闭Hadoop快捷命令为了以后方便使用Hadoop，在.bash_profile中添加 12alias hstart="/usr/local/Cellar/hadoop/2.6.0/sbin/start-dfs.sh;/usr/local/Cellar/hadoop/2.6.0/sbin/start-yarn.sh"alias hstop="/usr/local/Cellar/hadoop/2.6.0/sbin/stop-yarn.sh;/usr/local/Cellar/hadoop/2.6.0/sbin/stop-dfs.sh" 执行下面命令将配置生效： 1source ~/.bash_profile 以后就能使用命令hstart启动Hadoop服务，hstop关闭Hadoop 格式化HDFS在使用Hadoop之前，还需要将HDFS格式化 1$ hdfs namenode -format Running Hadoop奔跑吧Hadoop 1$ hstart 使用jps命令查看Hadoop运行状态 1234567$ jps18065 SecondaryNameNode18283 Jps17965 DataNode18258 NodeManager18171 ResourceManager17885 NameNode 下面是几个很有用的监控Hadoop地址： Resource Manager: http://localhost:50070 JobTracker: http://localhost:8088 Specific Node Information: http://localhost:8042 停止Hadoop： 1$ hstop 添加Hadoop环境变量为了以后安装Spark等方便，在~/.bash_profile配置中添加Hadoop环境变量 12345# Setting PATH for hadoopexport HADOOP_HOME=/usr/local/Cellar/hadoop/2.6.0/libexecexport PATH=$HADOOP_HOME/bin:$PATHexport HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop 可能遇到的问题跑起来之后，或者在跑起来的过程中，可能会遇到各种问题，由于控制台命令太多，很难知道到底是哪儿出的问题，所以我总结出几个我遇到的问题和解决方法，分享给大家。TBD NameNode启动失败 大功告成！可以在Hadoop上跑几个MapReduce任务了。]]></content>
      <categories>
        <category>Big Data</category>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Big Data</tag>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在不同的机器上写博客]]></title>
    <url>%2F2015%2F07%2F17%2F2015-07-17-write-blog-from-different-machine%2F</url>
    <content type="text"><![CDATA[在家的Mac上配置好了Octopress，上班到公司还是要面对大Windows，这时候，想写一篇博客记录一下遇到的问题，怎么办？ Octopress原理Octopress的git仓库(repository)有两个分支，分别是master和source。master存储的是博客网站本身（html静态页面），而source存储的是生成博客的源文件（包括配置等）。master的内容放在根目录的_deploy文件夹内，当你push源文件时会忽略，它使用的是rake deploy命令来更新的。 下面开始在一台新机器上搞 创建一个本地Octopress仓库将博客的源文件，也就是source分支clone到本地的Octopress文件夹内 1$ git clone -b source git@github.com:username/username.github.com.git octopress 然后将博客文件也就是master分支clone到Octopress文件夹的_deploy文件夹内 12$ cd octopress$ git clone git@github.com:username/username.github.com.git _deploy 然后安装博客 123$ gem install bundler$ bundle install$ rake setup_github_pages OK了 继续写博客当你要在一台电脑写博客或做更改时，首先更新source仓库 1234$ cd octopress$ git pull origin source # update the local source branch$ cd ./_deploy$ git pull origin master # update the local master branch 写完博客之后不要忘了push，下面的步骤在每次更改之后都必须做一遍。 12345$ rake generate$ git add .$ git commit -am "Some comment here." $ git push origin source # update the remote source branch $ rake deploy # update the remote master branch]]></content>
      <categories>
        <category>备忘</category>
      </categories>
      <tags>
        <tag>Octopress</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Github与Octopress写博客]]></title>
    <url>%2F2015%2F07%2F16%2F2015-07-16-Writing%20Blogs%20with%20Github%20Pages%20and%20Octopress%2F</url>
    <content type="text"><![CDATA[Octopress和Github Pages是什么？ Octopress是一个基于Ruby语言的开源静态网站框架 Github Pages是Github上的一项服务， 注册用户可以申请一个和自己账号关联的二级域名， 在上面可以托管一个静态网站，网站内容本身就是Github的一个repository也就是项目， 维护这个项目的代码就是在维护自己的网站。简单来说就是 yourname.github.io/ 使用Octopress搭建博客，然后使用Github托管，有以下几个原因： 免费 版本控制，可以使用git实现写文章、建网站时候修改的版本控制 Octopress容易上手，并且这个风格正是我喜欢的，尤其对于一个理工男 使用Markdown，markdown是世界上最流行的轻量级标记语言 很酷，能装逼，当然这不是重点 搭建Octopress博客系统Note: 在这儿写的是关于在Mac上安装Octopress博客系统，和windows有细微的差别，不过个人觉得还是用Mac，无论写代码还是做黑客都更专业。 安装基本工具git 对于Mac来说，安装XCode之后，自带了git，可以使用下面命令检查本机的git版本 1$ git version Ruby Mac本身自带Ruby，但是也许版本过低，在这儿多说一句：有时候在低版本ruby下搭建好的Octopress，莫名其妙不好用了，原因也许就是Mac升级之后，ruby也升级了，要注意一下 至于Mac下如何使用Homebrew安装，请查看其他文章。 12$ brew install ruby$ ruby --version Ruby版本在1.9.3以上就可以了，就可以使用gem来安装Ruby的包了 PS. gem在Ruby中，相当于Python中的pip 由于我们生活在一个伟大的国家，so在下一步安装前，先更改一下gem的更新源，改为淘宝的源 123gem sources -a http://ruby.taobao.org/gem sources -r http://rubygems.org/gem sources -l 三行命令的作用分别是：添加淘宝源；删除默认源；显示当前源列表。显示淘宝地址就表示成功。 安装bundle和bundler， 12gem install bundlegem install bundler Note: 安装配置完新版本的Ruby后，一定要重新安装bundle和bundler，否则bundle仍会bundler指向旧版本的Ruby，PS. 由于手贱，把MAC升级到最新系统了，结果各种奇妙的事情就发生了，不过处理方法一般都是：安装最新版本的Ruby，然后再安装bundler和bundler Octopress 这个就是我们要使用的框架，它是基于Jekyll的一个静态博客生成框架，Jekyll是一个静态网站生成框架，它有很多功能，也可以直接使用，但是就麻烦得多，很多东西要配置和从头写。 1234git clone git://github.com/imathis/octopress.git octopresscd octopressbundle installrake install 创建Github账号和Github Pages 大多数人都已经有了Github帐号了，访问Github来注册帐号，然后访问Github Pages来创建博客空间，唯一需要注意的是Repo必须是Github帐号.github.io，否则不会起作用。 然后运行： 1rake setup_github_pages 输入Github Page的Repo的地址，例如：git@github.com:username/username.github.io.git，就可以了 测试一下输入命令生成页面 1rake generate 生成完毕后，使用以下命令启动网站进程，默认占用4000端口， preview一下 1rake preview 可以使用 http://localhost:4000 访问你的博客页面了 配置博客配置文件是根目录下的 _config.yml文件，使用vim或者其他文本编辑器编辑它吧 12345678910# ----------------------- ## Main Configs ## ----------------------- #url: http://lvraikkonen.github.iotitle: My Data Science Pathsubtitle: Shut up, just codingauthor: Claus Lvsimple_search: https://www.google.com/searchdescription: 语法高亮例子： 1alert("欢迎") 1234import mathprint "Hello World"lst = range(100)print lst.map(lambda x: x**2) 123456789101112131415&lt;!-- mathjax config similar to math.stackexchange --&gt;&lt;script type="text/x-mathjax-config"&gt; MathJax.Hub.Config(&#123; jax: ["input/TeX", "output/HTML-CSS"], tex2jax: &#123; inlineMath: [ ['$', '$'] ], displayMath: [ ['$$', '$$']], processEscapes: true, skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'] &#125;, messageStyle: "none", "HTML-CSS": &#123; preferredFont: "TeX", availableFonts: ["STIX","TeX"] &#125; &#125;);&lt;/script&gt;&lt;script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"&gt;&lt;/script&gt; 添加社交分享Octopress默认是带有社交分享功能的，比如Twitter, Facebook, Google Plus等，但这些全世界都通用的东西在我大天朝就是不好使。 网站页的分享有很多第三方的库，这里用jiathis 在_config.yml中加入social_share: true 修改/source/_includes/post/sharing.html 访问jiathis获取分享的代码，放入新建的/source/_includes/post/social_media.html 添加博客评论Octopress也默认集成有评论系统Disqus，这个是国外最大的第三方评论平台，世界都在用，除了我大天朝。这里使用多说 到多说注册，获取用户名，也就是在多说上添的youname.duoshuo.com中的yourname 在_config.yml中添加 12duoshuo_comments: trueduoshuo_short_name: yourname 在/source/_layouts/post.html中把评论模版添加到网页中 创建/source/_includes/post/duoshuo.html，将上步获取的HTML代码放进去 国内访问加速jQuery源 修改jQuery的源 打开source/_includes/head.html，找到如下 1&lt;script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"&gt;&lt;/script&gt; 改为： 1&lt;script src="//ajax.aspnetcdn.com/ajax/jQuery/jquery-1.9.1.min.js"&gt;&lt;/script&gt; 字体源 Octopress的英文字体是加载的Google Fonts，我们将其改成国内的CDN源， 打开source/_includes/custom/head.html, 将其中的https://fonts.googleapis.com改为http://fonts.useso.com Twitter Facebook Google+关闭 在前面提到的_config.yml中相关的例如twitter_tweet_butto改为false 写博客要发布一篇新文章，在命令行中输入以下命令： 1rake new_post["postName"] 之后在/source/_post/里面就有该博文的markdown文件了，使用Markdown文本编辑器写博客吧 rake generate 生成静态的博客文件，生成的文件在_deploy中 rake preview 在本地预览博客，这与发布到Github Pages后的效果是一样的 rake deploy 这是最后一步，就是把Octopress生成的文件（在_deploy）发布到Github上面去。这里的实际是Octopress根据你的配置用sources中的模板，生成网页（HTML，JavaScript, CSS和资源），再把这些资源推送到yourname.github.io这个Repo中去，然后访问https://*yourname*.github.io 就能看到你的博客了 发布执行命令 12$ rake generate$ rake deploy 第一行命令用来生成页面，第二行命令用来部署页面，上述内容完成，就可以访问 http://[your_username].github.io/看博客了 Note: octopress 根目录为source分支， _deploy目录下为master分支，rake deploy时候会把_deploy下的内容发布到github上的master分支。别忘了把源文件（包括配置等）发布到source分支下 push时候可用 1$ git status 查看状态 执行以下命令，将源文件发布到Github的source分支 123git add .git commit -m "备注内容"git push origin source 如果遇到类似error: failed to push some refs to的错误，参考 stackoverflow解决]]></content>
      <categories>
        <category>备忘</category>
      </categories>
      <tags>
        <tag>Octopress</tag>
      </tags>
  </entry>
</search>