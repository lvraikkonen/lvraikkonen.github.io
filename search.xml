<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[SQL Server索引]]></title>
    <url>%2F2017%2F06%2F02%2FSQL-Server%E7%B4%A2%E5%BC%95%2F</url>
    <content type="text"><![CDATA[索引是与表或视图关联的磁盘上结构，可以加快从表或视图中检索行的速度。在SQL Server中，索引和表（这里指的是加了聚集索引的表）的存储结构是一样的,都是B树，B树是一种用于查找的平衡多叉树。 索引的利弊查询执行的大部分开销是I/O，使用索引提高性能的一个主要目标是避免全表扫描，因为全表扫描需要从磁盘上读取表的每一个数据页，如果有索引指向数据值，则查询只需要读少数次的磁盘就行啦。所以合理的使用索引能加速数据的查询。但是索引并不总是提高系统的性能，带索引的表需要在数据库中占用更多的存储空间，同样用来增删数据的命令运行时间以及维护索引所需的处理时间会更长。所以我们要合理使用索引，及时更新去除次优索引。 数据表的基本结构一个新表被创建之时，系统将在磁盘中分配一段以8K为单位的连续空间，当字段的值从内存写入磁盘时，就在这一既定空间随机保存，当一个 8K用完的时候，数据库指针会自动分配一个8K的空间。这里，每个8K空间被称为一个数据页（Page），又名页面或数据页面，并分配从0-7的页号, 每个文件的第0页记录引导信息，叫文件头（File header）；每8个数据页（64Ｋ）的组合形成扩展区（Extent），称为扩展。全部数据页的组合形成堆（Heap） 聚集索引和非聚集索引在SQL SERVER中，聚集索引的存储是以B树存储，B树的叶子直接存储聚集索引的数据 非聚集索引与聚集索引具有相同的 B 树结构，它们之间的显著差别在于以下两点： 基础表的数据行不按非聚集键的顺序排序和存储。 非聚集索引的叶层是由索引页而不是由数据页组成。 非聚集索引也是一个B树结构，与聚集索引不同的是，B树的叶子节点存的是指向堆或聚集索引的指针。 索引的设计原则]]></content>
      <tags>
        <tag>SQL Server</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SQL Server表分区]]></title>
    <url>%2F2017%2F06%2F02%2FSQL-Server%E8%A1%A8%E5%88%86%E5%8C%BA%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[T-SQL查询语句执行顺序]]></title>
    <url>%2F2017%2F06%2F02%2FT-SQL%E6%9F%A5%E8%AF%A2%E8%AF%AD%E5%8F%A5%E6%89%A7%E8%A1%8C%E9%A1%BA%E5%BA%8F%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[数据仓库增量数据处理的几种方法]]></title>
    <url>%2F2017%2F06%2F02%2F%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E5%A2%9E%E9%87%8F%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E7%9A%84%E5%87%A0%E7%A7%8D%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[看懂SQL Server执行计划]]></title>
    <url>%2F2017%2F06%2F02%2F%E7%9C%8B%E6%87%82SQL-Server%E6%89%A7%E8%A1%8C%E8%AE%A1%E5%88%92%2F</url>
    <content type="text"><![CDATA[当我们写的SQL语句传到SQL Server的时候，查询分析器会将语句依次进行解析（Parse）、绑定（Bind）、查询优化（Optimization，有时候也被称为简化）、执行（Execution）。除去执行步骤外，前三个步骤之后就生成了执行计划，也就是SQL Server按照该计划获取物理数据方式，最后执行步骤按照执行计划执行查询从而获得结果。 执行计划 查询优化器对输入的 T-SQL 查询语句通过”计算”而选择出效率最高的一种执行方案，这个执行方案就是执行计划。 执行计划可以告诉你这个查询将会被如何执行或者已经被如何执行过，可以通过执行计划看到 SQL 代码中那些效率比较低的地方。 查看执行计划的方式我们可以通过图形化的界面，或者文本，或者XML格式查看，这样会比较方便理解执行计划要表达出来的意思。 当一个查询被提交到 SQL Server 后，服务器端很多进程实际上要做很多事情来确保数据出入的完整性。 对于 T-SQL 来说, 处理的主要有两个阶段：关系引擎阶段( relational engine)和存储引擎阶段( storage engine)。 关系引擎主要要做的事情就是首先确保 Query 语句能够正确的解析，然后交给查询优化并产生执行计划，然后执行计划就以二进制格式发到存储引擎来更新或者读取数据。 存储引擎主要处理的比如像锁、索引的维护和事务等 所以对于执行计划，重点的是关注关系引擎。 估算执行计划和实际执行计划Estimated Execution Plans vs. Actual Execution Plans 它们之间的区别就是: 估算的执行计划 是从查询优化器来的，是输入关系引擎的，它的执行步骤包括一些运算符等等都是通过一系列的逻辑分析出来的，是一种通过逻辑推算出来的计划，只能代表查询优化器的观点；实际执行计划 是真实的执行了”估算执行计划”后的一种真实的结果，是实实在在真实的执行反馈, 是属于存储引擎。 以上描述了关于执行计划的概念，下面以实际案例去解读一些基本语句， 例如SELECT, UPDATE,INSERT, DELETE 等查询的执行计划。 ————————————我是分隔符———————————– 有大约78个执行计划中的操作符，可以去 MSDN Book Online 随时查 下表表示一下常见的执行计划元素 Select (Result) Sort Spool Clustered Index Scan Key Lookup Eager Spool NonClustered Index Scan Compute Scalar Stream Aggregate Clustered Index Seek Constant Scan Distribute Streams NonClustered Index Seek Table Scan Repartition Streams Hash Match RID Lookup Gather Streams Nested Loops Filter Bitmap Merge Join Lazy Spool Split 操作符分为阻断式 blocking 和非阻断式non-blocking Table Scan 表扫描 当表中没有聚集索引，又没有合适索引的情况下，会出现这个操作。这个操作是很耗性能的，他的出现也意味着优化器要遍历整张表去查找你所需要的数据 Clustered Index Scan / Index Scan 聚集索引扫描/非聚集索引扫描 这个图标两个操作都可以使用，一个聚集索引扫描，一个是非聚集索引扫描。 聚集索引扫描：聚集索引的数据体积实际是就是表本身，也就是说表有多少行多少列，聚集所有就有多少行多少列，那么聚集索引扫描就跟表扫描差不多，也要进行全表扫描，遍历所有表数据，查找出你想要的数据。 非聚集索引扫描：非聚集索引的体积是根据你的索引创建情况而定的，可以只包含你要查询的列。那么进行非聚集索引扫描，便是你非聚集中包含的列的所有行进行遍历，查找出你想要的数据。 Clustered Index Seek / Index Seek 聚集索引查找/非聚集索引查找 聚集索引查找和非聚集索引查找都是使用该图标。 聚集索引查找：聚集索引包含整个表的数据，也就是在聚集索引的数据上根据键值取数据。 非聚集索引查找：非聚集索引包含创建索引时所包含列的数据，在这些非聚集索引的数据上根据键值取数据。 Key Lookup 键值查找 首先需要说的是查找，查找与扫描在性能上完全不是一个级别的，扫描需要遍历整张表，而查找只需要通过键值直接提取数据，返回结果，性能要好。 当你查找的列没有完全被非聚集索引包含，就需要使用键值查找在聚集索引上查找非聚集索引不包含的列。 RID Lookup RID查找 跟键值查找类似，只不过RID查找，是需要查找的列没有完全被非聚集索引包含，而剩余的列所在的表又不存在聚集索引，不能键值查找，只能根据行表示Rid来查询数据。 Hash Match 这个图标有两种地方用到，一种是表关联，一种是数据聚合运算时。 Hashing：在数据库中根据每一行的数据内容，转换成唯一符号格式，存放到临时哈希表中，当需要原始数据时，可以给还原回来。类似加密解密技术，但是他能更有效的支持数据查询。 Hash Table：通过hashing处理，把数据以key/value的形式存储在表格中，在数据库中他被放在tempdb中。 表关联： 数据聚合运算 Nested Loops这个操作符号，把两个不同列的数据集汇总到一张表中。提示信息中的Output List中有两个数据集，下面的数据集（inner set）会一一扫描与上面的数据集（out set），直到扫描完为止，这个操作才算是完成。 Merge Join这种关联算法是对两个已经排过序的集合进行合并。如果两个聚合是无序的则将先给集合排序再进行一一合并，由于是排过序的集合，左右两个集合自上而下合并效率是相当快的。 Sort对数据集合进行排序，需要注意的是，有些数据集合在索引扫描后是自带排序的。 Filter根据出现在having之后的操作运算符，进行筛选 Computer Scalar在需要查询的列中需要自定义列，比如count(*) as cnt , select name+’’+age 等会出现此符号。 数据查询SQL Server有几种方式查找数据记录： [Table Scan] 表扫描（最慢）：对表记录逐行进行检查 [Clustered Index Scan] 聚集索引扫描（较慢）：按聚集索引对记录逐行进行检查 [Index Scan] 索引扫描（普通）：根据索引滤出部分数据在进行逐行检查 [Index Seek] 索引查找（较快）：根据索引定位记录所在位置再取出记录 [Clustered Index Seek] 聚集索引查找（最快）：直接根据聚集索引获取记录 根据执行计划细节要做的优化操作 如果select * 通常情况下聚集索引会比非聚集索引更优。 如果出现Nested Loops，需要查下是否需要聚集索引，非聚集索引是否可以包含所有需要的列。 Hash Match连接操作更适合于需要做Hashing算法集合很小的连接。 Merge Join时需要检查下原有的集合是否已经有排序，如果没有排序，使用索引能否解决。 出现表扫描，聚集索引扫描，非聚集索引扫描时，考虑语句是否可以加where限制，select * 是否可以去除不必要的列。 出现Rid查找时，是否可以加索引优化解决。 在计划中看到不是你想要的索引时，看能否在语句中强制使用你想用的索引解决问题，强制使用索引的办法Select CluName1,CluName2 from Table with(index=IndexName)。 看到不是你想要的连接算法时，尝试强制使用你想要的算法解决问题。强制使用连接算法的语句：select * from t1 left join t2 on t1.id=t2.id option(Hash/Loop/Merge Join) 看到不是你想要的聚合算法是，尝试强制使用你想要的聚合算法。强制使用聚合算法的语句示例：select age ,count(age) as cnt from t1 group by age option(order/hash group) 看到不是你想要的解析执行顺序是，或这解析顺序耗时过大时，尝试强制使用你定的执行顺序。option（force order） 看到有多个线程来合并执行你的sql语句而影响到性能时，尝试强制是不并行操作。option（maxdop 1） 在存储过程中，由于参数不同导致执行计划不同，也影响啦性能时尝试指定参数来优化。option（optiomize for（@name=’zlh’）） 不操作多余的列，多余的行，不做务必要的聚合，排序。]]></content>
      <tags>
        <tag>SQL Server</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SQL Server性能优化相关]]></title>
    <url>%2F2017%2F06%2F02%2FSQL-Server%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E7%9B%B8%E5%85%B3%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[Markdown Cheatsheet]]></title>
    <url>%2F2017%2F05%2F31%2FMarkdown-Cheatsheet%2F</url>
    <content type="text"><![CDATA[开始写博客后，Markdown成为一种常用的文档格式，Markdown 的语法全由一些符号所组成，这些符号经过精挑细选，其作用一目了然，里面的语法记下来，为了以后查询方便。 文本样式1234*斜体*或_斜体_**粗体*****加粗斜体***~~删除线~~ 斜体 或 斜体 粗体 加粗斜体 删除线 This text will be italic This will also be italic This text will be bold This will also be bold You can combine them 引用使用大于号 &gt; 表示引用内容： As Grace Hopper said: I’ve always been more interestedin the future than in the past. 分级标题第一种写法 1234这是一个一级标题============================这是一个二级标题-------------------------------------------------- 第二种写法 123456# 一级标题## 二级标题### 三级标题#### 四级标题##### 五级标题###### 六级标题 超链接Markdown 支持两种形式的链接语法： 行内式和参考式两种形式，行内式一般使用较多。 行内式语法说明： []里写链接文字，()里写链接地址, ()中的”“中可以为链接指定title属性，title属性可加可不加。title属性的效果是鼠标悬停在链接上会出现指定的 title文字。链接文字’这样的形式。链接地址与链接标题前有一个空格。 12欢迎来到[我的博客](https://lvraikkonen.github.io/)欢迎来到[我的博客](https://lvraikkonen.github.io/ &quot;博客名&quot;) 欢迎来到我的博客 欢迎来到我的博客 参考式参考式超链接一般用在学术论文上面，或者另一种情况，如果某一个链接在文章中多处使用，那么使用引用 的方式创建链接将非常好，它可以让你对链接进行统一的管理。 语法说明：参考式链接分为两部分，文中的写法 [链接文字][链接标记]，在文本的任意位置添加[链接标记]:链接地址 “链接标题”，链接地址与链接标题前有一个空格。 如果链接文字本身可以做为链接标记，你也可以写成[链接文字][][链接文字]：链接地址的形式，见代码的最后一行。 123456我经常去的几个网站[Google][1]、[Leanote][2]以及[自己的博客][3][Leanote 笔记][2]是一个不错的[网站][]。[1]:http://www.google.com &quot;Google&quot;[2]:http://www.leanote.com &quot;Leanote&quot;[3]:https://lvraikkonen.github.io &quot;lvraikkonen&quot;[网站]:https://lvraikkonen.github.io 我经常去的几个网站Google、Leanote 以及自己的博客，Leanote 笔记是一个不错的网站。 自动链接语法说明：Markdown 支持以比较简短的自动链接形式来处理网址和电子邮件信箱，只要是用&lt;&gt;包起来， Markdown 就会自动把它转成链接。一般网址的链接文字就和链接地址一样，例如： 12&lt;http://example.com/&gt;&lt;address@example.com&gt; 显示效果： http://example.com/ &#97;&#100;&#100;&#114;&#101;&#115;&#x73;&#x40;&#x65;&#120;&#x61;&#x6d;&#x70;&#x6c;&#101;&#46;&#x63;&#111;&#109; 列表无序列表使用星号 *、加号 +或是减号 - 作为列表标记 Red Green Blue 有序列表则使用数字接着一个英文句点： Bird McHale Parish 列表项目可以包含多个段落，每个项目下的段落都必须缩进 4 个空格或是 1 个制表符： This is a list item with two paragraphs. Lorem ipsum dolorsit amet, consectetuer adipiscing elit. Aliquam hendreritmi posuere lectus. Vestibulum enim wisi, viverra nec, fringilla in, laoreetvitae, risus. Donec sit amet nisl. Aliquam semper ipsumsit amet velit. Suspendisse id sem consectetuer libero luctus adipiscing. 如果要在列表项目内放进引用，那 &gt; 就需要缩进： A list item with a blockquote: This is a blockquoteinside a list item. 图像和链接很相似的语法来标记图片，同样也允许两种样式： 行内式和参考式 行内式 一个惊叹号 ! 接着一个方括号，里面放上图片的替代文字 接着一个普通括号，里面放上图片的网址，最后还可以用引号包住并加上 选择性的 ‘title’ 文字 123![Alt text](/path/to/img.jpg)![Alt text](/path/to/img.jpg &quot;Optional title&quot;) Inline-style: 参考式id是图片参考的名称，图片参考的定义方式则和链接参考一样：12![Alt text][id][id]: url/to/image &quot;Optional title attribute&quot; Reference-style: 代码如果要标记一小段行内代码，你可以用反引号把它包起来 code 也可以使用三个反引号包裹一段代码，并指定一种语言 12var s = "JavaScript syntax highlighting";alert(s); 12s = "Python syntax highlighting"print s 12No language indicated, so no syntax highlighting.But let&apos;s throw in a &lt;b&gt;tag&lt;/b&gt;. 表格 Tables Are Cool col 3 is right-aligned $1600 col 2 is centered $12 zebra stripes are neat $1 以下是Github支持的markdown语法： To-Do listTASK LISTS [x] this is a complete item [ ] this is an incomplete item [x] @mentions, #refs, links, formatting, and tags supported [x] list syntax required (any unordered or ordered list supported) EMOJIGitHub supports emoji!:+1: :sparkles: :camel: :tada::rocket: :metal: :octocat:]]></content>
      <tags>
        <tag>Markdown</tag>
        <tag>备忘</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计短链接TinyURL]]></title>
    <url>%2F2017%2F05%2F27%2F%E8%AE%BE%E8%AE%A1%E7%9F%AD%E9%93%BE%E6%8E%A5TinyURL%2F</url>
    <content type="text"></content>
      <tags>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python Collection集合模块]]></title>
    <url>%2F2017%2F05%2F27%2FPython-Collection%E9%9B%86%E5%90%88%E6%A8%A1%E5%9D%97%2F</url>
    <content type="text"><![CDATA[1import collections collections是Python内建的一个集合模块，提供了许多有用的集合类。Python拥有一些内置的数据类型，比如str, int, list, tuple, dict等， collections模块在这些内置数据类型的基础上，提供了几个额外的数据类型： namedtuple(): 生成可以使用名字来访问元素内容的tuple子类 deque: 双端队列，可以快速的从另外一侧追加和推出对象 Counter: 计数器，主要用来计数 OrderedDict: 有序字典 defaultdict: 带有默认值的字典 namedtuplenamedtuple主要用来产生可以使用名称来访问元素的数据对象，通常用来增强代码的可读性。 namedtuple是一个函数，它用来创建一个自定义的tuple对象，并且规定了tuple元素的个数，并可以用属性而不是索引来引用tuple的某个元素。 1234567891011from collections import namedtuple# namedtuple('名称', [属性list])Point = namedtuple('Point', ['x', 'y'])p = Point(1, 2)print p.x, p.y# 1, 2isinstance(p, Point)# Trueisinstance(p, tuple)# True 用namedtuple可以很方便地定义一种数据类型，它具备tuple的不变性，又可以根据属性来引用 dequedeque其实是double-ended queue的缩写，翻译过来就是双端队列，它最大的好处就是实现了从队列头部快速增加和取出对象: .popleft(), .appendleft() list对象的这两种用法的时间复杂度是 O(n) ，也就是说随着元素数量的增加耗时呈 线性上升。而使用deque对象则是 O(1) 的复杂度 123456&gt;&gt;&gt; from collections import deque&gt;&gt;&gt; q = deque(['a', 'b', 'c'])&gt;&gt;&gt; q.append('x')&gt;&gt;&gt; q.appendleft('y')&gt;&gt;&gt; qdeque(['y', 'a', 'b', 'c', 'x']) deque提供了很多方法，例如 rotate 123456789101112131415161718192021# -*- coding: utf-8 -*-"""下面这个是一个有趣的例子，主要使用了deque的rotate方法来实现了一个无限循环的加载动画"""import sysimport timefrom collections import dequefancy_loading = deque('&gt;--------------------')while True: print '\r%s' % ''.join(fancy_loading), fancy_loading.rotate(1) sys.stdout.flush() time.sleep(0.08)# Result:# 一个无尽循环的跑马灯# -------------&gt;------- CounterCounter是一个简单的计数器，例如，统计字符出现的个数 12345678910&gt;&gt;&gt; from collections import Counter&gt;&gt;&gt; c = Counter()&gt;&gt;&gt; for ch in 'programming':... c[ch] = c[ch] + 1...&gt;&gt;&gt; cCounter(&#123;'g': 2, 'm': 2, 'r': 2, 'a': 1, 'i': 1, 'o': 1, 'n': 1, 'p': 1&#125;)&gt;&gt;&gt; # 获取出现频率最高的5个字符&gt;&gt;&gt; print c.most_common(5) Counter实际上也是dict的一个子类，上面的结果可以看出，字符’g’、’m’、’r’各出现了两次，其他字符各出现了一次。 defaultdict使用dict时，如果引用的Key不存在，就会抛出KeyError。如果希望key不存在时，返回一个默认值，就可以用defaultdict : 如果使用defaultdict，只要你传入一个默认的工厂方法，那么请求一个不存在的key时， 便会调用这个工厂方法使用其结果来作为这个key的默认值 1234567&gt;&gt;&gt; from collections import defaultdict&gt;&gt;&gt; dd = defaultdict(lambda: 'N/A')&gt;&gt;&gt; dd['key1'] = 'abc'&gt;&gt;&gt; dd['key1'] # key1存在'abc'&gt;&gt;&gt; dd['key2'] # key2不存在，返回默认值'N/A' OrderedDict使用dict时，Key是无序的。在对dict做迭代时，我们无法确定Key的顺序。 如果要保持Key的顺序，可以用OrderedDict 1234567&gt;&gt;&gt; from collections import OrderedDict&gt;&gt;&gt; d = dict([('a', 1), ('b', 2), ('c', 3)])&gt;&gt;&gt; d # dict的Key是无序的&#123;'a': 1, 'c': 3, 'b': 2&#125;&gt;&gt;&gt; od = OrderedDict([('a', 1), ('b', 2), ('c', 3)])&gt;&gt;&gt; od # OrderedDict的Key是有序的OrderedDict([('a', 1), ('b', 2), ('c', 3)]) Note: OrderedDict的Key会按照插入的顺序排列，不是Key本身排序]]></content>
      <tags>
        <tag>Python</tag>
        <tag>Data Structure</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SQL Server数据库分页]]></title>
    <url>%2F2017%2F05%2F27%2FSQL-Server%E6%95%B0%E6%8D%AE%E5%BA%93%E5%88%86%E9%A1%B5%2F</url>
    <content type="text"><![CDATA[在编写Web应用程序等系统时，会涉及到与数据库的交互，如果数据库中数据量很大的话，一次检索所有的记录，会占用系统很大的资源，因此常常采用分页语句：需要多少数据就只从数据库中取多少条记录。 常见的对大数据量查询的解决方案有以下两种： 将全部数据先查询到内存中，然后在内存中进行分页，这种方式对内存占用较大，必须限制一次查询的数据量。 采用存储过程在数据库中进行分页，这种方式对数据库的依赖较大，不同的数据库实现机制不通，并且查询效率不够理想。以上两种方式对用户来说都不够友好。 使用ROW_NUMBER()函数分页SQL Server 2005之后引入了 ROW_NUMBER() 函数，通过该函数根据定好的排序字段规则，产生记录序号 123SELECT ROW_NUMBER() OVER ( ORDER BY dbo.Products.ProductID DESC ) AS rownum , *FROM dbo.Products 12345678SELECT *FROM ( SELECT TOP ( @pageSize * @pageIndex ) ROW_NUMBER() OVER ( ORDER BY dbo.Products.UnitPrice DESC ) AS rownum , * FROM dbo.Products ) AS tempWHERE temp.rownum &gt; ( @pageSize * ( @pageIndex - 1 ) )ORDER BY temp.UnitPrice 使用OFFSET FETCH子句分页SQL Server 2012中引入了OFFSET-FETCH语句，可以通过使用OFFSET-FETCH过滤器来实现分页 12345SELECT * FROM dbo.Products ORDER BY UnitPrice DESC OFFSET ( @pageSize * ( @pageIndex - 1 )) ROWS FETCH NEXT @pageSize ROWS ONLY;]]></content>
      <tags>
        <tag>Database</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[二叉树的遍历]]></title>
    <url>%2F2017%2F05%2F27%2F%E4%BA%8C%E5%8F%89%E6%A0%91%E7%9A%84%E9%81%8D%E5%8E%86%2F</url>
    <content type="text"><![CDATA[二叉树的遍历分为： 深度优先搜索(Depth First Search) 是沿着树的深度遍历树的节点，尽可能深的搜索树的分支。深度优先搜索二叉树是先访问根结点，然后遍历左子树接着是遍历右子树，因此我们可以利用堆栈的先进后出的特点，先将右子树压栈，再将左子树压栈，这样左子树就位于栈顶，可以保证结点的左子树先与右子树被遍历。 广度优先搜索(Breadth First Search) 是从根结点开始沿着树的宽度搜索遍历，可以利用队列实现广度优先搜索 二叉树的深度优先遍历的非递归的通用做法是采用栈，广度优先遍历的非递归的通用做法是采用队列 深度优先实现深度优先遍历又分为：前序、中序、后序遍历 前序遍历：根节点-&gt;左子树-&gt;右子树 中序遍历：左子树-&gt;根节点-&gt;右子树 后序遍历：左子树-&gt;右子树-&gt;根节点 note: 二叉搜索树BST的中序遍历，返回的结果是按顺序排列的 递归实现前序遍历伪代码： 123456preorder(node) if (node = null) return visit(node) preorder(node.left) preorder(node.right) 根节点-&gt;左子树-&gt;右子树 python实现 123456def preorder(self, node): """前序遍历""" if node: print node.data self.preorder(node.left) self.preorder(node.right) 中序遍历伪代码： 123456inorder(node) if (node = null) return inorder(node.left) visit(node) inorder(node.right) 左子树-&gt;根节点-&gt;右子树 123456def inorder(self, node): """中序遍历""" if node: self.inorder(node.left) print node.data self.inorder(node.right) 后序遍历伪代码： 123456postorder(node) if (node = null) return postorder(node.left) postorder(node.right) visit(node) 左子树-&gt;右子树-&gt;根节点 123456def postorder(self, node): """后序遍历""" if node: self.postorder(node.left) self.postorder(node.right) print node.data 非递归实现因为当遍历过根节点之后还要回来，所以必须将其存起来。考虑到后进先出的特点，选用栈存储。 前序遍历伪代码： 123456789iterativePreorder(node) parentStack = empty stack while (not parentStack.isEmpty() or node ≠ null) if (node ≠ null) visit(node) if (node.right ≠ null) parentStack.push(node.right) node = node.left else node = parentStack.pop() 123456789101112131415def preorderTraversal__iterative(root): """ :type root: TreeNode """ node = root stack = [] while node or stack: if node: print node.val if node.right: stack.append(node.right) node = node.left else: node = stack.pop() return 中序遍历伪代码： 12345678910iterativeInorder(node) s ← empty stack while (not s.isEmpty() or node ≠ null) while (node ≠ null) s.push(node) node ← node.left else node ← s.pop() visit(node) node ← node.right 1234567891011121314def inorderTraversal_iterative(root): """ :type root: TreeNode """ node = root stack = [] while node or stack: while node: stack.append(node) node = node.left node = stack.pop() print node.val node = node.right return result 后序遍历 后序遍历伪代码： 12345678910111213141516iterativePostorder(node) s ← empty stack lastNodeVisited ← null while (not s.isEmpty() or node ≠ null) if (node ≠ null) s.push(node) node ← node.left else peekNode ← s.peek() // if right child exists and traversing node // from left child, then move right if (peekNode.right ≠ null and lastNodeVisited ≠ peekNode.right) node ← peekNode.right else visit(peekNode) lastNodeVisited ← s.pop() 123456789101112131415161718def postorderTraversal(node): if node is None: return [] stack = [] result = [] lastNodeVisited = None while stack or node: if node: stack.append(node) node = node.left else: peekNode = stack[-1] if peekNode.right and lastNodeVisited != peekNode.right: node = peekNode.right else: result.append(peekNode) lastVisitedNode = stack.pop() return result 广度优先实现伪代码 12345678910levelorder(root) q ← empty queue q.enqueue(root) while (not q.isEmpty()) node ← q.dequeue() visit(node) if (node.left ≠ null) q.enqueue(node.left) if (node.right ≠ null) q.enqueue(node.right)]]></content>
      <tags>
        <tag>Data Structure</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python内置数据结构]]></title>
    <url>%2F2017%2F05%2F27%2FPython%E5%86%85%E7%BD%AE%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F</url>
    <content type="text"><![CDATA[Python 内置数据类型包括 list, tuple, dict, set ListPython内置的一种数据类型是列表：list。list是一种有序的集合，可以随时添加和删除其中的元素 列表常用操作及其复杂度 Operation big O description index [] O(1) 索引操作 index assignment O(1) 索引赋值操作 append O(1) 在列表末尾添加新的对象 pop() O(1) 移除列表中的一个元素（默认最后一个元素），并且返回该元素的值 pop(i) O(n) 移除列表中索引位置的值，并且返回该元素的值 insert(i,item) O(n) 将对象插入列表索引i位置 del operator O(n) 删除列表的的元素 iteration O(n) contains (in) O(n) get slice [x:y] O(k) del slice O(n) set slice O(n+k) reverse O(n) 反向列表中元素 remove O(n) 移除列表中某个值的第一个匹配项 concatenate O(k) sort O(n log n) 列表排序 multiply O(nk) DictionaryPython内置了字典：dict的支持，dict全称dictionary，在其他语言中也称为map，使用键-值（key-value）存储，具有极快的查找速度。dict内部存放的顺序和key放入的顺序是没有关系的。dict的key必须是不可变对象。这是因为dict根据key来计算value的存储位置，如果每次计算相同的key得出的结果不同，那dict内部就完全混乱了。这个通过key计算位置的算法称为哈希算法（Hash） dict: {&#39;A&#39;: 1, &#39;Z&#39;: -1} 创建方式 1234567a = dict(A=1, Z=-1)b = &#123;'A': 1, 'Z': -1&#125;c = dict(zip(['A', 'Z'], [1, -1]))d = dict([('A', 1), ('Z', -1)])e = dict(&#123;'Z': -1, 'A': 1&#125;)a == b == c == d == e# True 和list比较，dict有以下几个特点： 查找和插入的速度极快，不会随着key的增加而变慢； 需要占用大量的内存，内存浪费多。 而list相反： 查找和插入的时间随着元素的增加而增加； 占用空间小，浪费内存很少。 所以，dict是用空间来换取时间的一种方法 遍历一个dict，实际上是在遍历它的所有的Key的集合，然后用这个Key来获得对应的Value 12345678910111213d = &#123;'Adam': 95, 'Lisa': 85, 'Bart': 59, 'Paul': 75&#125;print d['Adam']# 95print d.get('Jason')# Nonefor key in d : print key, ':', d.get(key)# Lisa : 85# Paul : 75# Adam : 95# Bart : 59 Tupletuple和list非常类似，但是tuple一旦初始化就不能修改 Tuple 的不可变性元组一旦创建，它的元素就是不可变的， 例如如下： 1234t = ('a', 'b', ['A', 'B'])t[2][0] = 'X't[2][1] = 'Y'print t 当我们把list的元素’A’和’B’修改为’X’和’Y’后，tuple变为： Tuple的每个元素，指向永远不变，其中如果某个元素本身是可变的，那么元素内部也是可变的，但是元组的指向却是没有变化 Setset和dict类似，也是一组key的集合，但不存储value。由于key不能重复，所以，在set中，没有重复的key。 set可以看成数学意义上的无序和无重复元素的集合，因此，两个set可以做数学意义上的交集、并集等操作： 1234567s1 = set([1, 2, 3])s2 = set([2, 3, 4])s1 &amp; s2# set([2, 3])s1 | s2# set([1, 2, 3, 4]) set和dict的唯一区别仅在于没有存储对应的value，但是，set的原理和dict一样，所以，同样不可以放入可变对象，因为无法判断两个可变对象是否相等，也就无法保证set内部“不会有重复元素”。试试把list放入set，看看是否会报错。]]></content>
      <tags>
        <tag>Python</tag>
        <tag>Data Structure</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git 常用命令总结]]></title>
    <url>%2F2016%2F09%2F20%2Fgit%E4%BD%BF%E7%94%A8%E5%B0%8F%E7%BB%93%2F</url>
    <content type="text"><![CDATA[Git，目前主流的版本控制工具，git命令是一些命令行工具的集合，它可以用来跟踪，记录文件的变动。比如你可以进行保存，比对，分析，合并等等。 基本的git工作流 在工作目录中修改文件 暂存文件，将文件的快照放入暂存区域 提交更新，找到暂存区域的文件，将快照永久性存储到 Git 仓库目录 分支当你在做一个新功能的时候，最好是在一个独立的区域上开发，通常称之为分支。分支之间相互独立，并且拥有自己的历史记录。这样做的原因是： 稳定版本的代码不会被破坏 不同的功能可以由不同开发者同时开发 开发者可以专注于自己的分支，不用担心被其他人破坏了环境 在不确定之前，同一个特性可以拥有几个版本，便于比较 以上，简单介绍了一下git的基本概念，下面记录一下如下几个git命令以及使用： git clone git remote git commit git push git fetch git pull git clone从远程主机克隆一个版本库，git支持SSH, Git, HTTPS等 1git clone &lt;远程仓库&gt; &lt;本地文件夹名&gt; git remotegit remote命令用于管理主机名 1git remote git commit将索引内容添加到仓库中 1git commit -m "提交的描述信息" git pushgit push命令用于将本地分支的更新，推送到远程主机 1git push &lt;远程主机名&gt; &lt;本地分支名&gt;:&lt;远程分支名&gt; git fetchgit fetch命令从远程主机的更新取回本地 1git fetch &lt;远程主机名&gt; &lt;分支名&gt; git pullgit pull命令的作用是，取回远程主机某个分支的更新，再与本地的指定分支合并 1git pull &lt;远程主机名&gt; &lt;远程分支名&gt;:&lt;本地分支名&gt; 版权声明： 除非注明，本博文章均为原创，转载请以链接形式标明本文地址。]]></content>
      <categories>
        <category>备忘</category>
      </categories>
      <tags>
        <tag>git</tag>
        <tag>github</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Scrapy编写爬虫]]></title>
    <url>%2F2016%2F09%2F18%2F%E4%BD%BF%E7%94%A8Scrapy%E7%BC%96%E5%86%99%E7%88%AC%E8%99%AB%2F</url>
    <content type="text"><![CDATA[Scrapy是Python开发的一个快速,高层次的屏幕抓取和web抓取框架，用于抓取web站点并从页面中提取结构化的数据。Scrapy用途广泛，可以用于数据挖掘、监测和自动化测试。 Scrapy Home Site 1pip install scrapy Scrapy 处理流程图借个图简单介绍下Scrapy处理的流程(这就是框架，帮我们完成了大部分工作) 引擎(Scrapy Engine)，用来处理整个系统的数据流处理，触发事务。 调度器(Scheduler)，用来接受引擎发过来的请求，压入队列中，并在引擎再次请求的时候返回。 下载器(Downloader)，用于下载网页内容，并将网页内容返回给蜘蛛。 蜘蛛(Spiders)，蜘蛛是主要干活的，用它来制订特定域名或网页的解析规则。编写用于分析response并提取item(即获取到的item)或额外跟进的URL的类。 每个spider负责处理一个特定(或一些)网站。 项目管道(Item Pipeline)，负责处理有蜘蛛从网页中抽取的项目，他的主要任务是清晰、验证和存储数据。当页面被蜘蛛解析后，将被发送到项目管道，并经过几个特定的次序处理数据。 下载器中间件(Downloader Middlewares)，位于Scrapy引擎和下载器之间的钩子框架，主要是处理Scrapy引擎与下载器之间的请求及响应。 蜘蛛中间件(Spider Middlewares)，介于Scrapy引擎和蜘蛛之间的钩子框架，主要工作是处理蜘蛛的响应输入和请求输出。 调度中间件(Scheduler Middlewares)，介于Scrapy引擎和调度之间的中间件，从Scrapy引擎发送到调度的请求和响应。 接下来简单介绍一下爬虫的写法，从开发爬虫到部署，把爬取的数据写入mongodb: 创建新的Scrapy工程 定义要抽取的Item对象 编写一个Spider来爬取某个网站并提取出Item对象 编写Item Pipeline来存储提取出来的Item对象 使用Scrapyd-client部署Spider项目 把Aqicn上的北京空气质量的数据爬取下来，为日后分析做准备 创建Scrapy工程在目录下执行命令 1scrapy startproject projectName 创建如下的项目文件夹，目录结构如下 定义Item对象创建一个scrapy.Item类，将所要爬取的字段定义好 12345678910111213141516171819202122import scrapyclass AirqualityItem(scrapy.Item): date = scrapy.Field() hour = scrapy.Field() city = scrapy.Field() # area = scrapy.Field() aqivalue = scrapy.Field() aqilevel = scrapy.Field() pm2_5 = scrapy.Field() pm10 = scrapy.Field() co = scrapy.Field() no2 = scrapy.Field() o3 = scrapy.Field() so2 = scrapy.Field() temp = scrapy.Field() dew = scrapy.Field() pressure = scrapy.Field() humidity = scrapy.Field() wind = scrapy.Field() # add field to log spider crawl time crawl_time = scrapy.Field() 编写SpiderSpider类里面定义如何从一个domain组中爬取数据，包括：初始化url列表、如何跟踪url和如何解析页面提取Item，定义一个Spider，需要继承scrapy.Spider类 name: 定义Spider的名称，以后调用爬虫应用时候使用; start_url: 初始化url; parse(): 解析下载后的Response对象，解析并返回页面数据并提取出相应的Item对象 抽取Item对象内容Scrapy Selector是Scrapy提供的一套选择器，通过特定的XPath或者CSS表达式来选择HTML文件中某个部分 (note: Chrome浏览器自带的copy XPath或者CSS功能非常好用)，在开发过程中，可以使用Scrapy内置的Scrapy-Shell来debug选择器。 爬虫的代码如下 123456789101112131415161718192021222324252627282930313233class AirQualitySpider(CrawlSpider): name = "AqiSpider" download_delay = 2 allowed_domains = ['aqicn.org'] start_urls = ['http://aqicn.org/city/beijing/en/'] def parse(self, response): sel = Selector(response) pm25 = int(sel.xpath('//*[@id="cur_pm25"]/text()').extract()[0]) pm10 = int(sel.xpath('//*[@id="cur_pm10"]/text()').extract()[0]) o3 = int(sel.xpath('//*[@id="cur_o3"]/text()').extract()[0]) no2 = int(sel.xpath('//*[@id="cur_no2"]/text()').extract()[0]) so2 = int(sel.xpath('//*[@id="cur_so2"]/text()').extract()[0]) co = int(sel.xpath('//*[@id="cur_co"]/text()').extract()[0]) item = AirqualityItem() item['date'] = updatetime.strftime("%Y%m%d") item['hour'] = updatetime.hour # strftime("%H%M%S") item['city'] = city item['aqivalue'] = aqivalue item['aqilevel'] = aqilevel item['pm2_5'] = pm25 item['pm10'] = pm10 item['co'] = co item['no2'] = no2 item['o3'] = o3 item['so2'] = so2 item['temp'] = temp item['dew'] = dew item['pressure'] = pressure item['humidity'] = humidity item['wind'] = wind item['crawl_time'] = cur_time yield item 数据存储到MongoDB在Item已经被爬虫抓取之后，Item被发送到Item Pipeline去做更复杂的处理，比如存储到文件中或者数据库中。Item Pipeline常见的用途如下 清洗抓取来的HTML数据 验证抓取来的数据 查询与去除重复数据 将Item存储到数据库中 12345678910111213141516class AirqualityPipeline(object): def __init__(self): connection = pymongo.MongoClient(settings['MONGODB_SERVER'], settings['MONGODB_PORT']) db = connection[settings['MONGODB_DB']] self.collection = db[settings['MONGODB_COLLECTION']] def process_item(self, item, spider): # save data into mongodb valid = True if not item: valid = False raise DropItem("Missing &#123;0&#125;".format(item)) if valid: self.collection.insert(dict(item)) log.msg("an aqi data added to MongoDB database!", level=log.DEBUG, spider=spider) return item 接下来需要在setting.py文件中配置Item Pipeline与数据库信息 12345678ITEM_PIPELINES = &#123; 'AirQuality.pipelines.AirqualityPipeline': 300,&#125;MONGODB_SERVER = "localhost"MONGODB_PORT = 27017MONGODB_DB = "aqihistoricaldata"MONGODB_COLLECTION = "aqidata" 到此，简单的爬虫就已经写好了，可以使用以下命令来抓取相关页面来测试一下这个爬虫 1scrapy crawl AqiSpider 其中，AqiSpider就是在Spider程序中设置的Spider的name属性 防止爬虫被禁的几种方法很多网站都有反爬虫的机制，对于这些网站，可以采用以下的一些办法来绕开反爬虫机制： 使用User Agent池，每次发送请求的时候从池中选取不一样的浏览器头信息 禁止Cookie，有些网站会根据Cookie识别用户身份 设置dowload_delay，频繁请求数据肯定会被禁 使用Scrapyd和Scrapyd-client部署爬虫scrapyd是一个用于部署和运行scrapy爬虫的程序，它允许你通过JSON API来部署爬虫项目和控制爬虫运行。crapyd是一个守护进程，监听爬虫的运行和请求，然后启动进程来执行它们。 安装12pip install scrapydpip install scrapyd-client 启动服务1scrapyd 配置服务器信息编辑scrapy.cfg文件，添加如下内容 123[deploy:MySpider]url = http://localhost:6800/project = AirQuality 其中，MySpider是服务器名称， url是服务器地址 检查配置，列出当前可用的服务器 1scrapyd-deploy -l 部署Spider项目 1scrapyd-deploy MySpider -p AirQuality 部署完成后，在http://localhost:6800 可以看到如下的爬虫部署的监控信息 可以使用curl命令去调用爬虫，也可以使用contab命令来定时的去调用爬虫，来实现定时爬取的任务。 版权声明： 除非注明，本博文章均为原创，转载请以链接形式标明本文地址。]]></content>
      <categories>
        <category>Python</category>
        <category>框架</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何拍摄星空]]></title>
    <url>%2F2016%2F09%2F07%2F%E5%A6%82%E4%BD%95%E6%8B%8D%E6%91%84%E6%98%9F%E7%A9%BA%2F</url>
    <content type="text"><![CDATA[好久没写博客了，重新拾起来吧。今天说点技术无关的话题——摄影 我是一个纯业余的风光狗，向往美丽的大自然，喜欢仰望天空，把深邃的星空拍摄下来，以免子孙后代在严重的光污染中忘记了这篇美丽星空的存在。下面简单介绍一下如何拍星空： 来自知乎我的回答我曾经在西藏拍摄过星空，对于拍摄有一定的体会和经验，我说说拍摄方法吧： 首先，光污染问题。国内很多的地方，尤其是大城市，肉眼几乎看不到几颗星星，更不用说银河了。所以要拍银河星空的话，必须要到完全没有光污染的旷野，当然半夜在荒郊野岭拍照对于心理是个很大的挑战。 关于拍摄器材。要有个大光圈的广角镜头，最好用单反或者SONY的高级微单来拍，要的是机身的优秀高感也就是ISO能力；光圈的话，当然越大越好，因为外界环境很黑暗，所以需要长时间曝光，进光量一定要够；角度越广角越好，广角更能拍出宽阔感。推荐佳能的14L镜头，这个绝对是星空专用镜头。（一定要用三脚架，越稳定越好） 关于拍摄参数。M档，我的常用参数是，光圈f2.8，快门15-20秒，感光度ISO1600-6400关于对焦。我来纠正一下很多人误传的一条知识：手动对焦值无穷远再往回拧一点点。这是绝对的错误，这个一点点到底是多少？没人能说清。所以正确的对焦方法是：打开相机的实时取景功能LV，放大到10倍，然后看星点是否对上焦（也就是实心点不发虚），然后调整曝光参数，试拍两张，再调整感光度，达到满意为止。掌握这几点之后，你肯定能拍出肯漂亮的星空银河了，不过没有前景的话，照片就会很枯燥。所以找个漂亮的前景也很重要，因为使用大光圈，所以前景在构图时候不要离得太近，否则会虚掉。至于补光，我只能说多试几次。关于补光的话，不要一直开着手电筒对着前景照射，否则会强烈过曝，正确的方式是：在按下快门到曝光结束这段时间段的最后一秒，打开手电筒对着前景闪一下，这就足够了。 祝大家都能拍出漂亮的星空照附上在西藏拍的银河和泸沽湖拍的星空 西藏定日县城外小河沟拍的银河 云南泸沽湖边的星空 版权声明： 除非注明，本博文章均为原创，转载请以链接形式标明本文地址。]]></content>
      <tags>
        <tag>摄影</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[备份Hexo源文件至GitHub]]></title>
    <url>%2F2016%2F05%2F31%2F%E5%A4%87%E4%BB%BDHexo%E6%BA%90%E6%96%87%E4%BB%B6%E8%87%B3GitHub%2F</url>
    <content type="text"><![CDATA[本文转自：http://www.leyar.me/backup-your-blog-to-github/ Hexo是一款基于Node.js的静态博客框架，前一阵俺的笔记本泡水直接退役，但是博客的原文件还在那台死去的机器上，所以备份啊。。。本质上，Hexo是将本地的md文件编译成静态文件上传到github上（或者其他），所以建议是将本地的整个Hexo项目（blog）原件同步提交到github或者其他代码托管的站点。 下面记录一下备份、以及在另外的电脑上恢复博客的过程，为了以后备查。 前提已创建有 GitHub 仓库，并且已使用 hexo-deployer-git 部署到 master 分支。（发布博文并托管到Github上）如果不满足请自行 google hexo 部署到 GitHub 的操作方法。 备份过程在Github网站创建一个新仓库(或者使用Github托管博客的仓库，在该仓库下创建一个新的分支)，比如我新建的仓库名为 HEXOSource 在本地hexo根目录中， 初始化git仓库 1git init 创建并切换到名为 hexo_source 的分支 1git checkout -b hexo_source 创建忽略规则文件 .gitignore 1vi .gitignore 按需添加如下内容： 12345678.DS_StoreThumbs.dbdb.json *.log.deploy*/node_modules/.npmignorepublic/ 上面最后一行 public 目录，因其已被 hexo 插件同步到 master 分支里，因此不需要再同步，deploy 是 hexo 的 git 配置存放目录，也不需要同步。其他内容可选择忽略也可以选择同步。 添加内容到仓库并提交到远程仓库 1234git add .git commit -m "first commit"git remote add origin git@github.com:lvraikkonen/HEXOSource.git # 后面仓库目录改成自己新建的。git push -u origin hexo_source 按照以上的步骤就进行了 hexo 源文件的初次备份。以后每次修改了内容之后，都可通过以下几条命令实现同步。 123git add .git commit -m "..." # 双引号内填写更新内容git push origin hexo_source # 或者 git push 通过 git submodule 来同步第三方主题我们一般会选择第三方主题的仓库直接git clone下来。这是一个非常不好的习惯，正确做法是：Fork该第三方主题仓库，这样就会在自己账号下生成一个同名的仓库，并对应一个url，我们应该git clone自己账号下的url。 这样做的原因是：我们很有可能在原来主题基础上做一些自定义的小改动，为了保持多终端的同步，我们需要将这些改动提交到远程仓库。而第三方仓库我们是无法直接push的。 这样就会出现git仓库的嵌套问题，我们通过git submodule来解决这个问题。 1git submodule add git@github.com:lvraikkonen/hexo-theme-next.git themes/next 我们修改主题后: 12git commit -am "refine themes"git push origin hexo_source 然后就完成了第三方主题的备份 在其他电脑同步源文件时，需要执行如下命令来同步主题 12git submodule init // 这句很重要git submodule update 新机器同步在一个新机器上写博客，用以下步骤同步至最新状态 新建博客文件夹 hexo_blog 在该文件夹下初始化git仓库 1git init 为本地仓库添加远程仓库 1git remote add origin git@github.com:lvraikkonen/HEXOSource.git 切换至hexo_source分支 1git checkout -b hexo_source 获取hexo_source分支源文件 1git pull origin hexo_source 然后就是写博客，并将.md博客文件放至_posts文件夹，然后添加修改到本地仓库 123git add .git commit -m "写了一篇博客"git push origin hexo_source 至此，已经完成了博客的撰写并修改了远端仓库的博客源文件，然后使用hexo g和hexo d更新博客就OK啦！ 参考 关于博客同步的解决办法 使用Git Submodule管理子模块]]></content>
      <tags>
        <tag>备忘</tag>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用d3.js绘图]]></title>
    <url>%2F2015%2F11%2F28%2F%E4%BD%BF%E7%94%A8d3.js%E7%BB%98%E5%9B%BE%E6%B5%81%E6%B0%B4%E8%B4%A6%2F</url>
    <content type="text"><![CDATA[D3的全称是（Data-Driven Documents），是一个Javascript的函数库，主要用途是用HTML和SVG展现数据。下面简单回顾一下我从0出发把csv文件画在HTML页面上的过程。 0. 引入d3.js库引入js库可以直接引用网站上host的js库，也可以下载到本地folder下引入 1&lt;script src="http://d3js.org/d3.v3.min.js" charset="utf-8"&gt;&lt;/script&gt; 1. 创建SCG画布在 SVG 画布的预定义元素里，有六种基本图形： 矩形 圆形 椭圆 线段 折线 多边形 另外，还有一种比较特殊，也是功能最强的元素： 路径 画布中的所有图形，都是由以上七种元素组成。在绘制数据图表的时候，都是操作这几种图形元素。 123456789var margin = &#123;top: 20, right: 20, bottom: 30, left: 50&#125;, width = 960 - margin.left - margin.right, height = 500 - margin.top - margin.bottom;var svg = d3.select("body").append("svg") .attr("width", width + margin.left + margin.right) .attr("height", height + margin.top + margin.bottom) .append("g") .attr("transform", "translate(" + margin.left + "," + margin.top + ")"); 上面代码的意思是，选取HTML代码中的body元素，再后面添加svg画布元素，然后设置宽度高度等属性，svg的g元素类似于div，在这里作为一组元素的容器，后面加入的元素都放在g里面，g可以设置统一的css，里面的子元素会继承可继承css属性。margin和position对g的定位不起作用，只能使用translate通过位移来定位。 2. 定义比例尺对于画布或者图形的长度，不可能全部写死，需要通过数据的大小关系来动态确定，参考地图的比例尺。d3.js中，比例尺需要定义定义域和值域两个属性 有线性比例尺 d3.scale.linear() 和序数比例尺 d3.scale.ordinal() ，线性比例尺针对连续的定义域和值域，序数比例尺针对离散的。 12var xScale = d3.time.scale().range([0, width]);var yScale = d3.scale.linear().range([height, 0]); 这里先定义比例尺的值域，由于定义域需要根据数据来确定，所以写到了后面读取数据的部分。 3. 定义坐标轴d3.js中的坐标轴由 d3.svg.axis() 来实现，svg的坐标原点是左上角，向右为正，向下为正。 123456var xAxis = d3.svg.axis() .scale(xScale) .orient("bottom");var yAxis = d3.svg.axis() .scale(yScale) .orient("left"); x轴是日期，这里使用d3.time在时间和字符串之间做转换。y轴使用普通的线性缩放坐标轴。 4. 读取数据与绑定数据d3.js 中自带了读取csv、json等文件的方法。 123d3.json("data.json", function(error, json)&#123; // process data&#125;; d3.js 中是通过以下两个函数来绑定数据的： datum()：绑定一个数据到选择集上 data()：绑定一个数组到选择集上，数组的各项值分别与选择集的各元素绑定 12345678var data = json;// format date field to datedata.forEach(function(d)&#123; d.date = new Date(d.date); d.close = d.close;&#125;);xScale.domain(d3.extent(data, function(d)&#123; return d.date;&#125;));yScale.domain(d3.extent(data, function(d)&#123; return d.close;&#125;)); 在这里，data数据是一个列表对象，需要对列表中每一条数据的字段数据类型进行定义，之后需要做的是上面提到的定义x轴y轴的比例尺的定义域的定义。 5. 画线图形的主题是一条线，需要添加 path 元素，path的属性决定了线的路径，下面方法定义线的路径属性。 1234567var line = d3.svg.line() .x(function(d) &#123; return xScale(d.date); &#125;) .y(function(d) &#123; return yScale(d.close); &#125;);svg.append("path") .datum(data) .attr("class", "line") .attr("d", function(d)&#123; return line(d);&#125;); 6. 添加坐标轴call() 函数，其参数是前面定义的坐标轴 axis 1234567891011121314// add axissvg.append("g") .attr("class", "x axis") .attr("transform", "translate(0," + height + ")") .call(xAxis);svg.append("g") .attr("class", "y axis") .call(yAxis) .append("text") .attr("transform", "rotate(-90)") .attr("y", 6) .attr("dy", ".71em") .style("text-anchor", "end") .text("Price ($)"); 7. 生成图表上面的js脚本写好了之后，理论上就可以生成折线图了。不过在本地调试中，发现报错：文件没有找到。这个是因为由于安全考虑，浏览器不允许js脚本访问本地文件，解决方法有两个： 在本地开启一个web service 修改浏览器属性，允许访问本地文件 由于以后是要在网站上展示数据，所以我是用Flask在后台开启一个web服务，把d3所需要的数据生成出来 123456789101112131415161718from flask import Flaskfrom flask import render_templateimport jsonimport pandas as pdapp = Flask(__name__)data_path = './sampleData'@app.route("/")def index(): return render_template("line_chart.html")@app.route('/data')def get_data(): with open(data_path + '/line_chart.tsv') as data_file: sample_data = pd.read_csv(data_file, sep='\t') return sample_data.to_json(orient='records') 这样，在本地运行Flask，就可以展现出折线图了。这大约就是d3.js数据可视化的基本过程。 版权声明： 除非注明，本博文章均为原创，转载请以链接形式标明本文地址。]]></content>
      <tags>
        <tag>d3.js</tag>
        <tag>visualization</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Understanding TF-IDF]]></title>
    <url>%2F2015%2F11%2F27%2F2015-11-27-understanding-tf-idf%2F</url>
    <content type="text"></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>Scikit-Learn</tag>
        <tag>Text Mining</tag>
        <tag>Feature Extraction</tag>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Bag of Words Model]]></title>
    <url>%2F2015%2F11%2F26%2F2015-11-26-bag-of-words-model%2F</url>
    <content type="text"></content>
      <categories>
        <category>算法</category>
        <category>Kaggle</category>
      </categories>
      <tags>
        <tag>Scikit-Learn</tag>
        <tag>Kaggle</tag>
        <tag>Text Mining</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Cross-Validation in Scikit-Learn]]></title>
    <url>%2F2015%2F09%2F21%2F2015-09-21-cross-validation-in-scikit-learn%2F</url>
    <content type="text"></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>Python</tag>
        <tag>Scikit-Learn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[How to Evaluate Algorithm]]></title>
    <url>%2F2015%2F09%2F18%2F2015-09-18-how-to-evaluate-algorithm%2F</url>
    <content type="text"></content>
      <categories>
        <category>Machine Learning</category>
        <category>Scikit-Learn</category>
        <category>Python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[About Regularization]]></title>
    <url>%2F2015%2F09%2F18%2F2015-09-18-about-regularization%2F</url>
    <content type="text"><![CDATA[正则化防止过拟合]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>Algorithm</tag>
        <tag>Scikit-Learn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Logistic Regression原理以及应用]]></title>
    <url>%2F2015%2F08%2F19%2F2015-08-12-logistic-regression-classifier-on-hands%2F</url>
    <content type="text"><![CDATA[逻辑回归算法是一个很有用的分类算法，这篇文章总结一下逻辑回归算法的相关内容。数据使用scikit-learn自带的Iris数据集。 Iris datasetIris数据集，里面包含3种鸢尾花品种的4各属性，这个分类问题可以描述成使用鸢尾花的属性，来判断这个品种倒地属于哪个品种类别。为了简单，这里使用两个类别：Setosa和Versicolor，两个属性：Length和Width 12345678910111213from sklearn import datasetsimport matplotlib.pyplot as pltimport pandas as pdimport numpy as npdata = datasets.load_iris()X = data.data[:100, : 2]y = data.target[:100]setosa = plt.scatter(X[:50, 0], X[:50, 1], c='b')versicolor = plt.scatter(X[50:, 0], X[50:, 1], c='r')plt.xlabel("Sepal Length")plt.ylabel("Seqal Width")plt.legend((setosa, versicolor), ("Setosa", "Versicolor")) 可以看出来，两个品种可以被区分开，接下来要使用一种算法，让计算机把这两个类别区分开。可以想象，可以使用线性回归，也就是画一条线来把两个类别分开，但是这种分割很粗暴，准确性也不高，所以接下来要使用的算法要使用概率的方法区分两个类别，比如，算法返回0.9，那么代表属于类别A的概率是90% 逻辑函数 Logistic Function这里使用的逻辑函数正好符合概率的定义，即函数返回值在[0, 1]区间内，函数又被称作sigmod函数： $$y=\dfrac {1} {1+e^{-x}}$$ 12345x_values = np.linspace(-5, 5, 100)y_values = [1 / (1 + np.exp(-x)) for x in x_values]plt.plot(x_values, y_values)plt.xlabel("X")plt.ylabel("y") 将逻辑函数应用到数据上现在，数据集有两个属性Sepal Length和Sepal Width，这两个属性可以写到如下的等式中： $$x=\theta_{0}+\theta_{1}SW +\theta_{2}SL$$ SL代表Sepal Length这个特征，SW代表Sepal Width这个特征，假如神告诉我们 $\theta_{0} = 1$，$\theta_{1} = 2$，$\theta_{2} = 4$，那么，长度为5并且宽度为3.5的这个品种，$x=1+\left( 2\ast 3.5\right) +\left( 4\ast 5\right) = 28 $，代入逻辑函数： $$\dfrac{1} {1+e^{-28}}=0.99$$ 说明这个品种数据Setosa的概率为99%。那么，告诉我们 $\theta$的取值的神是谁呢？ 算法学习Cost Function在学习线性回归时候，当时使用的是Square Error作为损失函数，那么在逻辑回归中能不能也用这种损失函数呢？当然可以，不过在逻辑回归算法中，使用Square Error作为损失函数是非凸函数，也就是说有多个局部最小值，不能取到全局最小值，所以这里应该使用其他的损失函数。 想象一下，我们假设求出来一个属性的结果值是1，也就是预测为Setosa类别，那么预测为Versicolor类别的概率为0，在全部的数据集上，假设数据都是独立分布的，那么我们的目标就是：把每个单独类别的概率结果值累乘起来，并求最大值： $$\prod_{Setosa}\frac{1}{1 + e^{-(\theta_{0} + \theta{1}SW + \theta_{2}SL)}}\prod_{Versicolor}1 - \frac{1}{1 + e^{-(\theta_{0} + \theta{1}SW + \theta_{2}SL)}}$$ 参考上面定义的逻辑函数： $$h(x) = \frac{1}{1 + e^{-x}}$$ 那么我们的目标函数就是求下面函数的最大值： $$\prod_{Setosa}h(x)\prod_{Versicolor}1 - h(x)$$ 解释一下，加入类别分别为0和1，回归结果$h_\theta(x)$表示样本属于类别1的概率，那么样本属于类别0的概率为 $1-h_\theta(x)$，则有 $$p(y=1|x,\theta)=h_\theta(x)$$ $$p(y=0|x,\theta)=1-h_\theta(x)$$ 可以写为下面公式，含义为：某一个观测值的概率 $$p(y|x,\theta)=h_\theta(x)^y(1-h_\theta(x))^{1-y}$$ 由于各个观测值相互独立，那么联合分布可以表示成各个观测值概率的乘积： $$L(\theta)=\prod_{i=1}^m{h_\theta(x^{(i)})^{y^{(i)}}(1-h_\theta(x^{(i)}))^{1-y^{(i)}}}$$ 上式称为n个观测的似然函数。我们的目标是能够求出使这一似然函数的值最大的参数估计。对上面的似然函数取对数 $$\begin{aligned} l(\theta)=log(L(\theta))=log(\prod_{i=1}^m{h_\theta(x^{(i)})^{y^{(i)}}(1-h_\theta(x^{(i)}))^{1-y^{(i)}}})=\sum_{i=1}^m{(y^{(i)}log(h_\theta(x^{(i)}))+(1-y^{(i)})log(1-h_\theta(x^{(i)})))} \end{aligned}$$ 最大化似然函数，使用梯度下降法，求出$\theta$值，稍微变换一下，那就是求下面式子的最小值 $$J\left( \theta \right) = -\sum_{i=1}^{m}y^{(i)}log(h(x^{(i)})) + (1-y^{(i)})log(1-h(x^{(i)}))$$ 梯度下降算法梯度下降算法为： $$\begin{aligned} \theta_j:=\theta_j-\alpha\frac{\partial J(\theta)}{\partial\theta_j} \end{aligned}$$ 梯度下降算法的推导对$\theta$参数求导，可得 $$\begin{aligned} \frac{\partial logh_\theta(x^{(i)})}{\partial\theta_j}&amp;=\frac{\partial log(g(\theta^T x^{(i)}))}{\partial\theta_j}\&amp;=\frac{1}{g(\theta^T x^{(i)})}{ g(\theta^T x^{(i)})) (1-g(\theta^T x^{(i)}))x_j^{(i)}}\&amp;=(1- g(\theta^T x^{(i)}))) x_j^{(i)}\&amp;=(1-h_\theta(x^{(i)}))x_j^{(i)} \end{aligned}$$ 同理可得， $$\begin{aligned} \frac{\partial(1-logh_\theta(x^{(i)}))}{\partial\theta_j}=-h_\theta(x^{(i)})x_j^{(i)} \end{aligned}$$ 所以 $$\begin{aligned} \frac{\partial l(\theta)}{\partial\theta_j}&amp;=\sum_{i=1}^m{(y^{(i)}(1-h_\theta(x^{(i)}))x_j^{(i)}+(1-y^{(i)})(-h_\theta(x^{(i)})x_j^{(i)}))}\&amp;=\sum_{i=1}^m{(y^{(i)}-h_\theta(x^{(i)}))x_j^{(i)}} \end{aligned}$$ 那么，最终梯度下降算法为： $$\begin{aligned} \theta_j:=\theta_j-\alpha\sum_{i=1}^m{(y^{(i)}-h_\theta(x^{(i)}))x_j^{(i)}} \end{aligned}$$ 注：虽然得到的梯度下降算法表面上看去与线性回归一样，但是这里 的 $h_{\theta }\left( x\right) =\dfrac {1} {1+e^{-\theta ^{T}x}}$ 与线性回归中不同。 梯度下降算法的技巧 变量缩放 (Normalize Variable) $\alpha$选择 设定收敛条件 实现Logigtic Regrssion算法以上，介绍了Logistic Regression算法的详细推导过程，下面就用Python来实现这个算法 首先是逻辑回归函数，也就是sigmoid函数 12def sigmoid(theta, x): return 1.0 / (1 + np.exp(-x.dot(theta))) 然后使用梯度下降算法估算$\theta$值，首先是gradient值 $$(y^{(i)}-h_\theta(x^{(i)}))x_j^{(i)}$$ 123def gradient(theta, x, y): first_part = sigmoid(theta, x) - np.squeeze(y) return first_part.T.dot(x) 损失函数cost function $$-\sum_{i=1}^{m}y^{(i)}log(h(x^{(i)})) + (1-y^{(i)})log(1-h(x^{(i)}))$$ 123456def cost_function(theta, x, y): h_theta = sigmoid(theta, x) y = np.squeeze(y) first = y * np.log(h_theta) second = (1 - y) * np.log(1 - h_theta) return np.mean(-first - second) 梯度下降算法，这种梯度下降算法也叫批量梯度下降(Batch gradient descent) $$\begin{aligned} \theta_j:=\theta_j-\alpha\sum_{i=1}^m{(y^{(i)}-h_\theta(x^{(i)}))x_j^{(i)}} \end{aligned}$$ 123456789101112def gradient_descent(theta, X, y, alpha=0.001, converge): X = (X - np.mean(X, axis=0)) / np.std(X, axis=0) cost_iter = [] cost = cost_function(theta, X, y) cost_iter.append([0, cost]) i = 1 while(change_cost &gt; converge): theta = theta - (alpha * gradient(theta, X, y)) cost = cost_function(theta, X, y) cost_iter.append([i, cost]) i += 1 return theta, np.array(cost_iter) 预测方法 12345def predict_function(theta, x): x = (x - np.mean(x, axis=0)) / np.std(x, axis=0) pred_prob = sigmoid(theta, x) pred_value = np.where(pred_prob &gt;= 0.5, 1, 0) return pred_value 损失函数变化趋势画出cost function的变化趋势，看看是不是已经收敛了 看来梯度下降算法已经收敛了。 使用Scikit-Learn中的Logistic Regression算法Scikit-Learn库中，已经包含了逻辑回归算法，下面用这个工具集来体验一下这个算法。 1234from sklearn import linear_modelmodel = linear_model.LogisticRegression()model.fit(X, y)model.predict(X_test) 其他优化算法 BFGS 随机梯度下降 Stochastic gradient descent L-BFGS Conjugate Gradient]]></content>
      <categories>
        <category>算法</category>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>Algorithm</tag>
        <tag>Python</tag>
        <tag>Scikit-Learn</tag>
        <tag>Logistic Regression</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[单变量线性回归]]></title>
    <url>%2F2015%2F08%2F13%2F2015-08-19-linear-regression-with-one-variable%2F</url>
    <content type="text"><![CDATA[1月份的时候，参加了Coursera上面Andrew Ng的Machine Learning课程，课程断断续续的学，没有透彻的理解、推导，再加上作业使用Octave完成，并且还是有模版的，不是从头到尾做出来的，所以效果很差，虽然拿到了完成证书，但是过后即忘。我觉得是时候从头学习一遍，并且用Python实现所有的作业内容了。 这里写个系列，就当作为这门课程的课程笔记。 机器学习的本质是首先将训练集“喂给”学习算法，进而学习到一个假设(Hypothesis)，然后将特征值作为输入变量输入给Hypothesis，得出输出结果。 线性回归先说一元线性回归，这里只有一个特征值x，Hypothesis可以写为 $$h_{\theta }\left( x\right) =\theta_{0}+\theta_{1}x$$ 代价函数 Cost Function现在假设已经有了这个假设，那么如何评价这个假设的准确性呢，这里用模型预测的值减去训练集中实际值来衡量，这个叫建模误差。线性回归的目标就是使建模误差最小化，从而找出能使建模误差最小化的模型参数。代价函数为： $$J(\theta) = \frac{1}{2m}\sum_{i=1}^m (h_\theta(x^{(i)})-y^{(i)})^2$$ 则目标为求出使得$J(\theta)$最小的$\theta$参数 梯度下降算法现在使用梯度下降算法来求出$\theta$参数，梯度下降算法的推导如下： $$\begin{aligned} \theta_j:=\theta_j-\alpha\frac{\partial J(\theta)}{\partial\theta_j} \end{aligned}$$ 对$\theta_0$的偏导数为： $$\begin{aligned} \frac{\partial}{\partial \theta_0}J(\theta_0, \theta_1) = \frac{1}{m} \sum_{i=1}^m (h_\theta(x^{(i)})-y^{(i)})\end{aligned}$$ 对$\theta_1$的偏导数为： $$\begin{aligned} \frac{\partial}{\partial \theta_1}J(\theta_0, \theta_1) = \frac{1}{m} \sum_{i=1}^m (h_\theta(x^{(i)})-y^{(i)})x^{(i)}\end{aligned}$$ 使用Python实现一元线性回归为了提高性能，使用 numpy 包来实现向量化计算， 12345678import numpy as npdef hypothesis(theta, x): return np.dot(x, theta)def cost_function(theta, x, y): loss = hypothesis(theta, x) - y return np.sum(loss ** 2) / (2 * len(y)) 实现梯度下降算法，需要计算下面四个部分： 计算假设Hypothesis 计算损失 loss = hypothesis - y，然后求出square root 计算Gradient = X’ * loss / m 更新参数theta -= alpha * gradient 1234567891011121314def gradient_descent(alpha, x, y, iters): # number of training dataset m = x.shape[0] theta = np.zeros(2) cost_iter = [] for iter in range(iters): h_theta = hypothesis(theta, x) loss = h_theta - y J = np.sum(loss ** 2) / (2 * m) cost_iter.append([iter, J]) # print "iter %s | J: %.3f" % (iter, J) gradient = np.dot(x.T, loss) / m theta -= alpha * gradient return theta, cost_iter 接下来造一些假数据试验一下 1234567from sklearn.datasets.samples_generator import make_regressionx, y = make_regression(n_samples=100, n_features=1, n_informative=1, random_state=0, noise=35)m, n = np.shape(x)x = np.c_[np.ones(m), x] ## add column value 1 as x0alpha = 0.01theta, cost_iter = gradient_descent(alpha, x, y, 1000)print theta 求出的参数值为[-2.8484052 , 43.202331] 将训练集数据和Hypothesis函数画出来 1234for i in range(x.shape[1]): y_predict = theta[0] + theta[1] * xplt.plot(x[:, 1], y, 'o')plt.plot(x, y_predict, 'k-') 接下来画出代价函数在每次迭代过程中的变化趋势，可以看出算法是否收敛 123plt.plot(cost_iter[:500, 0], cost_iter[:500, 1])plt.xlabel("Iteration Number")plt.ylabel("Cost") 接下来有必要好好复习一下线性代数、numpy和向量化计算。]]></content>
      <categories>
        <category>算法</category>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>Algorithm</tag>
        <tag>Python</tag>
        <tag>Coursera</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[利用Spark进行单词计数]]></title>
    <url>%2F2015%2F08%2F12%2F2015-08-12-word-count-example-in-spark%2F</url>
    <content type="text"><![CDATA[这里就不再介绍Spark了，这篇文章主要记录一下关于Spark的核心RDD的相关操作以及以单词计数这个简单的例子，描述一下Spark的处理流程。 Spark RDDSpark是以RDD概念为中心运行的。RDD是一个容错的、可以被并行操作的元素集合。创建一个RDD有两个方法：在你的驱动程序中并行化一个已经存在的集合；从外部存储系统中引用一个数据集，这个存储系统可以是一个共享文件系统，比如HDFS、HBase或任意提供了Hadoop输入格式的数据来源。 RDD支持两类操作： 转换(Transform) 动作(Action) 还是不翻译的好，下面都用英文描述。Transform：用于从已有的数据集转换产生新的数据集，Transform的操作是Lazy Evaluation的，也就是说这条语句过后，转换并没有发生，而是在下一个Action调用的时候才会返回结果。Action：用于计算结果并向驱动程序返回结果。 演示一下上面两种基本操作： 123lines = sc.textFile("data.txt")lineLength = line.map(lambda x: len(x))totalLength = lineLength.reduce(lambda x, y: x + y) 第一行是有外部存储系统中创建一个RDD对象，第二行定义map操作，是一个Transform操作，由于Lazy Evaluation，对象lineLength并没有立即计算得到。第三行，reduce是一个Action操作，这时，Spark将整个计算过程划分成许多任务在多台机器上并行执行，每台机器运行自己部分的map操作和reduce操作，最终将自己部分的运算结果返回给驱动程序。 12lineLength.persist()# lineLength.cache() 这一行，Spark将lineLength对象保存在内存中，以便后面计算中使用。Spark的一个重要功能就是在将数据集持久化（或缓存）到内存中以便在多个操作中重复使用。 以上就是RDD的一些基本操作，API文档中写的都很清楚，我就不多说了。 统计一篇文档中单词的个数首先，写一个函数，用来计算单词个数 12345def wordCount(wordListRDD): wordCountsCollected = wordListRDD .map(lambda x: (x, 1)) .reduceByKey(lambda x, y: x + y) return wordCountsCollected 使用正则表达式清理原始文本 123456import reimport stringdef removePunctuation(text): regex = re.compile('[%s]' % re.escape(string.punctuation)) return regex.sub('', text).lower().strip()print removePunctuation(' No under_score!') 去读文件内容到RDD中 1234567import os.pathbaseDir = os.path.join('data')inputPath = os.path.join('cs100', 'lab1', 'shakespeare.txt')fileName = os.path.join(baseDir, inputPath)# shakespeareRDD = (sc.textFile(fileName, 8).map(removePunctuation))print '\n'.join(shakespeareRDD.zipWithIndex().map(lambda (l, num): '&#123;0&#125;: &#123;1&#125;'.format(num,l)).take(15)) 这时候，需要把单词通过空格隔开，然后过滤掉为空的内容 1234shakespeareWordsRDD = shakespeareRDD.flatMap(lambda x: x.split())shakespeareWordCount = shakespeareWordsRDD.count()print shakespeareWordsRDD.top(5)shakeWordsRDD = shakespeareWordsRDD 统计出出现次数前15多的单词以及个数： 12top15WordAndCounts = wordCount(shakeWordsRDD).takeOrdered(15, key=lambda (k, v): -v)print '\n'.join(map(lambda (w, c): '&#123;0&#125;: &#123;1&#125;'.format(w, c), top15WordsAndCounts)) 输出结果为： word count the: 27361 and: 26028 i: 20681 to: 19150 of: 17463 a: 14593 you: 13615 my: 12481 in: 10956 that: 10890 is: 9134 not: 8497 with: 7771 me: 7769 it: 7678 Spark是用Scala写出来的，所以可想而知如果用Scala写的效率会比Python高一些，在这儿顺便贴一个Scala版写的WordCount： 123val wordCounts = textFile.flatMap(line =&gt; line.split(" ")) .map(word =&gt; (word, 1)) .reduceByKey((a, b) =&gt; a + b) 真是简洁，Spark真好，嘿嘿~]]></content>
      <categories>
        <category>Big Data</category>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Big Data</tag>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[朴素贝叶斯算法的一些细节和小技巧]]></title>
    <url>%2F2015%2F08%2F11%2F2015-08-11-some-note-about-naive-bayes%2F</url>
    <content type="text"><![CDATA[某特征属性的条件概率为0当特征属性为离散值时，只要统计训练样本中各个划分在每个类别中出现的频率即可用来估计P(a|y)，若某一特征值的概率为0则会使整个概率乘积变为0，这会让分类器的准确性大幅下降。 这时候使用Laplace校准：即假定训练数据库很大，以至于对每个计数加1造成的估计概率的变化忽略不计。 连续分布假定值服从高斯分布(正态分布)当特征属性为连续值时，通常假定其值服从高斯分布，即： $$p\left( x_{i}|y\right) =\dfrac {1} {\sqrt {2\pi \sigma_{y}^{2}}}exp\left( -\dfrac {\left( x_{i}-\mu_{y}\right) ^{2}} {2\sigma_{y}^{2}}\right)$$ 所以，对于连续分布的样本特征的训练就是计算其均值和方差 小数连续相乘实际项目中，概率P往往是值很小的小数，连续的微小小数相乘容易造成下溢出使乘积为0或者得不到正确答案。一种解决办法就是对乘积取自然对数，将连乘变为连加，$\ln \left( AB\right) =\ln A+\ln B$。采用自然对数处理不会带来任何损失，可以避免下溢出或者浮点数舍入导致的错误。]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>Algorithm</tag>
        <tag>Bayes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[配置Octopress支持LaTex数学公式]]></title>
    <url>%2F2015%2F08%2F08%2F2015-08-08-adding-support-for-math-formula%2F</url>
    <content type="text"><![CDATA[Octopress 默认不支持 LaTex 写数学公式需要更改配置才可以。 设置需要使用kramdown来支持LaTex写数学公式 用kramdown替换rdiscount 安装kramdown 1$ sudo gem install kramdown 修改_config.yml配置文件，将所有rdiscount替换成kramdown 修改Gemfile，把gem &#39;rdiscount&#39;换成gem &#39;kramdown&#39; 添加MathJax配置在/source/_includes/custom/head.html文件中，添加如下代码： 123456789101112131415&lt;!-- mathjax config similar to math.stackexchange --&gt;&lt;script type="text/x-mathjax-config"&gt;MathJax.Hub.Config(&#123; jax: ["input/TeX", "output/HTML-CSS"], tex2jax: &#123; inlineMath: [ ['$', '$'] ], displayMath: [ ['$$', '$$']], processEscapes: true, skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'] &#125;, messageStyle: "none", "HTML-CSS": &#123; preferredFont: "TeX", availableFonts: ["STIX","TeX"] &#125;&#125;);&lt;/script&gt;&lt;script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"&gt;&lt;/script&gt; 修复 MathJax 右击页面空白 bug修改~/sass/base/theme.scss文件，如下代码变为： 1234&gt; div#main &#123; background: $sidebar-bg $noise-bg; border-bottom: 1px solid $page-border-bottom; &gt; div &#123; 随之出现的问题，以及解决方法将rdiscount替换成kramdown之后，以前写的博客里面，很多内容都不能正确显示了，并且在rake generate时候，会报错，内容大约是：Error: Pygments can&#39;t parse unknown language: &lt;/p&gt; 原生的语法高亮插件Pygments很强大，支持语言也很多，但是这时候报的错误让人一头雾水。 找出原因报错部分代码在/plugins/pygments_code.rb文件中， 12345678910111213def self.pygments(code, lang) if defined?(PYGMENTS_CACHE_DIR) path = File.join(PYGMENTS_CACHE_DIR, "#&#123;lang&#125;-#&#123;Digest::MD5.hexdigest(code)&#125;.html") if File.exist?(path) highlighted_code = File.read(path) else begin highlighted_code = Pygments.highlight(code, :lexer =&gt; lang, :formatter =&gt; 'html', :options =&gt; &#123;:encoding =&gt; 'utf-8', :startinline =&gt; true&#125;) rescue MentosError raise "Pygments can't parse unknown language: #&#123;lang&#125;." end File.open(path, 'w') &#123;|f| f.print(highlighted_code) &#125; end 修改一下代码，将出问题的代码高亮部分抛出来， 1raise &quot;Pygments can&apos;t parse unknown language: #&#123;lang&#125;#&#123;code&#125;.&quot; Google了一下原因， 原来是因为最新版的pygments这个插件对于Markdown的书写要求更严格了： Some of my older blog posts did not contain a space between the triple-backtick characters and the name of the language being highlighted. Earlier versions of pygments did not care, but the current version is a stickler. pygments appears to want a blank line between any triple-backtick line and any other text in the blog post. 好吧，以后写文章要更细心一点了。:) 参考： Octopress中使用Latex写数学公式 Pygments Unknown Language]]></content>
      <categories>
        <category>备忘</category>
      </categories>
      <tags>
        <tag>Octopress</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[朴素贝叶斯分类器实践]]></title>
    <url>%2F2015%2F08%2F07%2F2015-08-07-naive-bayes-classifier%2F</url>
    <content type="text"><![CDATA[实际案例举个运动员的例子： 如果我问你Brittney Griner的运动项目是什么，她有6尺8寸高，207磅重，你会说“篮球”；我再问你对此分类的准确度有多少信心，你会回答“非常有信心”。我再问你Heather Zurich，6尺1寸高，重176磅，你可能就不能确定地说她是打篮球的了，至少不会像之前判定Brittney那样肯定。因为从Heather的身高体重来看她也有可能是跑马拉松的。最后，我再问你Yumiko Hara的运动项目，她5尺4寸高，95磅重，你也许会说她是跳体操的，但也不太敢肯定，因为有些马拉松运动员也是类似的身高体重。 ——选自A Programmer’s Guide to Data Mining 这里所说的分类，就用到了所谓的概率模型。 朴素贝叶斯算法朴素贝叶斯算法使用每个属性(特征)属于某个类的概率做出预测，这是一个监督性学习算法，对一个预测性问题进行概率建模。训练模型的过程可以看做是对条件概率的计算，何以计算每个类别的相应条件概率来估计分类结果。 这个算法基于一个假设：所有特征相互独立，任意特征的值和其他特征的值没有关联关系，这种假设在实际生活中几乎不存在，但是朴素贝叶斯算法在很多领域，尤其是自然语言处理领域很成功。其他的典型应用还有垃圾邮件过滤等等。 贝叶斯分类器的基本原理 图片引用自Matt Buck 贝叶斯定理给出贝叶斯定理： $$p\left( h|D\right) =\dfrac {p\left( D|h\right) .p\left( h\right) } {p\left( D\right) }$$ 这个公式是贝叶斯方法论的基石。拿分类问题来说，h代表分类的类别，D代表已知的特征d1, d2, d3…，朴素贝叶斯算法的朴素之处在于，假设了特征d1, d2, d3…相互独立，所以贝叶斯定理又能写成： $$p\left( h|f_{1},f_{2}…f_{n}\right) =\dfrac {p\left( h\right) \prod_{i=1}^n p\left( f_{i}|h\right) } {p\left( f_{1},f_{2}…f_{n}\right) }$$ 由于$P\left( f_{1},\ldots f_{n}\right) $ 可视作常数，类变量$h$的条件概率分布就可以表达为： $$p\left( h|f_{1},…,f_{n}\right) =\dfrac {1} {Z}.p\left( h\right) \prod _{i=1}^{n}p\left( F_{i}|h\right)$$ 从概率模型中构造分类器以上，就导出了独立分布的特征模型，也就是朴素贝叶斯模型，使用最大后验概率MAP(Maximum A Posteriori estimation)选出条件概率最大的那个分类，这就是朴素贝叶斯分类器： $$\widehat {y}=\arg \max_{y}p\left( y\right) \prod_{i=1}^{n}p\left( x_{i}|y\right) $$ 参数估计所有的模型参数都可以通过训练集的相关频率来估计。常用方法是概率的最大似然估计，类的先验概率可以通过假设各类等概率来计算（先验概率 = 1 / (类的数量)），或者通过训练集的各类样本出现的次数来估计（A类先验概率=（A类样本的数量）/(样本总数)）。为了估计特征的分布参数，我们要先假设训练集数据满足某种分布或者非参数模型。 常见的分布模型：高斯分布(Gaussian naive Bayes)、多项分布(Multinomial naive Bayes)、伯努利分布(Bernoulli naive Bayes)等. 使用Scikit-learn进行文本分类目的：使用Scikit-learn库自带的新闻信息数据来进行试验，该数据集有19,000个新闻信息组成，通过新闻文本的内容，使用scikit-learn中的朴素贝叶斯算法，来判断新闻属于什么主题类别。参考：Scikit-learn Totorial 数据集123from sklearn.datesets import fetch_20newsgroupsnews = fetch_20newsgroups(subset='all')print news.keys() 查看一下第一条新闻的内容和分组 [‘description’, ‘DESCR’, ‘filenames’, ‘target_names’, ‘data’, ‘target’] From: Mamatha Devineni Ratnam &#109;&#114;&#52;&#x37;&#43;&#64;&#97;&#x6e;&#100;&#x72;&#101;&#x77;&#x2e;&#x63;&#x6d;&#x75;&#46;&#x65;&#100;&#x75;Subject: Pens fans reactionsOrganization: Post Office, Carnegie Mellon, Pittsburgh, PA… 10 rec.sport.hockey 划分训练集和测试集，分为80%训练集，20%测试集 123456split_rate = 0.8split_size = int(len(news.data) * split_rate)X_train = news.data[:split_size]y_train = news.target[:split_size]X_test = news.data[split_size:]y_test = news.target[split_size:] 特征提取为了使机器学习算法应用在文本内容上，首先应该把文本内容装换为数字特征。这里使用词袋模型(Bags of words) 词袋模型在信息检索中，Bag of words model假定对于一个文本，忽略其词序和语法，句法，将其仅仅看做是一个词集合，或者说是词的一个组合，文本中每个词的出现都是独立的，不依赖于其他词是否出现，或者说当这篇文章的作者在任意一个位置选择一个词汇都不受前面句子的影响而独立选择的。 Scikit-learn提供了一些实用工具(sklearn.feature_extraction.text)可以从文本内容中提取数值特征 1234567891011from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizerfrom sklearn.feature_extraction.text import TfidfTransformer# Tokenizing textcount_vect = CountVectorizer()X_train_counts = count_vect.fit_transform(X_train)# Tftf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)X_train_tf = tf_transformer.transform(X_train_counts)# Tf_idftfidf_transformer = TfidfTransformer()X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts) 稀疏性大多数文档通常只会使用语料库中所有词的一个子集，因而产生的矩阵将有许多特征值是0（通常99%以上都是0）。例如，一组10,000个短文本（比如email）会使用100,000的词汇总量，而每个文档会使用100到1,000个唯一的词。为了能够在内存中存储这个矩阵，同时也提供矩阵/向量代数运算的速度，通常会使用稀疏表征例如在scipy.sparse包中提供的表征。 训练模型上面使用文本中词的出现次数作为数值特征，可以使用多项分布估计这个特征，使用sklearn.naive_bayes模块的MultinomialNB类来训练模型。 12345678910from sklearn.naive_bayes import MultinomialNB# create classifierclf = MultinomialNB().fit(X_train_tfidf, y_train)docs_new = ['God is love', 'OpenGL on the GPU is fast']X_new_counts = count_vect.transform(docs_new)X_new_tfidf = tfidf_transformer.transform(X_new_counts)# using classifier to predictpredicted = clf.predict(X_new_tfidf)for doc, category in zip(docs_new, predicted): print('%r =&gt; %s' % (doc, news.target_names[category])) 使用Pipline这个类构建复合分类器Scikit-learn为了使向量化 =&gt; 转换 =&gt; 分类这个过程更容易，提供了Pipeline类来构建复合分类器，例如： 1234from sklearn.pipeline import Pipelinetext_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),]) 创建新的训练模型 1234567891011121314151617from sklearn.naive_bayes import MultinomialNBfrom sklearn.pipeline import Pipelinefrom sklearn.feature_extraction.text import TfidfVectorizer, HashingVectorizer, CountVectorizer #nbc means naive bayes classifiernbc_1 = Pipeline([ ('vect', CountVectorizer()), ('clf', MultinomialNB()),])nbc_2 = Pipeline([ ('vect', HashingVectorizer(non_negative=True)), ('clf', MultinomialNB()),])nbc_3 = Pipeline([ ('vect', TfidfVectorizer()), ('clf', MultinomialNB()),])# classifiernbcs = [nbc_1, nbc_2, nbc_3] 交叉验证下面是一个交叉验证函数： 1234567891011from sklearn.cross_validation import cross_val_score, KFoldfrom scipy.stats import semimport numpy as np# cross validation functiondef evaluate_cross_validation(clf, X, y, K): # create a k-fold croos validation iterator of k folds cv = KFold(len(y), K, shuffle=True, random_state=0) # by default the score used is the one returned by score method of the estimator (accuracy) scores = cross_val_score(clf, X, y, cv=cv) print scores print ("Mean score: &#123;0:.3f&#125; (+/-&#123;1:.3f&#125;)").format(np.mean(scores), sem(scores)) 将训练集分为10份，输出验证分数： 12for nbc in nbcs: evaluate_cross_validation(nbc, X_train, y_train, 10) 结果为： CountVectorizer Mean score: 0.849 (+/-0.002) HashingVectorizer Mean score: 0.765 (+/-0.006) TfidfVectorizer Mean score: 0.848 (+/-0.004) 可以看出：CountVectorizer和TfidfVectorizer特征提取的方法要比HashingVectorizer效果好。 优化模型优化单词提取在使用TfidfVectorizer特征提取时候，使用正则表达式，默认的正则表达式是u&#39;(?u)\b\w\w+\b&#39;，使用新的正则表达式ur&quot;\b[a-z0-9_\-\.]+[a-z][a-z0-9_\-\.]+\b&quot; 1234567nbc_4 = Pipeline([ ('vect', TfidfVectorizer( token_pattern=ur"\b[a-z0-9_\-\.]+[a-z][a-z0-9_\-\.]+\b",) ), ('clf', MultinomialNB()),])evaluate_cross_validation(nbc_4, X_train, y_train, 10) 分数是：Mean score: 0.861 (+/-0.004) ，结果好了一点 排除停止词TfidfVectorizer的一个参数stop_words，这个参数指定的词将被省略不计入到标记词的列表中，这里使用鼎鼎有名的NLTK语料库。 1234567891011import nltk# nltk.download()stopwords = nltk.corpus.stopwords.words('english')nbc_5 = Pipeline([ ('vect', TfidfVectorizer( stop_words=stop_words, token_pattern=ur"\b[a-z0-9_\-\.]+[a-z][a-z0-9_\-\.]+\b", )), ('clf', MultinomialNB()),])evaluate_cross_validation(nbc_5, X_train, Y_train, 10) 分数是：Mean score: 0.879 (+/-0.003)，结果又提高了 调整贝叶斯分类器的alpha参数MultinomialNB有一个alpha参数，该参数是一个平滑参数，默认是1.0，我们将其设为0.01 12345678nbc_6 = Pipeline([ ('vect', TfidfVectorizer( stop_words=stopwords, token_pattern=ur"\b[a-z0-9_\-\.]+[a-z][a-z0-9_\-\.]+\b", )), ('clf', MultinomialNB(alpha=0.01)),])evaluate_cross_validation(nbc_6, X_train, y_train, 10) 分数为：Mean score: 0.917 (+/-0.002)，哎呦，好像不错哦！不过问题来了，调整参数优化不能靠蒙，如何寻找最好的参数，使得交叉验证的分数最高呢？ 使用Grid Search优化参数使用GridSearch寻找vectorizer词频统计, tfidftransformer特征变换和MultinomialNB classifier的最优参数 Scikit-learn上关于GridSearch的介绍 1234567891011121314151617pipeline = Pipeline([('vect',CountVectorizer()),('tfidf',TfidfTransformer()),('clf',MultinomialNB()),]);parameters = &#123; 'vect__max_df': (0.5, 0.75), 'vect__max_features': (None, 5000, 10000), 'tfidf__use_idf': (True, False), 'clf__alpha': (1, 0.1, 0.01, 0.001, 0.0001),&#125;grid_search = GridSearchCV(pipeline, parameters, n_jobs=1)from time import timet0 = time()grid_search.fit(X_train, y_train)print "done in %0.3fs" % (time() - t0)print "Best score: %0.3f" % grid_search.best_score_ 输出最优参数1234567891011from sklearn import metricsbest_parameters = dict()best_parameters = grid_search.best_estimator_.get_params()for param_name in sorted(parameters.keys()): print "\t%s: %r" % (param_name, best_parameters[param_name])pipeline.set_params(clf__alpha = 1e-05, tfidf__use_idf = True, vect__max_df = 0.5, vect__max_features = None)pipeline.fit(X_train, y_train)pred = pipeline.predict(X_test) 经过漫长的等待，终于找出了最优参数： done in 1578.965sBest score: 0.902 clfalpha: 0.01tfidfuse_idf: Truevectmax_df: 0.5vectmax_features: None 在测试集上的准确率为：0.915，分类效果还是不错的 1print np.mean(pred == y_test) 评价分类效果在测试集上测试朴素贝叶斯分类器的分类效果 12345678from sklearn import metricsimport numpy as np#print X_test[0], y_test[0]for i in range(20): print str(i) + ": " + news.target_names[i]predicted = pipeline.fit(X_train, y_train).predict(X_test)print np.mean(predicted == y_test)print metrics.classification_report(y_test, predicted) 结果是这样的： id groupname 0 alt.atheism 1 comp.graphics 2 comp.os.ms-windows.misc 3 comp.sys.ibm.pc.hardware 4 comp.sys.mac.hardware 5 comp.windows.x 6 misc.forsale 7 rec.autos 8 rec.motorcycles 9 rec.sport.baseball 10 rec.sport.hockey 11 sci.crypt 12 sci.electronics 13 sci.med 14 sci.space 15 soc.religion.christian 16 talk.politics.guns 17 talk.politics.mideast 18 talk.politics.misc 19 talk.religion.misc 准确率：0.922811671088 id precision recall f1-score support 0 0.94 0.87 0.91 175 1 0.85 0.87 0.86 199 2 0.91 0.84 0.88 221 3 0.81 0.87 0.84 179 4 0.87 0.92 0.89 177 5 0.91 0.92 0.91 179 6 0.88 0.79 0.83 205 7 0.94 0.95 0.94 228 8 0.96 0.98 0.97 183 9 0.96 0.95 0.96 197 10 0.98 1.00 0.99 204 11 0.96 0.98 0.97 218 12 0.93 0.92 0.92 172 13 0.93 0.95 0.94 200 14 0.96 0.96 0.96 198 15 0.93 0.97 0.95 191 16 0.92 0.97 0.94 173 17 0.98 0.99 0.98 184 18 0.95 0.92 0.94 172 19 0.83 0.78 0.81 115 avg / total 0.92 0.92 0.92 3770 参考 JasonDing的 【机器学习实验】使用朴素贝叶斯进行文本的分类； Scikit-Learn Totorial]]></content>
      <categories>
        <category>算法</category>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>Algorithm</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[开始使用Scikit-Learn]]></title>
    <url>%2F2015%2F07%2F23%2F2015-07-23-start-to-use-scikit-learn%2F</url>
    <content type="text"><![CDATA[Python和R是做数据分析、数据挖掘、机器学习非常好的两门语言，在这儿不去讨论谁更好这个问题，没有最好，只有合适上手。对于码农出身，非科班统计学的我来说，使用Python相当习惯和顺手。 Python数据科学栈Python有很多做数据的类库，先列出常用的几个： Numpy、Scipy 基础数据类型 Matplotlib 绘图库 Pandas Ipython notebook Scikit-learn、MLlib 机器学习库 使用Scikit-learn的过程数据加载首先，将数据加载到内存中 12345678import numpy as npimport urlliburl = "http://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data"# download the fileraw_data = urllib.urlopen(url)dataset = np.loadtxt(raw_data, delimiter=',')X = dataset[:, 0:7]y = dataset[:, 8] X为特征数组，y为目标变量 数据标准化 (Data Normalization)大多数的梯度算法对数据的缩放很敏感，比如列A是体重数据(50kg等等)，列B是身高数据(170cm等等)，简单地说就是两个属性尺度不一样，所以在运行算法前要进行标准化或者叫归一化。 123from sklearn import preprocessing# normalize the data attributesnormalized_X = preprocessing.normalize(X) 特征选取虽然特征工程是一个相当有创造性的过程，有时候更多的是靠直觉和专业的知识，但对于特征的选取，已经有很多的算法可供直接使用。Scikit-Learn中的Recursive Feature Elimination Algorithm算法： 12345678from sklearn.feature_selection import RFEfrom sklearn.linear_model import LogisticRegressionmodel = LogisticRegression()# create RFE model and select 3 attributesrfe = RFE(model, 3)rfe = rfe.fit(X, y)print rfe.support_print rfe.ranking_ 机器学习算法看一看Scikit-learn库中所带的算法： 逻辑回归算法 Logistic Regression 朴素贝叶斯算法 Naive Bayes k-最邻算法 KNN 决策树 Decision Tree 支持向量机 SVM 逻辑回归 大多数情况下被用来解决分类问题（二元分类），但多类的分类（所谓的一对多方法）也适用。这个算法的优点是对于每一个输出的对象都有一个对应类别的概率。 1234567891011from sklearn import metricsfrom sklearn.linear_model import LogisticRegressionmodel = LogisticRegression()model.fit(X, y)print(model)# make predictionsexpected = ypredicted = model.predict(X)# summarize the fit of the modelprint(metrics.classification_report(expected, predicted))print(metrics.confusion_matrix(expected, predicted)) 朴素贝叶斯 它也是最有名的机器学习的算法之一，它的主要任务是恢复训练样本的数据分布密度。这个方法通常在多类的分类问题上表现的很好。 1234567891011from sklearn import metricsfrom sklearn.naive_bayes import GaussianNBmodel = GaussianNB()model.fit(X, y)print(model)# make predictionsexpected = ypredicted = model.predict(X)# summarize the fit of the modelprint(metrics.classification_report(expected, predicted))print(metrics.confusion_matrix(expected, predicted)) k-最近邻 kNN（k-最近邻）方法通常用于一个更复杂分类算法的一部分。例如，我们可以用它的估计值做为一个对象的特征。有时候，一个简单的kNN算法在良好选择的特征上会有很出色的表现。当参数（主要是metrics）被设置得当，这个算法在回归问题中通常表现出最好的质量。 123456789101112from sklearn import metricsfrom sklearn.neighbors import KNeighborsClassifier# fit a k-nearest neighbor model to the datamodel = KNeighborsClassifier()model.fit(X, y)print(model)# make predictionsexpected = ypredicted = model.predict(X)# summarize the fit of the modelprint(metrics.classification_report(expected, predicted))print(metrics.confusion_matrix(expected, predicted)) 决策树 分类和回归树（CART）经常被用于这么一类问题，在这类问题中对象有可分类的特征且被用于回归和分类问题。决策树很适用于多类分类。 123456789101112from sklearn import metricsfrom sklearn.tree import DecisionTreeClassifier# fit a CART model to the datamodel = DecisionTreeClassifier()model.fit(X, y)print(model)# make predictionsexpected = ypredicted = model.predict(X)# summarize the fit of the modelprint(metrics.classification_report(expected, predicted))print(metrics.confusion_matrix(expected, predicted)) 支持向量机SVM SVM（支持向量机）是最流行的机器学习算法之一，它主要用于分类问题。同样也用于逻辑回归，SVM在一对多方法的帮助下可以实现多类分类。 123456789101112from sklearn import metricsfrom sklearn.svm import SVC# fit a SVM model to the datamodel = SVC()model.fit(X, y)print(model)# make predictionsexpected = ypredicted = model.predict(X)# summarize the fit of the modelprint(metrics.classification_report(expected, predicted))print(metrics.confusion_matrix(expected, predicted)) 评价算法TBD 评价算法的好坏大约有几个方面： precision recall F1 score 优化算法的参数TBD]]></content>
      <categories>
        <category>算法</category>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>Python</tag>
        <tag>Scikit-Learn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Install Spark on Mac OSX Yosemite]]></title>
    <url>%2F2015%2F07%2F17%2F2015-07-17-install-spark-on-mac-osx-yosemite%2F</url>
    <content type="text"><![CDATA[Spark是个好东西。 Spark有以下四种运行模式： local: 本地单进程模式，用于本地开发测试Spark代码 standalone：分布式集群模式，Master-Worker架构，Master负责调度，Worker负责具体Task的执行 on yarn/mesos: ‌运行在yarn/mesos等资源管理框架之上，yarn/mesos提供资源管理，spark提供计算调度，并可与其他计算框架(如MapReduce/MPI/Storm)共同运行在同一个集群之上 on cloud(EC2): 运行在AWS的EC2之上 在Spark上又有多个应用，尤其是MLlib，Spark SQL和DataFrame，提供给数据科学家们无缝接口去搞所谓Data Science 本文记录一下我在Mac上安装Spark单机为分布式的过程 1.安装环境Spark依赖JDK 6.0以及Scala 2.9.3以上版本，安装好Java和Scala，然后配置好Java、Scala环境，最后再用java -version和scala -version验证一下 在~/.bash_profile中加入： 123# Setting PATH for scalaexport SCALA_HOME=/usr/local/Cellar/scala/2.11.6export PATH=$SCALA_HOME/bin:$PATH 别忘了 1source ~/.bash_profile 生效 由于在后面学习中主要会用到Spark的Python接口pyspark，所以在这儿也需要配置Python的环境变量： 1234# Setting PATH for Python 2.7# The orginal version is saved in .bash_profile.pysavePATH="/usr/local/Cellar/python/2.7.9/bin:$&#123;PATH&#125;"export PATH 2.伪分布式安装Spark的安装和简单，只需要将Spark的安装包download下来，加入PATH即可。这里我用的是Spark 1.4.0 当然，这里也可以使用Homebrew安装，那就更轻松了，直接 1$ brew install apache-spark 就搞定了，不过Homebrew安装没办法自己控制箱要安装的版本 这里我使用下载对Hadoop2.6的预编译版本安装 123cd /usr/local/Cellar/wget http://www.apache.org/dyn/closer.cgi/spark/spark-1.4.0/spark-1.4.0-bin-hadoop2.6.tgztar zxvf spark-1.4.0-bin-hadoop2.6.tgz 设置Spark环境变量，~/.bash_profile： 123456export SPARK_MASTER=localhostexport SPARK_LOCAL_IP=localhostexport SPARK_HOME=/usr/local/Cellar/spark-1.4.0-bin-hadoop2.6export PATH=$PATH:$SCALA_HOME/bin:$SPARK_HOME/binexport PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.8.2.1-src.zip:$PYTHONPATHexport PATH="/usr/local/sbin:$PATH" 安装完成，貌似也没什么安装哈~ 跑起来执行Spark根目录下的pyspark就可以以交互的模式使用Spark了，这也是他的一个优点 出现Spark的标志，那就说明安装成功了。下面再小配置下，让画面的log简单一点。在$SPARK_HOME/conf/下配置一下log4j的设置。 把log4j.properties.template文件复制一份，并删掉.template的扩展名 把这个文件中的INFO内容全部替换成WARN 在IPython中运行Spark说Spark好，那么IPython更是一大杀器，这个以后再介绍。先说设置 首先，创建IPython的Spark配置 1$ ipython profile create pyspark 然后创建文件$HOME/.ipython/profile_spark/startup/00-pyspark-setup.py并添加： 12345678import osimport sysspark_home = os.environ.get('SPARK_HOME', None)if not spark_home: raise ValueError('SPARK_HOME environment variable is not set')sys.path.insert(0, os.path.join(spark_home, 'python'))sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.8.2.1-src.zip'))execfile(os.path.join(spark_home, 'python/pyspark/shell.py')) 在IPython notebook中跑Spark 1$ ipython notebook --profile=pyspark 开始学习Spark吧！ 参考： Getting Started with Spark (in Python)]]></content>
      <categories>
        <category>Big Data</category>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Big Data</tag>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Install Hadoop on Yosemite]]></title>
    <url>%2F2015%2F07%2F17%2F2015-07-17-install-hadoop-on-mac-osx-yosemite%2F</url>
    <content type="text"><![CDATA[终于进入正题，开始写一写我在大数据方面走过的路，自认为被其他人甩下了，所以一定要紧追而上。 首先现在我的Mac上装上单节点的Hadoop玩玩，个人感觉Apache系列的项目，只要download下来，再配置以下参数就能玩了。 在这里感谢如下教程： INSTALLING HADOOP ON MAC Writing an Hadoop MapReduce Program in Python 下面开始吧 准备这个阶段主要就是准备一下JAVA的环境，Mac默认是安装了Java的，不过版本就不知道了，这个还是自己安装一下并且写到环境变量里来得踏实 Java Download 安装完之后，Java被装到了这个位置1/Library/Java/JavaVirtualMachines/jdk1.7.0_79.jdk/Contents/Home ，把这个地址写到系统的环境变量文件.bash_profile里 123# Setting PATH for javaexport JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.7.0_79.jdk/Contents/Homeexport PATH=$JAVA_HOME/bin:$PATH 配置SSH Nothing needs to be done here if you have already generated ssh keys. To verify just check for the existance of ~/.ssh/id_rsa and the ~/.ssh/id_rsa.pub files. If not the keys can be generated using 1$ ssh-keygen -t rsa Enable Remote Login “System Preferences” -&gt; “Sharing”. Check “Remote Login”Authorize SSH KeysTo allow your system to accept login, we have to make it aware of the keys that will be used 1$ cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys Let’s try to login. 123$ ssh localhostLast login: Fri Mar 6 20:30:53 2015$ exit 安装Homebrew在Mac上，最好的包安装工具就是Homebrew，执行下面代码安装： 1$ ruby -e "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)" 安装Hadoop我去，这么简单，直接 1$ brew install hadoop 就搞定了。。。这样，Hadoop会被安装在/usr/local/Cellar/hadoop目录下 下面才是重点，配置Hadoop 配置Hadoophadoop-env.sh该文件在/usr/local/Cellar/hadoop/2.6.0/libexec/etc/hadoop/hadoop-env.sh 找到如下这行： 1export HADOOP_OPTS="$HADOOP_OPTS -Djava.net.preferIPv4Stack=true" 改为： 1export HADOOP_OPTS="$HADOOP_OPTS -Djava.net.preferIPv4Stack=true -Djava.security.krb5.realm= -Djava.security.krb5.kdc=" Core-site.xml该文件在/usr/local/Cellar/hadoop/2.6.0/libexec/etc/hadoop/core-site.xml 123456789&lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/usr/local/Cellar/hadoop/hdfs/tmp&lt;/value&gt; &lt;description&gt;A base for other temporary directories.&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;fs.default.name&lt;/name&gt; &lt;value&gt;hdfs://localhost:9000&lt;/value&gt; &lt;/property&gt; mapred-site.xml文件在/usr/local/Cellar/hadoop/2.6.0/libexec/etc/hadoop/mapred-site.xml 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapred.job.tracker&lt;/name&gt; &lt;value&gt;localhost:9010&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; hdfs-site.xml文件在/usr/local/Cellar/hadoop/2.6.0/libexec/etc/hadoop/hdfs-site.xml 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 这就配置好了 添加启动关闭Hadoop快捷命令为了以后方便使用Hadoop，在.bash_profile中添加 12alias hstart="/usr/local/Cellar/hadoop/2.6.0/sbin/start-dfs.sh;/usr/local/Cellar/hadoop/2.6.0/sbin/start-yarn.sh"alias hstop="/usr/local/Cellar/hadoop/2.6.0/sbin/stop-yarn.sh;/usr/local/Cellar/hadoop/2.6.0/sbin/stop-dfs.sh" 执行下面命令将配置生效： 1source ~/.bash_profile 以后就能使用命令hstart启动Hadoop服务，hstop关闭Hadoop 格式化HDFS在使用Hadoop之前，还需要将HDFS格式化 1$ hdfs namenode -format Running Hadoop奔跑吧Hadoop 1$ hstart 使用jps命令查看Hadoop运行状态 1234567$ jps18065 SecondaryNameNode18283 Jps17965 DataNode18258 NodeManager18171 ResourceManager17885 NameNode 下面是几个很有用的监控Hadoop地址： Resource Manager: http://localhost:50070 JobTracker: http://localhost:8088 Specific Node Information: http://localhost:8042 停止Hadoop： 1$ hstop 添加Hadoop环境变量为了以后安装Spark等方便，在~/.bash_profile配置中添加Hadoop环境变量 12345# Setting PATH for hadoopexport HADOOP_HOME=/usr/local/Cellar/hadoop/2.6.0/libexecexport PATH=$HADOOP_HOME/bin:$PATHexport HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop 可能遇到的问题跑起来之后，或者在跑起来的过程中，可能会遇到各种问题，由于控制台命令太多，很难知道到底是哪儿出的问题，所以我总结出几个我遇到的问题和解决方法，分享给大家。TBD NameNode启动失败 大功告成！可以在Hadoop上跑几个MapReduce任务了。]]></content>
      <categories>
        <category>Big Data</category>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Big Data</tag>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在不同的机器上写博客]]></title>
    <url>%2F2015%2F07%2F17%2F2015-07-17-write-blog-from-different-machine%2F</url>
    <content type="text"><![CDATA[在家的Mac上配置好了Octopress，上班到公司还是要面对大Windows，这时候，想写一篇博客记录一下遇到的问题，怎么办？ Octopress原理Octopress的git仓库(repository)有两个分支，分别是master和source。master存储的是博客网站本身（html静态页面），而source存储的是生成博客的源文件（包括配置等）。master的内容放在根目录的_deploy文件夹内，当你push源文件时会忽略，它使用的是rake deploy命令来更新的。 下面开始在一台新机器上搞 创建一个本地Octopress仓库将博客的源文件，也就是source分支clone到本地的Octopress文件夹内 1$ git clone -b source git@github.com:username/username.github.com.git octopress 然后将博客文件也就是master分支clone到Octopress文件夹的_deploy文件夹内 12$ cd octopress$ git clone git@github.com:username/username.github.com.git _deploy 然后安装博客 123$ gem install bundler$ bundle install$ rake setup_github_pages OK了 继续写博客当你要在一台电脑写博客或做更改时，首先更新source仓库 1234$ cd octopress$ git pull origin source # update the local source branch$ cd ./_deploy$ git pull origin master # update the local master branch 写完博客之后不要忘了push，下面的步骤在每次更改之后都必须做一遍。 12345$ rake generate$ git add .$ git commit -am "Some comment here." $ git push origin source # update the remote source branch $ rake deploy # update the remote master branch]]></content>
      <categories>
        <category>备忘</category>
      </categories>
      <tags>
        <tag>Octopress</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Github与Octopress写博客]]></title>
    <url>%2F2015%2F07%2F16%2F2015-07-16-Writing%20Blogs%20with%20Github%20Pages%20and%20Octopress%2F</url>
    <content type="text"><![CDATA[Octopress和Github Pages是什么？ Octopress是一个基于Ruby语言的开源静态网站框架 Github Pages是Github上的一项服务， 注册用户可以申请一个和自己账号关联的二级域名， 在上面可以托管一个静态网站，网站内容本身就是Github的一个repository也就是项目， 维护这个项目的代码就是在维护自己的网站。简单来说就是 yourname.github.io/ 使用Octopress搭建博客，然后使用Github托管，有以下几个原因： 免费 版本控制，可以使用git实现写文章、建网站时候修改的版本控制 Octopress容易上手，并且这个风格正是我喜欢的，尤其对于一个理工男 使用Markdown，markdown是世界上最流行的轻量级标记语言 很酷，能装逼，当然这不是重点 搭建Octopress博客系统Note: 在这儿写的是关于在Mac上安装Octopress博客系统，和windows有细微的差别，不过个人觉得还是用Mac，无论写代码还是做黑客都更专业。 安装基本工具git 对于Mac来说，安装XCode之后，自带了git，可以使用下面命令检查本机的git版本 1$ git version Ruby Mac本身自带Ruby，但是也许版本过低，在这儿多说一句：有时候在低版本ruby下搭建好的Octopress，莫名其妙不好用了，原因也许就是Mac升级之后，ruby也升级了，要注意一下 至于Mac下如何使用Homebrew安装，请查看其他文章。 12$ brew install ruby$ ruby --version Ruby版本在1.9.3以上就可以了，就可以使用gem来安装Ruby的包了 PS. gem在Ruby中，相当于Python中的pip 由于我们生活在一个伟大的国家，so在下一步安装前，先更改一下gem的更新源，改为淘宝的源 123gem sources -a http://ruby.taobao.org/gem sources -r http://rubygems.org/gem sources -l 三行命令的作用分别是：添加淘宝源；删除默认源；显示当前源列表。显示淘宝地址就表示成功。 安装bundle和bundler， 12gem install bundlegem install bundler Note: 安装配置完新版本的Ruby后，一定要重新安装bundle和bundler，否则bundle仍会bundler指向旧版本的Ruby，PS. 由于手贱，把MAC升级到最新系统了，结果各种奇妙的事情就发生了，不过处理方法一般都是：安装最新版本的Ruby，然后再安装bundler和bundler Octopress 这个就是我们要使用的框架，它是基于Jekyll的一个静态博客生成框架，Jekyll是一个静态网站生成框架，它有很多功能，也可以直接使用，但是就麻烦得多，很多东西要配置和从头写。 1234git clone git://github.com/imathis/octopress.git octopresscd octopressbundle installrake install 创建Github账号和Github Pages 大多数人都已经有了Github帐号了，访问Github来注册帐号，然后访问Github Pages来创建博客空间，唯一需要注意的是Repo必须是Github帐号.github.io，否则不会起作用。 然后运行： 1rake setup_github_pages 输入Github Page的Repo的地址，例如：git@github.com:username/username.github.io.git，就可以了 测试一下输入命令生成页面 1rake generate 生成完毕后，使用以下命令启动网站进程，默认占用4000端口， preview一下 1rake preview 可以使用 http://localhost:4000 访问你的博客页面了 配置博客配置文件是根目录下的 _config.yml文件，使用vim或者其他文本编辑器编辑它吧 12345678910# ----------------------- ## Main Configs ## ----------------------- #url: http://lvraikkonen.github.iotitle: My Data Science Pathsubtitle: Shut up, just codingauthor: Claus Lvsimple_search: https://www.google.com/searchdescription: 语法高亮例子： 1alert("欢迎") 1234import mathprint "Hello World"lst = range(100)print lst.map(lambda x: x**2) 123456789101112131415&lt;!-- mathjax config similar to math.stackexchange --&gt;&lt;script type="text/x-mathjax-config"&gt; MathJax.Hub.Config(&#123; jax: ["input/TeX", "output/HTML-CSS"], tex2jax: &#123; inlineMath: [ ['$', '$'] ], displayMath: [ ['$$', '$$']], processEscapes: true, skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'] &#125;, messageStyle: "none", "HTML-CSS": &#123; preferredFont: "TeX", availableFonts: ["STIX","TeX"] &#125; &#125;);&lt;/script&gt;&lt;script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"&gt;&lt;/script&gt; 添加社交分享Octopress默认是带有社交分享功能的，比如Twitter, Facebook, Google Plus等，但这些全世界都通用的东西在我大天朝就是不好使。 网站页的分享有很多第三方的库，这里用jiathis 在_config.yml中加入social_share: true 修改/source/_includes/post/sharing.html 访问jiathis获取分享的代码，放入新建的/source/_includes/post/social_media.html 添加博客评论Octopress也默认集成有评论系统Disqus，这个是国外最大的第三方评论平台，世界都在用，除了我大天朝。这里使用多说 到多说注册，获取用户名，也就是在多说上添的youname.duoshuo.com中的yourname 在_config.yml中添加 12duoshuo_comments: trueduoshuo_short_name: yourname 在/source/_layouts/post.html中把评论模版添加到网页中 创建/source/_includes/post/duoshuo.html，将上步获取的HTML代码放进去 国内访问加速jQuery源 修改jQuery的源 打开source/_includes/head.html，找到如下 1&lt;script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"&gt;&lt;/script&gt; 改为： 1&lt;script src="//ajax.aspnetcdn.com/ajax/jQuery/jquery-1.9.1.min.js"&gt;&lt;/script&gt; 字体源 Octopress的英文字体是加载的Google Fonts，我们将其改成国内的CDN源， 打开source/_includes/custom/head.html, 将其中的https://fonts.googleapis.com改为http://fonts.useso.com Twitter Facebook Google+关闭 在前面提到的_config.yml中相关的例如twitter_tweet_butto改为false 写博客要发布一篇新文章，在命令行中输入以下命令： 1rake new_post["postName"] 之后在/source/_post/里面就有该博文的markdown文件了，使用Markdown文本编辑器写博客吧 rake generate 生成静态的博客文件，生成的文件在_deploy中 rake preview 在本地预览博客，这与发布到Github Pages后的效果是一样的 rake deploy 这是最后一步，就是把Octopress生成的文件（在_deploy）发布到Github上面去。这里的实际是Octopress根据你的配置用sources中的模板，生成网页（HTML，JavaScript, CSS和资源），再把这些资源推送到yourname.github.io这个Repo中去，然后访问https://*yourname*.github.io 就能看到你的博客了 发布执行命令 12$ rake generate$ rake deploy 第一行命令用来生成页面，第二行命令用来部署页面，上述内容完成，就可以访问 http://[your_username].github.io/看博客了 Note: octopress 根目录为source分支， _deploy目录下为master分支，rake deploy时候会把_deploy下的内容发布到github上的master分支。别忘了把源文件（包括配置等）发布到source分支下 push时候可用 1$ git status 查看状态 执行以下命令，将源文件发布到Github的source分支 123git add .git commit -m "备注内容"git push origin source 如果遇到类似error: failed to push some refs to的错误，参考 stackoverflow解决]]></content>
      <categories>
        <category>备忘</category>
      </categories>
      <tags>
        <tag>Octopress</tag>
      </tags>
  </entry>
</search>